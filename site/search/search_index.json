{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"The Applied Generative AI Engineering Handbook <p>             Zero theory-only fluff. A code-first curriculum taking you from tensor basics to deploying production-grade RAG agents.              Open Source. Interactive. Community-Driven. </p> <p>Select your path to start:</p> school                     Understanding AI                  code                     Engineering AI                  biotech                     Researching AI                  architecture                     Architecting AI                  business_center                     Business AI                  <p>MASTER THE MODERN AI STACK</p> \ud83d\udd25 PyTorch \ud83e\udd17 Hugging Face \u26a1 vLLM \ud83d\udcc8 Weights &amp; Biases \ud83d\ude80 FastAPI Everything you would expect <p>                 Interactive Jupyter notebooks with live code execution. Progressive learning paths with 50+ lessons and 200+ hands-on exercises.                  Production-ready projects including RAG systems, model deployment, and API development. Vibrant community of 5,000+ AI engineers                  with weekly events and real-time support.             </p>                  Stay ahead of the curve <p>                 Get the latest GenAI trends, tutorials, and model breakdowns delivered weekly.                 Join 10,000+ AI engineers staying updated.             </p>                      Subscribe                     arrow_forward <p> lock                 No spam ever. Unsubscribe anytime.             </p> Follow us on social media auto_awesome                     MadeForAI                  <p>                     Empowering the next generation of AI engineers.                 </p> Understanding AI Engineering AI Researching AI Architecting AI Business AI <p>\u00a9 2026 MadeForAI Inc.</p>"},{"location":"community/","title":"Community","text":"Join Our Community <p>             Connect with thousands of AI enthusiasts, engineers, and researchers building the future together.         </p> 5,000+ Members 500+ GitHub Stars 100+ Contributors Discord Community <p>                 Join our vibrant Discord server to connect with fellow learners, get real-time help, share your AI projects, and participate in weekly events and study groups.             </p> Join Discord Server Open Source on GitHub <p>                 All our content is open source and available on GitHub. Contribute code, fix bugs, add new modules, improve documentation, or simply star the repository to show your support.             </p> View on GitHub Follow on X <p>                 Stay updated with the latest AI trends, new module releases, community highlights, and industry insights. Join the conversation and connect with AI practitioners worldwide.             </p> Follow @madeforai \ud83e\udd1d Code of Conduct <p>             We're committed to providing a welcoming, inclusive, and harassment-free community for everyone. Please be respectful, constructive, and supportive in all interactions.         </p> <p>.community-hero {     padding: 4rem 0 3rem;     text-align: center;     border-bottom: 1px solid #f1f5f9;     margin-bottom: 3rem; }</p> <p>.community-icon {     display: inline-flex;     align-items: center;     justify-content: center;     width: 64px;     height: 64px;     background: linear-gradient(135deg, #8b5cf6 0%, #7c3aed 100%);     border-radius: 1rem;     margin-bottom: 1.5rem;     box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); }</p> <p>.community-icon .material-symbols-outlined {     font-size: 2rem;     color: #ffffff;     font-weight: 400; }</p> <p>.community-title {     font-family: 'Plus Jakarta Sans', sans-serif;     font-size: 2.5rem;     font-weight: 800;     color: #0f172a;     margin: 0 0 0.75rem 0;     letter-spacing: -0.025em; }</p> <p>.community-subtitle {     font-size: 1.125rem;     color: #64748b;     margin: 0 0 2rem 0;     font-weight: 400;     max-width: 600px;     margin-left: auto;     margin-right: auto; }</p> <p>.community-stats {     display: grid;     grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));     gap: 1.5rem;     margin: 3rem 0; }</p> <p>.stat-card {     background: #ffffff;     border: 1px solid #e2e8f0;     border-radius: 0.75rem;     padding: 1.5rem;     text-align: center;     transition: all 0.2s ease; }</p> <p>.stat-card:hover {     border-color: #8b5cf6;     box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);     transform: translateY(-2px); }</p> <p>.stat-number {     font-family: 'Plus Jakarta Sans', sans-serif;     font-size: 2rem;     font-weight: 800;     color: #8b5cf6;     margin: 0 0 0.25rem 0; }</p> <p>.stat-label {     font-size: 0.875rem;     color: #64748b;     font-weight: 600;     text-transform: uppercase;     letter-spacing: 0.05em; }</p> <p>.platform-section {     margin: 3rem 0; }</p> <p>.platform-card {     background: #ffffff;     border: 1px solid #e2e8f0;     border-radius: 0.75rem;     padding: 2rem;     margin-bottom: 1.5rem;     transition: all 0.2s ease; }</p> <p>.platform-card:hover {     border-color: #8b5cf6;     box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);     transform: translateY(-2px); }</p> <p>.platform-header {     display: flex;     align-items: center;     gap: 1rem;     margin-bottom: 1.25rem; }</p> <p>.platform-icon {     display: flex;     align-items: center;     justify-content: center;     width: 48px;     height: 48px;     border-radius: 0.75rem;     flex-shrink: 0; }</p> <p>.platform-icon.discord {     background: linear-gradient(135deg, #5865f2 0%, #4752c4 100%); }</p> <p>.platform-icon.github {     background: linear-gradient(135deg, #24292e 0%, #1b1f23 100%); }</p> <p>.platform-icon.twitter {     background: linear-gradient(135deg, #1da1f2 0%, #0c85d0 100%); }</p> <p>.platform-icon svg {     width: 24px;     height: 24px;     fill: #ffffff; }</p> <p>.platform-title {     font-family: 'Plus Jakarta Sans', sans-serif;     font-size: 1.5rem;     font-weight: 700;     color: #0f172a;     margin: 0; }</p> <p>.platform-description {     font-size: 1rem;     color: #475569;     line-height: 1.7;     margin-bottom: 1.5rem; }</p> <p>.feature-list {     display: grid;     grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));     gap: 1rem;     margin-bottom: 1.5rem; }</p> <p>.feature-item {     display: flex;     align-items: flex-start;     gap: 0.75rem;     padding: 0.75rem;     background: #f8fafc;     border-radius: 0.5rem; }</p> <p>.feature-icon {     font-size: 1.25rem;     flex-shrink: 0; }</p> <p>.feature-text {     flex: 1; }</p> <p>.feature-text strong {     display: block;     font-weight: 600;     color: #0f172a;     margin-bottom: 0.125rem; }</p> <p>.feature-text span {     font-size: 0.875rem;     color: #64748b; }</p> <p>.btn-platform {     display: inline-flex;     align-items: center;     gap: 0.5rem;     padding: 0.875rem 1.75rem;     border-radius: 0.5rem;     text-decoration: none;     font-weight: 600;     font-size: 0.9375rem;     transition: all 0.2s ease; }</p> <p>.btn-platform.discord {     background: #5865f2;     color: #ffffff; }</p> <p>.btn-platform.discord:hover {     background: #4752c4;     transform: translateY(-2px);     box-shadow: 0 4px 12px rgba(88, 101, 242, 0.3);     color: #ffffff; }</p> <p>.btn-platform.github {     background: #24292e;     color: #ffffff; }</p> <p>.btn-platform.github:hover {     background: #1b1f23;     transform: translateY(-2px);     box-shadow: 0 4px 12px rgba(36, 41, 46, 0.3);     color: #ffffff; }</p> <p>.btn-platform.twitter {     background: #1da1f2;     color: #ffffff; }</p> <p>.btn-platform.twitter:hover {     background: #0c85d0;     transform: translateY(-2px);     box-shadow: 0 4px 12px rgba(29, 161, 242, 0.3);     color: #ffffff; }</p> <p>.contribution-steps {     display: grid;     grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));     gap: 1.5rem;     margin: 2rem 0; }</p> <p>.step-card {     background: #f8fafc;     border: 1px solid #e2e8f0;     border-radius: 0.75rem;     padding: 1.5rem;     text-align: center; }</p> <p>.step-number {     display: inline-flex;     align-items: center;     justify-content: center;     width: 32px;     height: 32px;     background: #8b5cf6;     color: #ffffff;     border-radius: 50%;     font-weight: 700;     font-size: 0.875rem;     margin-bottom: 1rem; }</p> <p>.step-title {     font-weight: 600;     color: #0f172a;     margin-bottom: 0.5rem; }</p> <p>.step-description {     font-size: 0.875rem;     color: #64748b; }</p> <p>.conduct-section {     margin-top: 4rem;     padding: 2rem;     background: linear-gradient(135deg, #8b5cf608 0%, #8b5cf603 100%);     border: 1px solid #8b5cf620;     border-radius: 1rem;     text-align: center; }</p> <p>.conduct-section h3 {     font-family: 'Plus Jakarta Sans', sans-serif;     font-size: 1.25rem;     font-weight: 700;     color: #0f172a;     margin: 0 0 0.75rem 0; }</p> <p>.conduct-section p {     font-size: 1rem;     color: #475569;     margin: 0;     line-height: 1.7; }</p> <p>@media (max-width: 768px) {     .community-hero {         padding: 3rem 0 2rem;     }</p> <pre><code>.community-title {\n    font-size: 2rem;\n}\n\n.community-stats {\n    grid-template-columns: repeat(2, 1fr);\n}\n\n.feature-list {\n    grid-template-columns: 1fr;\n}\n\n.contribution-steps {\n    grid-template-columns: 1fr;\n}\n</code></pre> <p>} </p> groups Join Our Community <p>Connect with thousands of AI enthusiasts, engineers, and researchers building the future together</p> 5,000+ Community Members 500+ GitHub Stars 100+ Contributors 50+ Learning Modules Discord Community <p>Join our vibrant Discord server to connect with fellow learners, get real-time help, and share your AI projects with the community.</p> \ud83c\udd98 Help Channels Get answers fast \ud83d\udca1 Project Showcase Share your work \ud83d\udce2 Announcements Stay updated \ud83c\udf93 Study Groups Learn together Join Discord Server Open Source on GitHub <p>All our content is open source and available on GitHub. Contribute code, fix bugs, add new modules, or improve documentation.</p> 1 Fork Repository Create your own copy 2 Make Changes Add your improvements 3 Submit PR Share with community 4 Get Recognized Join contributors list View on GitHub Follow on Twitter/X <p>Stay updated with the latest AI trends, new module releases, community highlights, and industry insights. Join the conversation!</p> Follow @madeforai \ud83e\udd1d Code of Conduct <p>We're committed to providing a welcoming, inclusive, and harassment-free community for everyone. Please be respectful, constructive, and supportive in all interactions. Together, we're building something amazing.</p>"},{"location":"contributing/","title":"Contributing","text":"Contributing to MadeForAI <p>             Thank you for your interest in contributing! We welcome contributions from everyone, whether you're fixing a typo, adding a new module, or improving documentation.         </p> Ways to Contribute \ud83d\udc1b Report Bugs <p>                     Found a bug? Create an issue with clear steps to reproduce, expected vs actual behavior, and your environment details.                 </p> \u2728 Suggest Features <p>                     Have an idea? Open an issue describing the feature, use cases, benefits, and possible implementation approach.                 </p> \ud83d\udcdd Improve Documentation <p>                     Fix typos, clarify confusing sections, add missing information, or improve code examples.                 </p> \ud83d\udcda Add Content <p>                     Contribute new Jupyter notebooks, tutorials, guides, code examples, or curated resources.                 </p> \ud83c\udf0d Translate <p>                     Help make AI education accessible globally by translating content into other languages.                 </p> \ud83c\udfa8 Design &amp; UX <p>                     Improve the visual design, user experience, or create illustrations and diagrams.                 </p> Quick Start <p>             Get started with contributing in just a few steps:         </p> <code># Fork and clone the repository git clone https://github.com/YOUR-USERNAME/madeforai.git cd madeforai  # Create a virtual environment python -m venv venv source venv/bin/activate  # On Windows: venv\\Scripts\\activate  # Install dependencies pip install -r requirements.txt  # Create a new branch git checkout -b feature/your-feature-name  # Start the development server mkdocs serve  # Visit http://127.0.0.1:8000 in your browser</code> <p>             Make your changes, test them locally, commit, and create a pull request. We'll review it and provide feedback!         </p> Need Help? <p>             We're here to help! Reach out through any of these channels:         </p> \ud83d\udcac Join Discord \ud83d\udcad GitHub Discussions \ud83d\udc1b Open an Issue Thank you for contributing! \ud83c\udf89 <p>             Every contribution, no matter how small, helps make this resource better for everyone. By contributing, you agree that your contributions will be licensed under the MIT License.         </p>"},{"location":"hooks/","title":"MadeForAI Build Hooks","text":""},{"location":"hooks/#madeforai-build-hooks","title":"MadeForAI Build Hooks","text":"<p>This directory contains MkDocs hooks that enhance the documentation build process.</p>"},{"location":"hooks/#available-hooks","title":"Available Hooks","text":""},{"location":"hooks/#1-notebook_enhancerpy","title":"1. <code>notebook_enhancer.py</code>","text":"<p>Enhances Jupyter notebook rendering with interactive features like copy buttons and improved styling.</p>"},{"location":"hooks/#2-path_generatorpy-new","title":"2. <code>path_generator.py</code> \u2b50 NEW","text":"<p>Dynamically generates beautiful learning path overview pages based on the navigation structure in <code>mkdocs.yml</code>.</p>"},{"location":"hooks/#how-it-works","title":"How It Works","text":"<p>The path generator automatically creates/updates the <code>index.md</code> file for each learning path during the build process. It:</p> <ol> <li>Reads the navigation structure from <code>mkdocs.yml</code></li> <li>Extracts modules and chapters for each learning path</li> <li>Generates beautiful HTML/Markdown with:</li> <li>Hero section with path icon and description</li> <li>Prerequisites and learning outcomes in styled cards</li> <li>Module cards with chapter links</li> <li>\"Coming Soon\" placeholders for future modules</li> <li>Responsive design with animations</li> <li>Call-to-action button to start learning</li> </ol>"},{"location":"hooks/#configuration","title":"Configuration","text":"<p>Each learning path is configured in <code>path_generator.py</code> with:</p> <pre><code>path_configs = {\n    'understanding-ai': {\n        'title': 'Understanding AI',\n        'subtitle': 'From Absolute Beginner to AI Enthusiast',\n        'description': '...',\n        'icon': '\ud83c\udf93',\n        'color': '#3b82f6',\n        'prerequisites': [...],\n        'outcomes': [...]\n    },\n    # ... more paths\n}\n</code></pre>"},{"location":"hooks/#adding-a-new-learning-path","title":"Adding a New Learning Path","text":"<ol> <li> <p>Add the path to <code>mkdocs.yml</code> navigation: </p><pre><code>nav:\n  - New Path:\n      - New Path: paths/new-path/index.md\n      - Module 1:\n          - Chapter 1: paths/new-path/module-1/chapter-1.ipynb\n</code></pre><p></p> </li> <li> <p>Add configuration in <code>path_generator.py</code>: </p><pre><code>'new-path': {\n    'title': 'New Path',\n    'subtitle': 'Your subtitle',\n    'description': 'Path description',\n    'icon': '\ud83d\ude80',\n    'color': '#10b981',\n    'prerequisites': ['Prerequisite 1', 'Prerequisite 2'],\n    'outcomes': ['Outcome 1', 'Outcome 2']\n}\n</code></pre><p></p> </li> <li> <p>Build the site: </p><pre><code>mkdocs build\n# or\nmkdocs serve\n</code></pre><p></p> </li> </ol> <p>The <code>index.md</code> file will be automatically generated/updated!</p>"},{"location":"hooks/#benefits","title":"Benefits","text":"<p>\u2705 No manual updates needed - Pages update automatically when you add/remove chapters \u2705 Consistent design - All learning paths have the same beautiful layout \u2705 Dynamic content - Reflects the actual navigation structure \u2705 Easy maintenance - Update once in mkdocs.yml, reflects everywhere \u2705 Beautiful UI - Modern cards, animations, and responsive design  </p>"},{"location":"hooks/#customization","title":"Customization","text":"<p>To customize the generated pages:</p> <ol> <li>Styling: Edit the inline styles in <code>generate_markdown()</code> function</li> <li>Layout: Modify the HTML structure in <code>generate_markdown()</code></li> <li>Colors: Update the <code>color</code> field in path configs</li> <li>Icons: Change the <code>icon</code> field (supports emoji)</li> </ol>"},{"location":"hooks/#usage","title":"Usage","text":"<p>These hooks are automatically executed during the MkDocs build process. No manual intervention required!</p> <pre><code># Development server (hooks run on file changes)\nmkdocs serve\n\n# Production build (hooks run once)\nmkdocs build\n</code></pre>"},{"location":"hooks/#development","title":"Development","text":"<p>To test the path generator independently:</p> <pre><code>import sys\nsys.path.insert(0, 'docs/hooks')\nfrom path_generator import on_pre_build\n\nclass MockConfig(dict):\n    def __init__(self):\n        super().__init__()\n        self['docs_dir'] = 'docs'\n        self['config_file_path'] = 'mkdocs.yml'\n\non_pre_build(MockConfig())\n</code></pre>"},{"location":"hooks/notebook_enhancer/","title":"Notebook enhancer","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nMkDocs hook to enhance notebook rendering - Clean &amp; Minimal\n\"\"\"\n</pre> \"\"\" MkDocs hook to enhance notebook rendering - Clean &amp; Minimal \"\"\" In\u00a0[\u00a0]: Copied! <pre>import re\nfrom pathlib import Path\n</pre> import re from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>def on_page_content(html, page, config, files):\n    \"\"\"\n    Hook to modify the HTML content of notebook pages\n    \"\"\"\n    # Only process notebook pages\n    if not page.file.src_path.endswith('.ipynb'):\n        return html\n    \n    # Check if this is a notebook page\n    if 'jp-Notebook' not in html:\n        return html\n    \n    # Extract Title (H1)\n    title_match = re.search(r'&lt;h1.*?&gt;(.*?)&lt;/h1&gt;', html, re.DOTALL)\n    title = title_match.group(1) if title_match else \"Documentation\"\n    \n    # Remove \"Chapter X:\" prefix from title\n    title = re.sub(r'^Chapter\\s+\\d+:\\s*', '', title, flags=re.IGNORECASE)\n    \n    # Remove original H1\n    html = re.sub(r'&lt;h1.*?&gt;.*?&lt;/h1&gt;', '', html, count=1, flags=re.DOTALL)\n    \n    # Remove the Colab badge\n    html = re.sub(r'&lt;p&gt;\\s*&lt;a[^&gt;]*colab[^&gt;]*&gt;\\s*&lt;img[^&gt;]*colab-badge[^&gt;]*&gt;\\s*&lt;/a&gt;\\s*&lt;/p&gt;', '', html, flags=re.IGNORECASE)\n    \n    # Remove first HR\n    html = re.sub(r'&lt;hr&gt;\\s*', '', html, count=1)\n    \n    # Colab link\n    notebook_path = page.file.src_path\n    colab_url = f\"https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/{notebook_path}\"\n\n    # Minimal header\n    header_html = f\"\"\"\n    &lt;div class=\"nb-header\"&gt;\n        &lt;h1&gt;{title}&lt;/h1&gt;\n        &lt;a href=\"{colab_url}\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"nb-colab-btn\"&gt;\n            &lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"currentColor\"&gt;\n                &lt;path d=\"M12 0C5.373 0 0 5.373 0 12s5.373 12 12 12 12-5.373 12-12S18.627 0 12 0zm0 2.4c5.302 0 9.6 4.298 9.6 9.6s-4.298 9.6-9.6 9.6S2.4 17.302 2.4 12 6.698 2.4 12 2.4z\"/&gt;\n                &lt;path d=\"M8.4 7.2h7.2v1.2H8.4zm0 2.4h7.2v1.2H8.4zm0 2.4h7.2v1.2H8.4zm0 2.4h4.8v1.2H8.4z\"/&gt;\n            &lt;/svg&gt;\n            Open in Colab\n        &lt;/a&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return header_html + html\n</pre> def on_page_content(html, page, config, files):     \"\"\"     Hook to modify the HTML content of notebook pages     \"\"\"     # Only process notebook pages     if not page.file.src_path.endswith('.ipynb'):         return html          # Check if this is a notebook page     if 'jp-Notebook' not in html:         return html          # Extract Title (H1)     title_match = re.search(r'(.*?)', html, re.DOTALL)     title = title_match.group(1) if title_match else \"Documentation\"          # Remove \"Chapter X:\" prefix from title     title = re.sub(r'^Chapter\\s+\\d+:\\s*', '', title, flags=re.IGNORECASE)          # Remove original H1     html = re.sub(r'.*?', '', html, count=1, flags=re.DOTALL)          # Remove the Colab badge     html = re.sub(r'<p>\\s*]*colab[^&gt;]*&gt;\\s*]*colab-badge[^&gt;]*&gt;\\s*\\s*</p>', '', html, flags=re.IGNORECASE)          # Remove first HR     html = re.sub(r'\\s*', '', html, count=1)          # Colab link     notebook_path = page.file.src_path     colab_url = f\"https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/{notebook_path}\"      # Minimal header     header_html = f\"\"\"      {title}              Open in Colab               \"\"\"      return header_html + html In\u00a0[\u00a0]: Copied! <pre>def on_post_page(output, page, config):\n    \"\"\"\n    Minimal post-processing\n    \"\"\"\n    if not page.file.src_path.endswith('.ipynb'):\n        return output\n    \n    return output\n</pre> def on_post_page(output, page, config):     \"\"\"     Minimal post-processing     \"\"\"     if not page.file.src_path.endswith('.ipynb'):         return output          return output"},{"location":"hooks/path_generator/","title":"Path generator","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nDynamic Learning Path Page Generator for MadeForAI\nAutomatically generates beautiful learning path overview pages based on mkdocs.yml navigation\n\"\"\"\n</pre> \"\"\" Dynamic Learning Path Page Generator for MadeForAI Automatically generates beautiful learning path overview pages based on mkdocs.yml navigation \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport yaml\nfrom pathlib import Path\n</pre> import os import yaml from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>def on_pre_build(config):\n    \"\"\"Generate learning path pages before build\"\"\"\n    \n    # Path configurations for each learning path\n    path_configs = {\n        'understanding-ai': {\n            'title': 'Understanding AI',\n            'subtitle': 'Build a solid foundation in AI fundamentals',\n            'description': 'Master the core concepts of artificial intelligence, machine learning, and deep learning. Build your first models and understand how modern AI systems work through hands-on interactive lessons.',\n            'icon': 'school',\n            'color': '#3b82f6',\n            'gradient_start': '#3b82f6',\n            'gradient_end': '#2563eb',\n            'prerequisites': [\n                'Basic Python programming',\n                'High school mathematics',\n                'Curiosity about AI'\n            ],\n            'outcomes': [\n                'Understand AI, ML, and Deep Learning',\n                'Build neural networks from scratch',\n                'Work with modern LLMs',\n                'Master prompt engineering basics'\n            ]\n        },\n        'engineering-ai': {\n            'title': 'Engineering AI',\n            'subtitle': 'Build production-ready LLM applications',\n            'description': 'Dive deep into transformer architecture, fine-tuning techniques, and deployment strategies. Learn to build scalable AI systems that work in production environments.',\n            'icon': 'code',\n            'color': '#8b5cf6',\n            'gradient_start': '#8b5cf6',\n            'gradient_end': '#7c3aed',\n            'prerequisites': [\n                'Python programming experience',\n                'Basic ML concepts',\n                'Deep learning frameworks'\n            ],\n            'outcomes': [\n                'Master Transformer architecture',\n                'Fine-tune models with LoRA/QLoRA',\n                'Build RAG systems',\n                'Deploy LLMs to production'\n            ]\n        },\n        'researching-ai': {\n            'title': 'Researching AI',\n            'subtitle': 'Push the boundaries of AI innovation',\n            'description': 'Explore cutting-edge research, learn to read and implement papers, and contribute to the field. Develop the skills to design novel architectures and conduct rigorous experiments.',\n            'icon': 'biotech',\n            'color': '#ec4899',\n            'gradient_start': '#ec4899',\n            'gradient_end': '#db2777',\n            'prerequisites': [\n                'Strong mathematical background',\n                'Deep learning fundamentals',\n                'Research experience helpful'\n            ],\n            'outcomes': [\n                'Read and implement papers',\n                'Design novel architectures',\n                'Conduct rigorous experiments',\n                'Contribute to open-source research'\n            ]\n        },\n        'architecting-ai': {\n            'title': 'Architecting AI',\n            'subtitle': 'Design scalable AI systems',\n            'description': 'Learn system design patterns, infrastructure choices, and best practices for building reliable, scalable AI systems. Master the art of architecting production-grade AI solutions.',\n            'icon': 'architecture',\n            'color': '#f59e0b',\n            'gradient_start': '#f59e0b',\n            'gradient_end': '#d97706',\n            'prerequisites': [\n                'System design experience',\n                'Cloud infrastructure knowledge',\n                'AI/ML basics'\n            ],\n            'outcomes': [\n                'Design scalable architectures',\n                'Choose optimal infrastructure',\n                'Implement monitoring systems',\n                'Optimize costs and performance'\n            ]\n        },\n        'business-ai': {\n            'title': 'Business AI',\n            'subtitle': 'Lead AI strategy and transformation',\n            'description': 'Understand AI strategy without diving into technical details. Learn to identify use cases, evaluate vendors, build teams, and navigate the business side of AI implementation.',\n            'icon': 'business_center',\n            'color': '#10b981',\n            'gradient_start': '#10b981',\n            'gradient_end': '#059669',\n            'prerequisites': [\n                'No technical background required',\n                'Business experience helpful',\n                'Interest in AI strategy'\n            ],\n            'outcomes': [\n                'Understand AI/LLM landscape',\n                'Identify high-value use cases',\n                'Make build vs. buy decisions',\n                'Build and manage AI teams'\n            ]\n        }\n    }\n    \n    # Read mkdocs.yml to get navigation structure\n    mkdocs_path = Path(config['config_file_path'])\n    with open(mkdocs_path, 'r', encoding='utf-8') as f:\n        # Use unsafe load since mkdocs.yml contains Python objects\n        mkdocs_config = yaml.unsafe_load(f)\n    \n    nav = mkdocs_config.get('nav', [])\n    \n    # Process each learning path\n    for nav_item in nav:\n        if isinstance(nav_item, dict):\n            for path_name, path_content in nav_item.items():\n                # Skip non-path items\n                if path_name in ['madeforAI', 'Community', 'About', 'Contributing']:\n                    continue\n                \n                # Get path slug\n                path_slug = path_name.lower().replace(' ', '-')\n                \n                if path_slug in path_configs and isinstance(path_content, list):\n                    generate_path_page(\n                        config,\n                        path_slug,\n                        path_configs[path_slug],\n                        path_content\n                    )\n</pre> def on_pre_build(config):     \"\"\"Generate learning path pages before build\"\"\"          # Path configurations for each learning path     path_configs = {         'understanding-ai': {             'title': 'Understanding AI',             'subtitle': 'Build a solid foundation in AI fundamentals',             'description': 'Master the core concepts of artificial intelligence, machine learning, and deep learning. Build your first models and understand how modern AI systems work through hands-on interactive lessons.',             'icon': 'school',             'color': '#3b82f6',             'gradient_start': '#3b82f6',             'gradient_end': '#2563eb',             'prerequisites': [                 'Basic Python programming',                 'High school mathematics',                 'Curiosity about AI'             ],             'outcomes': [                 'Understand AI, ML, and Deep Learning',                 'Build neural networks from scratch',                 'Work with modern LLMs',                 'Master prompt engineering basics'             ]         },         'engineering-ai': {             'title': 'Engineering AI',             'subtitle': 'Build production-ready LLM applications',             'description': 'Dive deep into transformer architecture, fine-tuning techniques, and deployment strategies. Learn to build scalable AI systems that work in production environments.',             'icon': 'code',             'color': '#8b5cf6',             'gradient_start': '#8b5cf6',             'gradient_end': '#7c3aed',             'prerequisites': [                 'Python programming experience',                 'Basic ML concepts',                 'Deep learning frameworks'             ],             'outcomes': [                 'Master Transformer architecture',                 'Fine-tune models with LoRA/QLoRA',                 'Build RAG systems',                 'Deploy LLMs to production'             ]         },         'researching-ai': {             'title': 'Researching AI',             'subtitle': 'Push the boundaries of AI innovation',             'description': 'Explore cutting-edge research, learn to read and implement papers, and contribute to the field. Develop the skills to design novel architectures and conduct rigorous experiments.',             'icon': 'biotech',             'color': '#ec4899',             'gradient_start': '#ec4899',             'gradient_end': '#db2777',             'prerequisites': [                 'Strong mathematical background',                 'Deep learning fundamentals',                 'Research experience helpful'             ],             'outcomes': [                 'Read and implement papers',                 'Design novel architectures',                 'Conduct rigorous experiments',                 'Contribute to open-source research'             ]         },         'architecting-ai': {             'title': 'Architecting AI',             'subtitle': 'Design scalable AI systems',             'description': 'Learn system design patterns, infrastructure choices, and best practices for building reliable, scalable AI systems. Master the art of architecting production-grade AI solutions.',             'icon': 'architecture',             'color': '#f59e0b',             'gradient_start': '#f59e0b',             'gradient_end': '#d97706',             'prerequisites': [                 'System design experience',                 'Cloud infrastructure knowledge',                 'AI/ML basics'             ],             'outcomes': [                 'Design scalable architectures',                 'Choose optimal infrastructure',                 'Implement monitoring systems',                 'Optimize costs and performance'             ]         },         'business-ai': {             'title': 'Business AI',             'subtitle': 'Lead AI strategy and transformation',             'description': 'Understand AI strategy without diving into technical details. Learn to identify use cases, evaluate vendors, build teams, and navigate the business side of AI implementation.',             'icon': 'business_center',             'color': '#10b981',             'gradient_start': '#10b981',             'gradient_end': '#059669',             'prerequisites': [                 'No technical background required',                 'Business experience helpful',                 'Interest in AI strategy'             ],             'outcomes': [                 'Understand AI/LLM landscape',                 'Identify high-value use cases',                 'Make build vs. buy decisions',                 'Build and manage AI teams'             ]         }     }          # Read mkdocs.yml to get navigation structure     mkdocs_path = Path(config['config_file_path'])     with open(mkdocs_path, 'r', encoding='utf-8') as f:         # Use unsafe load since mkdocs.yml contains Python objects         mkdocs_config = yaml.unsafe_load(f)          nav = mkdocs_config.get('nav', [])          # Process each learning path     for nav_item in nav:         if isinstance(nav_item, dict):             for path_name, path_content in nav_item.items():                 # Skip non-path items                 if path_name in ['madeforAI', 'Community', 'About', 'Contributing']:                     continue                                  # Get path slug                 path_slug = path_name.lower().replace(' ', '-')                                  if path_slug in path_configs and isinstance(path_content, list):                     generate_path_page(                         config,                         path_slug,                         path_configs[path_slug],                         path_content                     ) In\u00a0[\u00a0]: Copied! <pre>def generate_path_page(config, path_slug, path_config, nav_content):\n    \"\"\"Generate a beautiful learning path page\"\"\"\n    \n    docs_dir = Path(config['docs_dir'])\n    path_dir = docs_dir / 'paths' / path_slug\n    output_file = path_dir / 'index.md'\n    \n    # Extract modules and chapters from navigation\n    modules = []\n    first_chapter_link = None\n    \n    for item in nav_content:\n        if isinstance(item, dict):\n            for module_name, module_content in item.items():\n                # Skip the index page itself\n                if isinstance(module_content, str) and 'index.md' in module_content:\n                    continue\n                \n                # Process module\n                module = {\n                    'name': module_name,\n                    'chapters': []\n                }\n                \n                if isinstance(module_content, str):\n                    # Single chapter module\n                    chapter_link = module_content.replace('paths/' + path_slug + '/', '')\n                    module['chapters'].append({\n                        'title': module_name,\n                        'link': chapter_link\n                    })\n                    if not first_chapter_link:\n                        first_chapter_link = chapter_link\n                        \n                elif isinstance(module_content, list):\n                    # Multi-chapter module\n                    for chapter_item in module_content:\n                        if isinstance(chapter_item, dict):\n                            for chapter_title, chapter_path in chapter_item.items():\n                                chapter_link = chapter_path.replace('paths/' + path_slug + '/', '')\n                                module['chapters'].append({\n                                    'title': chapter_title,\n                                    'link': chapter_link\n                                })\n                                if not first_chapter_link:\n                                    first_chapter_link = chapter_link\n                \n                if module['chapters']:\n                    modules.append(module)\n    \n    # Generate markdown content\n    content = generate_markdown(path_config, modules, first_chapter_link)\n    \n    # Only write if content has changed (prevent infinite rebuild loop)\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n    \n    should_write = True\n    if output_file.exists():\n        with open(output_file, 'r', encoding='utf-8') as f:\n            existing_content = f.read()\n        if existing_content == content:\n            should_write = False\n    \n    if should_write:\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(content)\n        print(f\"\u2713 Generated: {output_file}\")\n    else:\n        print(f\"\u2713 Unchanged: {output_file}\")\n</pre> def generate_path_page(config, path_slug, path_config, nav_content):     \"\"\"Generate a beautiful learning path page\"\"\"          docs_dir = Path(config['docs_dir'])     path_dir = docs_dir / 'paths' / path_slug     output_file = path_dir / 'index.md'          # Extract modules and chapters from navigation     modules = []     first_chapter_link = None          for item in nav_content:         if isinstance(item, dict):             for module_name, module_content in item.items():                 # Skip the index page itself                 if isinstance(module_content, str) and 'index.md' in module_content:                     continue                                  # Process module                 module = {                     'name': module_name,                     'chapters': []                 }                                  if isinstance(module_content, str):                     # Single chapter module                     chapter_link = module_content.replace('paths/' + path_slug + '/', '')                     module['chapters'].append({                         'title': module_name,                         'link': chapter_link                     })                     if not first_chapter_link:                         first_chapter_link = chapter_link                                          elif isinstance(module_content, list):                     # Multi-chapter module                     for chapter_item in module_content:                         if isinstance(chapter_item, dict):                             for chapter_title, chapter_path in chapter_item.items():                                 chapter_link = chapter_path.replace('paths/' + path_slug + '/', '')                                 module['chapters'].append({                                     'title': chapter_title,                                     'link': chapter_link                                 })                                 if not first_chapter_link:                                     first_chapter_link = chapter_link                                  if module['chapters']:                     modules.append(module)          # Generate markdown content     content = generate_markdown(path_config, modules, first_chapter_link)          # Only write if content has changed (prevent infinite rebuild loop)     output_file.parent.mkdir(parents=True, exist_ok=True)          should_write = True     if output_file.exists():         with open(output_file, 'r', encoding='utf-8') as f:             existing_content = f.read()         if existing_content == content:             should_write = False          if should_write:         with open(output_file, 'w', encoding='utf-8') as f:             f.write(content)         print(f\"\u2713 Generated: {output_file}\")     else:         print(f\"\u2713 Unchanged: {output_file}\") In\u00a0[\u00a0]: Copied! <pre>def generate_markdown(config, modules, first_chapter_link):\n    \"\"\"Generate beautiful markdown content matching homepage design\"\"\"\n    \n    md = f\"\"\"---\ntitle: {config['title']}\ndescription: {config['subtitle']}\n---\n\n&lt;style&gt;\n/* Path Page Styles - Matching Homepage Design Philosophy */\n.md-content__inner {{\n    max-width: 900px;\n    margin: 0 auto;\n}}\n\n.path-hero {{\n    padding: 4rem 0 3rem;\n    text-align: center;\n    border-bottom: 1px solid #f1f5f9;\n    margin-bottom: 3rem;\n}}\n\n.path-icon {{\n    display: inline-flex;\n    align-items: center;\n    justify-content: center;\n    width: 64px;\n    height: 64px;\n    background: linear-gradient(135deg, {config['gradient_start']} 0%, {config['gradient_end']} 100%);\n    border-radius: 1rem;\n    margin-bottom: 1.5rem;\n    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);\n}}\n\n.path-icon .material-symbols-outlined {{\n    font-size: 2rem;\n    color: #ffffff;\n    font-weight: 400;\n}}\n\n.path-title {{\n    font-family: 'Plus Jakarta Sans', sans-serif;\n    font-size: 2.5rem;\n    font-weight: 800;\n    color: #0f172a;\n    margin: 0 0 0.75rem 0;\n    letter-spacing: -0.025em;\n}}\n\n.path-subtitle {{\n    font-size: 1.125rem;\n    color: #64748b;\n    margin: 0 0 2rem 0;\n    font-weight: 400;\n}}\n\n.path-description {{\n    font-size: 1rem;\n    color: #475569;\n    line-height: 1.7;\n    max-width: 700px;\n    margin: 0 auto;\n}}\n\n.path-meta {{\n    display: grid;\n    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));\n    gap: 1.5rem;\n    margin: 3rem 0;\n}}\n\n.meta-card {{\n    background: #ffffff;\n    border: 1px solid #e2e8f0;\n    border-radius: 0.75rem;\n    padding: 1.5rem;\n    text-align: left;\n}}\n\n.meta-card h3 {{\n    font-family: 'Plus Jakarta Sans', sans-serif;\n    font-size: 0.875rem;\n    font-weight: 700;\n    text-transform: uppercase;\n    letter-spacing: 0.05em;\n    color: #64748b;\n    margin: 0 0 1rem 0;\n}}\n\n.meta-card ul {{\n    list-style: none;\n    padding: 0;\n    margin: 0;\n}}\n\n.meta-card li {{\n    font-size: 0.9375rem;\n    color: #475569;\n    padding: 0.5rem 0;\n    padding-left: 1.5rem;\n    position: relative;\n    line-height: 1.5;\n}}\n\n.meta-card li::before {{\n    content: '';\n    position: absolute;\n    left: 0;\n    top: 0.875rem;\n    width: 6px;\n    height: 6px;\n    background: {config['color']};\n    border-radius: 50%;\n}}\n\n.modules-section {{\n    margin: 3rem 0;\n}}\n\n.section-header {{\n    margin-bottom: 2rem;\n}}\n\n.section-title {{\n    font-family: 'Plus Jakarta Sans', sans-serif;\n    font-size: 1.5rem;\n    font-weight: 700;\n    color: #0f172a;\n    margin: 0 0 0.5rem 0;\n    letter-spacing: -0.025em;\n}}\n\n.section-description {{\n    font-size: 1rem;\n    color: #64748b;\n    margin: 0;\n}}\n\n.module-card {{\n    background: #ffffff;\n    border: 1px solid #e2e8f0;\n    border-radius: 0.75rem;\n    padding: 1.75rem;\n    margin-bottom: 1.5rem;\n    transition: all 0.2s ease;\n}}\n\n.module-card:hover {{\n    border-color: {config['color']};\n    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);\n    transform: translateY(-2px);\n}}\n\n.module-header {{\n    display: flex;\n    align-items: center;\n    gap: 1rem;\n    margin-bottom: 1.25rem;\n}}\n\n.module-number {{\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    width: 40px;\n    height: 40px;\n    background: linear-gradient(135deg, {config['gradient_start']} 0%, {config['gradient_end']} 100%);\n    color: #ffffff;\n    border-radius: 0.5rem;\n    font-weight: 700;\n    font-size: 1.125rem;\n    flex-shrink: 0;\n}}\n\n.module-title {{\n    font-family: 'Plus Jakarta Sans', sans-serif;\n    font-size: 1.25rem;\n    font-weight: 700;\n    color: #0f172a;\n    margin: 0;\n}}\n\n.chapter-list {{\n    display: flex;\n    flex-direction: column;\n    gap: 0.75rem;\n}}\n\n.chapter-link {{\n    display: flex;\n    align-items: center;\n    gap: 0.75rem;\n    padding: 0.875rem 1rem;\n    background: #f8fafc;\n    border: 1px solid #e2e8f0;\n    border-radius: 0.5rem;\n    text-decoration: none;\n    color: #475569;\n    font-weight: 500;\n    font-size: 0.9375rem;\n    transition: all 0.2s ease;\n}}\n\n.chapter-link:hover {{\n    background: {config['color']}10;\n    border-color: {config['color']};\n    color: #0f172a;\n    transform: translateX(4px);\n}}\n\n.chapter-link .material-symbols-outlined {{\n    font-size: 1.25rem;\n    color: {config['color']};\n}}\n\n.module-card.coming-soon {{\n    background: #f9fafb;\n    border-style: dashed;\n    border-color: #d1d5db;\n    opacity: 0.7;\n}}\n\n.module-card.coming-soon .module-number {{\n    background: #9ca3af;\n}}\n\n.module-card.coming-soon .module-title {{\n    color: #6b7280;\n}}\n\n.coming-soon-badge {{\n    display: inline-block;\n    padding: 0.25rem 0.75rem;\n    background: #f1f5f9;\n    color: #64748b;\n    border-radius: 9999px;\n    font-size: 0.75rem;\n    font-weight: 600;\n    text-transform: uppercase;\n    letter-spacing: 0.05em;\n}}\n\n.cta-section {{\n    text-align: center;\n    padding: 3rem 0;\n    margin-top: 3rem;\n    border-top: 1px solid #f1f5f9;\n}}\n\n.cta-title {{\n    font-family: 'Plus Jakarta Sans', sans-serif;\n    font-size: 1.75rem;\n    font-weight: 700;\n    color: #0f172a;\n    margin: 0 0 1.5rem 0;\n}}\n\n.btn-start {{\n    display: inline-flex;\n    align-items: center;\n    gap: 0.5rem;\n    padding: 1rem 2rem;\n    background: linear-gradient(135deg, {config['gradient_start']} 0%, {config['gradient_end']} 100%);\n    color: #ffffff;\n    border-radius: 9999px;\n    text-decoration: none;\n    font-weight: 600;\n    font-size: 1rem;\n    transition: all 0.2s ease;\n    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);\n}}\n\n.btn-start:hover {{\n    transform: translateY(-2px);\n    box-shadow: 0 8px 20px rgba(0, 0, 0, 0.2);\n    color: #ffffff;\n}}\n\n.btn-start .material-symbols-outlined {{\n    font-size: 1.25rem;\n}}\n\n.stay-updated-section {{\n    margin-top: 4rem;\n    padding: 2rem;\n    background: linear-gradient(135deg, {config['color']}08 0%, {config['color']}03 100%);\n    border: 1px solid {config['color']}20;\n    border-radius: 1rem;\n}}\n\n.stay-updated-content {{\n    display: flex;\n    align-items: center;\n    gap: 1.5rem;\n    flex-wrap: wrap;\n}}\n\n.stay-updated-icon {{\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    width: 48px;\n    height: 48px;\n    background: {config['color']};\n    border-radius: 0.75rem;\n    flex-shrink: 0;\n}}\n\n.stay-updated-icon .material-symbols-outlined {{\n    font-size: 1.5rem;\n    color: #ffffff;\n}}\n\n.stay-updated-text {{\n    flex: 1;\n    min-width: 200px;\n}}\n\n.stay-updated-text h3 {{\n    font-family: 'Plus Jakarta Sans', sans-serif;\n    font-size: 1.125rem;\n    font-weight: 700;\n    color: #0f172a;\n    margin: 0 0 0.25rem 0;\n}}\n\n.stay-updated-text p {{\n    font-size: 0.9375rem;\n    color: #64748b;\n    margin: 0;\n    line-height: 1.5;\n}}\n\n.btn-github {{\n    display: inline-flex;\n    align-items: center;\n    gap: 0.5rem;\n    padding: 0.75rem 1.5rem;\n    background: #24292e;\n    color: #ffffff;\n    border-radius: 0.5rem;\n    text-decoration: none;\n    font-weight: 600;\n    font-size: 0.9375rem;\n    transition: all 0.2s ease;\n    white-space: nowrap;\n}}\n\n.btn-github:hover {{\n    background: #1b1f23;\n    transform: translateY(-2px);\n    box-shadow: 0 4px 12px rgba(36, 41, 46, 0.3);\n    color: #ffffff;\n}}\n\n.btn-github svg {{\n    flex-shrink: 0;\n}}\n\n.stay-updated-section.coming-soon-variant {{\n    background: linear-gradient(135deg, #f1f5f908 0%, #e2e8f003 100%);\n    border-color: #e2e8f0;\n}}\n\n.stay-updated-section.coming-soon-variant .stay-updated-icon {{\n    background: #64748b;\n}}\n\n@media (max-width: 768px) {{\n    .path-hero {{\n        padding: 3rem 0 2rem;\n    }}\n    \n    .path-title {{\n        font-size: 2rem;\n    }}\n    \n    .path-subtitle {{\n        font-size: 1rem;\n    }}\n    \n    .path-meta {{\n        grid-template-columns: 1fr;\n        gap: 1rem;\n    }}\n    \n    .module-header {{\n        flex-direction: column;\n        align-items: flex-start;\n    }}\n    \n    .stay-updated-content {{\n        flex-direction: column;\n        align-items: flex-start;\n        text-align: left;\n    }}\n    \n    .btn-github {{\n        width: 100%;\n        justify-content: center;\n    }}\n}}\n&lt;/style&gt;\n\n&lt;div class=\"path-hero\"&gt;\n    &lt;div class=\"path-icon\"&gt;\n        &lt;span class=\"material-symbols-outlined\"&gt;{config['icon']}&lt;/span&gt;\n    &lt;/div&gt;\n    &lt;h1 class=\"path-title\"&gt;{config['title']}&lt;/h1&gt;\n    &lt;p class=\"path-subtitle\"&gt;{config['subtitle']}&lt;/p&gt;\n    &lt;p class=\"path-description\"&gt;{config['description']}&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;div class=\"path-meta\"&gt;\n    &lt;div class=\"meta-card\"&gt;\n        &lt;h3&gt;Prerequisites&lt;/h3&gt;\n        &lt;ul&gt;\n\"\"\"\n    \n    for prereq in config['prerequisites']:\n        md += f\"            &lt;li&gt;{prereq}&lt;/li&gt;\\n\"\n    \n    md += f\"\"\"        &lt;/ul&gt;\n    &lt;/div&gt;\n    \n    &lt;div class=\"meta-card\"&gt;\n        &lt;h3&gt;What You'll Learn&lt;/h3&gt;\n        &lt;ul&gt;\n\"\"\"\n    \n    for outcome in config['outcomes']:\n        md += f\"            &lt;li&gt;{outcome}&lt;/li&gt;\\n\"\n    \n    md += \"\"\"        &lt;/ul&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div class=\"modules-section\"&gt;\n    &lt;div class=\"section-header\"&gt;\n        &lt;h2 class=\"section-title\"&gt;Learning Modules&lt;/h2&gt;\n        &lt;p class=\"section-description\"&gt;Progressive curriculum designed to take you from fundamentals to mastery&lt;/p&gt;\n    &lt;/div&gt;\n    \n\"\"\"\n    \n    # Add modules and chapters\n    for idx, module in enumerate(modules, 1):\n        has_chapters = len(module['chapters']) &gt; 0\n        \n        if has_chapters:\n            md += f\"\"\"    &lt;div class=\"module-card\"&gt;\n        &lt;div class=\"module-header\"&gt;\n            &lt;div class=\"module-number\"&gt;{idx}&lt;/div&gt;\n            &lt;h3 class=\"module-title\"&gt;{module['name']}&lt;/h3&gt;\n        &lt;/div&gt;\n        &lt;div class=\"chapter-list\"&gt;\n\"\"\"\n            \n            for chapter in module['chapters']:\n                md += f\"\"\"            &lt;a href=\"{chapter['link']}\" class=\"chapter-link\"&gt;\n                &lt;span class=\"material-symbols-outlined\"&gt;play_circle&lt;/span&gt;\n                &lt;span&gt;{chapter['title']}&lt;/span&gt;\n            &lt;/a&gt;\n\"\"\"\n            \n            md += \"\"\"        &lt;/div&gt;\n    &lt;/div&gt;\n    \n\"\"\"\n        else:\n            # Coming soon module\n            md += f\"\"\"    &lt;div class=\"module-card coming-soon\"&gt;\n        &lt;div class=\"module-header\"&gt;\n            &lt;div class=\"module-number\"&gt;{idx}&lt;/div&gt;\n            &lt;h3 class=\"module-title\"&gt;{module['name']}&lt;/h3&gt;\n        &lt;/div&gt;\n        &lt;span class=\"coming-soon-badge\"&gt;Coming Soon&lt;/span&gt;\n    &lt;/div&gt;\n    \n\"\"\"\n    \n    # Add call to action\n    if first_chapter_link:\n        md += f\"\"\"&lt;/div&gt;\n\n&lt;div class=\"cta-section\"&gt;\n    &lt;h2 class=\"cta-title\"&gt;Ready to Begin?&lt;/h2&gt;\n    &lt;a href=\"{first_chapter_link}\" class=\"btn-start\"&gt;\n        Start Learning\n        &lt;span class=\"material-symbols-outlined\"&gt;arrow_forward&lt;/span&gt;\n    &lt;/a&gt;\n&lt;/div&gt;\n\n&lt;div class=\"stay-updated-section\"&gt;\n    &lt;div class=\"stay-updated-content\"&gt;\n        &lt;div class=\"stay-updated-icon\"&gt;\n            &lt;span class=\"material-symbols-outlined\"&gt;notifications_active&lt;/span&gt;\n        &lt;/div&gt;\n        &lt;div class=\"stay-updated-text\"&gt;\n            &lt;h3&gt;Stay Updated&lt;/h3&gt;\n            &lt;p&gt;Follow our GitHub repository to get notified when new modules are released&lt;/p&gt;\n        &lt;/div&gt;\n        &lt;a href=\"https://github.com/madeforai/madeforai\" target=\"_blank\" rel=\"noopener\" class=\"btn-github\"&gt;\n            &lt;svg viewBox=\"0 0 24 24\" fill=\"currentColor\" width=\"20\" height=\"20\"&gt;\n                &lt;path d=\"M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z\"/&gt;\n            &lt;/svg&gt;\n            &lt;span&gt;Star on GitHub&lt;/span&gt;\n        &lt;/a&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n    else:\n        md += \"\"\"&lt;/div&gt;\n\n&lt;div class=\"stay-updated-section coming-soon-variant\"&gt;\n    &lt;div class=\"stay-updated-content\"&gt;\n        &lt;div class=\"stay-updated-icon\"&gt;\n            &lt;span class=\"material-symbols-outlined\"&gt;schedule&lt;/span&gt;\n        &lt;/div&gt;\n        &lt;div class=\"stay-updated-text\"&gt;\n            &lt;h3&gt;Coming Soon&lt;/h3&gt;\n            &lt;p&gt;This learning path is under development. Follow our GitHub to get notified when modules are released&lt;/p&gt;\n        &lt;/div&gt;\n        &lt;a href=\"https://github.com/madeforai/madeforai\" target=\"_blank\" rel=\"noopener\" class=\"btn-github\"&gt;\n            &lt;svg viewBox=\"0 0 24 24\" fill=\"currentColor\" width=\"20\" height=\"20\"&gt;\n                &lt;path d=\"M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z\"/&gt;\n            &lt;/svg&gt;\n            &lt;span&gt;Follow on GitHub&lt;/span&gt;\n        &lt;/a&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n    \n    return md\n</pre> def generate_markdown(config, modules, first_chapter_link):     \"\"\"Generate beautiful markdown content matching homepage design\"\"\"          md = f\"\"\"--- title: {config['title']} description: {config['subtitle']} ---   {config['icon']} {config['title']} <p>{config['subtitle']}</p> <p>{config['description']}</p> Prerequisites <ul> \"\"\"          for prereq in config['prerequisites']:         md += f\"            <li>{prereq}</li>\\n\"          md += f\"\"\"        </ul> What You'll Learn <ul> \"\"\"          for outcome in config['outcomes']:         md += f\"            <li>{outcome}</li>\\n\"          md += \"\"\"        </ul> Learning Modules <p>Progressive curriculum designed to take you from fundamentals to mastery</p>       \"\"\"          # Add modules and chapters     for idx, module in enumerate(modules, 1):         has_chapters = len(module['chapters']) &gt; 0                  if has_chapters:             md += f\"\"\"     {idx} {module['name']}  \"\"\"                          for chapter in module['chapters']:                 md += f\"\"\"             play_circle {chapter['title']}  \"\"\"                          md += \"\"\"               \"\"\"         else:             # Coming soon module             md += f\"\"\"     {idx} {module['name']} Coming Soon       \"\"\"          # Add call to action     if first_chapter_link:         md += f\"\"\" Ready to Begin?          Start Learning         arrow_forward notifications_active Stay Updated <p>Follow our GitHub repository to get notified when new modules are released</p> Star on GitHub  \"\"\"     else:         md += \"\"\" schedule Coming Soon <p>This learning path is under development. Follow our GitHub to get notified when modules are released</p> Follow on GitHub  \"\"\"          return md"},{"location":"paths/architecting-ai/","title":"Architecting AI","text":"architecture Architecting AI <p>Design scalable AI systems</p> <p>Learn system design patterns, infrastructure choices, and best practices for building reliable, scalable AI systems. Master the art of architecting production-grade AI solutions.</p> Prerequisites <ul> <li>System design experience</li> <li>Cloud infrastructure knowledge</li> <li>AI/ML basics</li> </ul> What You'll Learn <ul> <li>Design scalable architectures</li> <li>Choose optimal infrastructure</li> <li>Implement monitoring systems</li> <li>Optimize costs and performance</li> </ul> Learning Modules <p>Progressive curriculum designed to take you from fundamentals to mastery</p> schedule Coming Soon <p>This learning path is under development. Follow our GitHub to get notified when modules are released</p> Follow on GitHub"},{"location":"paths/business-ai/","title":"Business AI","text":"business_center Business AI <p>Lead AI strategy and transformation</p> <p>Understand AI strategy without diving into technical details. Learn to identify use cases, evaluate vendors, build teams, and navigate the business side of AI implementation.</p> Prerequisites <ul> <li>No technical background required</li> <li>Business experience helpful</li> <li>Interest in AI strategy</li> </ul> What You'll Learn <ul> <li>Understand AI/LLM landscape</li> <li>Identify high-value use cases</li> <li>Make build vs. buy decisions</li> <li>Build and manage AI teams</li> </ul> Learning Modules <p>Progressive curriculum designed to take you from fundamentals to mastery</p> schedule Coming Soon <p>This learning path is under development. Follow our GitHub to get notified when modules are released</p> Follow on GitHub"},{"location":"paths/engineering-ai/","title":"Engineering AI","text":"code Engineering AI <p>Build production-ready LLM applications</p> <p>Dive deep into transformer architecture, fine-tuning techniques, and deployment strategies. Learn to build scalable AI systems that work in production environments.</p> Prerequisites <ul> <li>Python programming experience</li> <li>Basic ML concepts</li> <li>Deep learning frameworks</li> </ul> What You'll Learn <ul> <li>Master Transformer architecture</li> <li>Fine-tune models with LoRA/QLoRA</li> <li>Build RAG systems</li> <li>Deploy LLMs to production</li> </ul> Learning Modules <p>Progressive curriculum designed to take you from fundamentals to mastery</p> 1 Transformer Architecture play_circle Transformer Architecture Ready to Begin?          Start Learning         arrow_forward notifications_active Stay Updated <p>Follow our GitHub repository to get notified when new modules are released</p> Star on GitHub"},{"location":"paths/engineering-ai/01-transformer-architecture/","title":"Deep Dive: The Transformer Architecture","text":"Deep Dive: The Transformer Architecture\u00b6              Open in Colab          <p>\ud83d\udcbb The Engineer Path | Module 1 of 7</p> <p>The Transformer architecture, introduced in the seminal paper \"Attention Is All You Need\", revolutionized natural language processing by dispensing with recurrence and convolutions entirely. Instead, it relies solely on attention mechanisms to draw global dependencies between input and output.</p> In\u00a0[\u00a0]: Copied! <pre># Install required packages (uncomment if running in Colab)\n# !pip install torch numpy matplotlib -q\n</pre> # Install required packages (uncomment if running in Colab) # !pip install torch numpy matplotlib -q In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Using device: {device}\")\nprint(\"\u2705 Setup complete!\")\n</pre> import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt import math  # Set device device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print(f\"PyTorch version: {torch.__version__}\") print(f\"Using device: {device}\") print(\"\u2705 Setup complete!\") In\u00a0[\u00a0]: Copied! <pre>def scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Compute scaled dot-product attention.\n    \n    Args:\n        Q: Queries of shape (batch, seq_len, d_k)\n        K: Keys of shape (batch, seq_len, d_k)\n        V: Values of shape (batch, seq_len, d_v)\n        mask: Optional mask for future tokens\n        \n    Returns:\n        Output and attention weights\n    \"\"\"\n    d_k = Q.size(-1)\n    \n    # Compute attention scores: QK^T / sqrt(d_k)\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n    \n    # Apply mask if provided (for decoder)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    \n    # Softmax to get attention weights\n    attention_weights = F.softmax(scores, dim=-1)\n    \n    # Multiply by values\n    output = torch.matmul(attention_weights, V)\n    \n    return output, attention_weights\n</pre> def scaled_dot_product_attention(Q, K, V, mask=None):     \"\"\"     Compute scaled dot-product attention.          Args:         Q: Queries of shape (batch, seq_len, d_k)         K: Keys of shape (batch, seq_len, d_k)         V: Values of shape (batch, seq_len, d_v)         mask: Optional mask for future tokens              Returns:         Output and attention weights     \"\"\"     d_k = Q.size(-1)          # Compute attention scores: QK^T / sqrt(d_k)     scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)          # Apply mask if provided (for decoder)     if mask is not None:         scores = scores.masked_fill(mask == 0, float('-inf'))          # Softmax to get attention weights     attention_weights = F.softmax(scores, dim=-1)          # Multiply by values     output = torch.matmul(attention_weights, V)          return output, attention_weights In\u00a0[\u00a0]: Copied! <pre># Example: Self-attention on a simple sequence\nbatch_size = 1\nseq_len = 4\nd_model = 8\n\n# Create sample input (random vectors for 4 tokens)\ntorch.manual_seed(42)\nx = torch.randn(batch_size, seq_len, d_model)\n\n# For self-attention, Q = K = V = input (or linear projections)\nQ = K = V = x\n\n# Compute attention\noutput, attn_weights = scaled_dot_product_attention(Q, K, V)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"\\nAttention weights (how much each token attends to others):\")\nprint(attn_weights.squeeze().numpy().round(3))\n</pre> # Example: Self-attention on a simple sequence batch_size = 1 seq_len = 4 d_model = 8  # Create sample input (random vectors for 4 tokens) torch.manual_seed(42) x = torch.randn(batch_size, seq_len, d_model)  # For self-attention, Q = K = V = input (or linear projections) Q = K = V = x  # Compute attention output, attn_weights = scaled_dot_product_attention(Q, K, V)  print(f\"Input shape: {x.shape}\") print(f\"Output shape: {output.shape}\") print(f\"\\nAttention weights (how much each token attends to others):\") print(attn_weights.squeeze().numpy().round(3)) In\u00a0[\u00a0]: Copied! <pre># Visualize attention weights\nplt.figure(figsize=(8, 6))\nplt.imshow(attn_weights.squeeze().numpy(), cmap='Blues', aspect='auto')\nplt.colorbar(label='Attention Weight')\nplt.xlabel('Key Position')\nplt.ylabel('Query Position')\nplt.title('Self-Attention Weights')\nplt.xticks(range(seq_len), [f'Token {i}' for i in range(seq_len)])\nplt.yticks(range(seq_len), [f'Token {i}' for i in range(seq_len)])\nplt.tight_layout()\nplt.show()\n</pre> # Visualize attention weights plt.figure(figsize=(8, 6)) plt.imshow(attn_weights.squeeze().numpy(), cmap='Blues', aspect='auto') plt.colorbar(label='Attention Weight') plt.xlabel('Key Position') plt.ylabel('Query Position') plt.title('Self-Attention Weights') plt.xticks(range(seq_len), [f'Token {i}' for i in range(seq_len)]) plt.yticks(range(seq_len), [f'Token {i}' for i in range(seq_len)]) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>class MultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Attention mechanism.\"\"\"\n    \n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        # Linear projections for Q, K, V\n        self.W_q = nn.Linear(d_model, d_model, bias=False)\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n        \n        # Output projection\n        self.W_o = nn.Linear(d_model, d_model, bias=False)\n        \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        # Linear projections\n        Q = self.W_q(Q)\n        K = self.W_k(K)\n        V = self.W_v(V)\n        \n        # Reshape to (batch, num_heads, seq_len, d_k)\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Compute attention\n        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model\n        )\n        \n        # Final linear projection\n        output = self.W_o(attn_output)\n        \n        return output, attn_weights\n</pre> class MultiHeadAttention(nn.Module):     \"\"\"Multi-Head Attention mechanism.\"\"\"          def __init__(self, d_model, num_heads):         super().__init__()         assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"                  self.d_model = d_model         self.num_heads = num_heads         self.d_k = d_model // num_heads                  # Linear projections for Q, K, V         self.W_q = nn.Linear(d_model, d_model, bias=False)         self.W_k = nn.Linear(d_model, d_model, bias=False)         self.W_v = nn.Linear(d_model, d_model, bias=False)                  # Output projection         self.W_o = nn.Linear(d_model, d_model, bias=False)              def forward(self, Q, K, V, mask=None):         batch_size = Q.size(0)                  # Linear projections         Q = self.W_q(Q)         K = self.W_k(K)         V = self.W_v(V)                  # Reshape to (batch, num_heads, seq_len, d_k)         Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)         K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)         V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)                  # Compute attention         attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)                  # Concatenate heads         attn_output = attn_output.transpose(1, 2).contiguous().view(             batch_size, -1, self.d_model         )                  # Final linear projection         output = self.W_o(attn_output)                  return output, attn_weights In\u00a0[\u00a0]: Copied! <pre># Test Multi-Head Attention\nd_model = 512\nnum_heads = 8\nseq_len = 10\nbatch_size = 2\n\nmha = MultiHeadAttention(d_model, num_heads)\nx = torch.randn(batch_size, seq_len, d_model)\n\noutput, attn_weights = mha(x, x, x)  # Self-attention\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Attention weights shape: {attn_weights.shape}\")\nprint(f\"\\n\u2705 Multi-Head Attention working!\")\n</pre> # Test Multi-Head Attention d_model = 512 num_heads = 8 seq_len = 10 batch_size = 2  mha = MultiHeadAttention(d_model, num_heads) x = torch.randn(batch_size, seq_len, d_model)  output, attn_weights = mha(x, x, x)  # Self-attention  print(f\"Input shape: {x.shape}\") print(f\"Output shape: {output.shape}\") print(f\"Attention weights shape: {attn_weights.shape}\") print(f\"\\n\u2705 Multi-Head Attention working!\") In\u00a0[\u00a0]: Copied! <pre>class PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding.\"\"\"\n    \n    def __init__(self, d_model, max_seq_len=5000):\n        super().__init__()\n        \n        # Create positional encoding matrix\n        pe = torch.zeros(max_seq_len, d_model)\n        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n        \n        # Compute the div term\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * \n            (-math.log(10000.0) / d_model)\n        )\n        \n        # Apply sin to even indices, cos to odd indices\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        # Add batch dimension and register as buffer\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        \"\"\"Add positional encoding to input.\"\"\"\n        return x + self.pe[:, :x.size(1), :]\n</pre> class PositionalEncoding(nn.Module):     \"\"\"Sinusoidal positional encoding.\"\"\"          def __init__(self, d_model, max_seq_len=5000):         super().__init__()                  # Create positional encoding matrix         pe = torch.zeros(max_seq_len, d_model)         position = torch.arange(0, max_seq_len).unsqueeze(1).float()                  # Compute the div term         div_term = torch.exp(             torch.arange(0, d_model, 2).float() *              (-math.log(10000.0) / d_model)         )                  # Apply sin to even indices, cos to odd indices         pe[:, 0::2] = torch.sin(position * div_term)         pe[:, 1::2] = torch.cos(position * div_term)                  # Add batch dimension and register as buffer         pe = pe.unsqueeze(0)         self.register_buffer('pe', pe)              def forward(self, x):         \"\"\"Add positional encoding to input.\"\"\"         return x + self.pe[:, :x.size(1), :] In\u00a0[\u00a0]: Copied! <pre># Visualize positional encoding\npe_module = PositionalEncoding(d_model=64, max_seq_len=100)\npe_matrix = pe_module.pe.squeeze().numpy()\n\nplt.figure(figsize=(12, 5))\nplt.imshow(pe_matrix[:50, :].T, cmap='RdBu', aspect='auto')\nplt.colorbar(label='Value')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\nplt.title('Positional Encoding Visualization')\nplt.tight_layout()\nplt.show()\n</pre> # Visualize positional encoding pe_module = PositionalEncoding(d_model=64, max_seq_len=100) pe_matrix = pe_module.pe.squeeze().numpy()  plt.figure(figsize=(12, 5)) plt.imshow(pe_matrix[:50, :].T, cmap='RdBu', aspect='auto') plt.colorbar(label='Value') plt.xlabel('Position') plt.ylabel('Dimension') plt.title('Positional Encoding Visualization') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>class TransformerBlock(nn.Module):\n    \"\"\"A single Transformer block.\"\"\"\n    \n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        \n        # Multi-head attention\n        self.attention = MultiHeadAttention(d_model, num_heads)\n        \n        # Feed-forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout)\n        )\n        \n        # Layer normalization\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        # Self-attention with residual connection\n        attn_output, _ = self.attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed-forward with residual connection\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + ffn_output)\n        \n        return x\n</pre> class TransformerBlock(nn.Module):     \"\"\"A single Transformer block.\"\"\"          def __init__(self, d_model, num_heads, d_ff, dropout=0.1):         super().__init__()                  # Multi-head attention         self.attention = MultiHeadAttention(d_model, num_heads)                  # Feed-forward network         self.ffn = nn.Sequential(             nn.Linear(d_model, d_ff),             nn.GELU(),             nn.Dropout(dropout),             nn.Linear(d_ff, d_model),             nn.Dropout(dropout)         )                  # Layer normalization         self.norm1 = nn.LayerNorm(d_model)         self.norm2 = nn.LayerNorm(d_model)                  # Dropout         self.dropout = nn.Dropout(dropout)              def forward(self, x, mask=None):         # Self-attention with residual connection         attn_output, _ = self.attention(x, x, x, mask)         x = self.norm1(x + self.dropout(attn_output))                  # Feed-forward with residual connection         ffn_output = self.ffn(x)         x = self.norm2(x + ffn_output)                  return x In\u00a0[\u00a0]: Copied! <pre># Test the complete Transformer block\nd_model = 512\nnum_heads = 8\nd_ff = 2048\nseq_len = 20\nbatch_size = 4\n\ntransformer_block = TransformerBlock(d_model, num_heads, d_ff)\nx = torch.randn(batch_size, seq_len, d_model)\n\noutput = transformer_block(x)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"\\n\u2705 Transformer Block working!\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in transformer_block.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\n</pre> # Test the complete Transformer block d_model = 512 num_heads = 8 d_ff = 2048 seq_len = 20 batch_size = 4  transformer_block = TransformerBlock(d_model, num_heads, d_ff) x = torch.randn(batch_size, seq_len, d_model)  output = transformer_block(x)  print(f\"Input shape: {x.shape}\") print(f\"Output shape: {output.shape}\") print(f\"\\n\u2705 Transformer Block working!\")  # Count parameters total_params = sum(p.numel() for p in transformer_block.parameters()) print(f\"\\nTotal parameters: {total_params:,}\")"},{"location":"paths/engineering-ai/01-transformer-architecture/#what-youll-learn","title":"\ud83d\udcda What You'll Learn\u00b6","text":"<ul> <li>Self-attention mechanism explained</li> <li>Multi-head attention implementation</li> <li>Positional encoding strategies</li> <li>Complete Transformer block in PyTorch</li> </ul> <p>Prerequisites: Basic understanding of vector embeddings and matrix multiplication. We'll use PyTorch for implementation.</p>"},{"location":"paths/engineering-ai/01-transformer-architecture/#setup","title":"\ud83d\udd27 Setup\u00b6","text":"<p>First, let's install and import the necessary libraries.</p>"},{"location":"paths/engineering-ai/01-transformer-architecture/#1-the-self-attention-mechanism","title":"1. The Self-Attention Mechanism\u00b6","text":"<p>At the core of the Transformer is the self-attention mechanism. It allows the model to weigh the importance of different words in a sentence when encoding a specific word.</p>"},{"location":"paths/engineering-ai/01-transformer-architecture/#the-attention-formula","title":"The Attention Formula\u00b6","text":"<p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p> <p>Where:</p> <ul> <li>Q (Query): What we're looking for</li> <li>K (Key): What we match against</li> <li>V (Value): What we retrieve</li> <li>d_k: Dimension of keys (for scaling)</li> </ul>"},{"location":"paths/engineering-ai/01-transformer-architecture/#2-multi-head-attention","title":"2. Multi-Head Attention\u00b6","text":"<p>Multi-head attention runs the attention mechanism multiple times in parallel with different learned projections. This allows the model to attend to information from different representation subspaces.</p> <p>$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$</p> <p>where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</p>"},{"location":"paths/engineering-ai/01-transformer-architecture/#3-positional-encoding","title":"3. Positional Encoding\u00b6","text":"<p>Since transformers process all positions in parallel, we need to inject information about token positions. We use sinusoidal positional encoding:</p> <p>$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$ $$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$</p>"},{"location":"paths/engineering-ai/01-transformer-architecture/#4-complete-transformer-block","title":"4. Complete Transformer Block\u00b6","text":"<p>A Transformer block combines:</p> <ol> <li>Multi-Head Self-Attention</li> <li>Layer Normalization</li> <li>Feed-Forward Network</li> <li>Residual Connections</li> </ol>"},{"location":"paths/engineering-ai/01-transformer-architecture/#key-takeaways","title":"\ud83c\udfaf Key Takeaways\u00b6","text":"<ol> <li>Self-Attention lets each token attend to all other tokens in parallel</li> <li>Multi-Head Attention learns multiple attention patterns simultaneously</li> <li>Positional Encoding injects sequence order information</li> <li>Transformer Blocks combine attention, FFN, and residual connections</li> </ol> <p>\ud83d\udca1 Optimization Tip: When running on production data, always ensure your input tensors are on the GPU by calling <code>.to('cuda')</code> on your model and data.</p>"},{"location":"paths/engineering-ai/01-transformer-architecture/#next-steps","title":"\ud83d\udcd6 Next Steps\u00b6","text":"<p>Continue to Module 2: Tokenization &amp; Embeddings to learn how text is converted to the vectors that Transformers process.</p> <p>\u00a9 2026 MadeForAI. Learn more at madeforai.github.io</p>"},{"location":"paths/researching-ai/","title":"Researching AI","text":"biotech Researching AI <p>Push the boundaries of AI innovation</p> <p>Explore cutting-edge research, learn to read and implement papers, and contribute to the field. Develop the skills to design novel architectures and conduct rigorous experiments.</p> Prerequisites <ul> <li>Strong mathematical background</li> <li>Deep learning fundamentals</li> <li>Research experience helpful</li> </ul> What You'll Learn <ul> <li>Read and implement papers</li> <li>Design novel architectures</li> <li>Conduct rigorous experiments</li> <li>Contribute to open-source research</li> </ul> Learning Modules <p>Progressive curriculum designed to take you from fundamentals to mastery</p> schedule Coming Soon <p>This learning path is under development. Follow our GitHub to get notified when modules are released</p> Follow on GitHub"},{"location":"paths/understanding-ai/","title":"Understanding AI","text":"school Understanding AI <p>Build a solid foundation in AI fundamentals</p> <p>Master the core concepts of artificial intelligence, machine learning, and deep learning. Build your first models and understand how modern AI systems work through hands-on interactive lessons.</p> Prerequisites <ul> <li>Basic Python programming</li> <li>High school mathematics</li> <li>Curiosity about AI</li> </ul> What You'll Learn <ul> <li>Understand AI, ML, and Deep Learning</li> <li>Build neural networks from scratch</li> <li>Work with modern LLMs</li> <li>Master prompt engineering basics</li> </ul> Learning Modules <p>Progressive curriculum designed to take you from fundamentals to mastery</p> 1 Foundations play_circle Introduction to AI, ML &amp; Deep Learning play_circle Your First Neural Network Ready to Begin?          Start Learning         arrow_forward notifications_active Stay Updated <p>Follow our GitHub repository to get notified when new modules are released</p> Star on GitHub"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/","title":"Chapter 1: Introduction to AI, ML, and Deep Learning","text":"Introduction to AI, ML, and Deep Learning\u00b6              Open in Colab          <p>Welcome to your first step in the AI journey! In this chapter, we'll explore the foundations of Artificial Intelligence, Machine Learning, and Deep Learning. You'll understand how these concepts relate to each other and build your first simple ML model.</p> In\u00a0[\u00a0]: Copied! <pre># Install required packages (uncomment if running in Colab)\n# !pip install numpy matplotlib scikit-learn -q\n</pre> # Install required packages (uncomment if running in Colab) # !pip install numpy matplotlib scikit-learn -q In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Set style for better-looking plots\nplt.style.use('seaborn-v0_8-darkgrid')\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 11\n\nprint(f\"NumPy version: {np.__version__}\")\nprint(\"Setup complete!\")\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  # Set style for better-looking plots plt.style.use('seaborn-v0_8-darkgrid') plt.rcParams['figure.figsize'] = (10, 6) plt.rcParams['font.size'] = 11  print(f\"NumPy version: {np.__version__}\") print(\"Setup complete!\") In\u00a0[\u00a0]: Copied! <pre># Let's visualize the ML paradigms\nparadigms = {\n    ' Supervised Learning': {\n        'description': 'Learning from labeled data (input \u2192 output pairs)',\n        'examples': ['Classification', 'Regression'],\n        'use_cases': ['Spam detection', 'House price prediction', 'Image recognition']\n    },\n    ' Unsupervised Learning': {\n        'description': 'Finding patterns in unlabeled data',\n        'examples': ['Clustering', 'Dimensionality Reduction'],\n        'use_cases': ['Customer segmentation', 'Anomaly detection', 'Data compression']\n    },\n    ' Reinforcement Learning': {\n        'description': 'Learning through trial and error with rewards',\n        'examples': ['Q-Learning', 'Policy Gradients'],\n        'use_cases': ['Game playing (AlphaGo)', 'Robotics', 'Self-driving cars']\n    }\n}\n\nfor paradigm, info in paradigms.items():\n    print(f\"\\n{paradigm}\")\n    print(f\"   {info['description']}\")\n    print(f\"   Examples: {', '.join(info['examples'])}\")\n    print(f\"   Use Cases: {', '.join(info['use_cases'])}\")\n</pre> # Let's visualize the ML paradigms paradigms = {     ' Supervised Learning': {         'description': 'Learning from labeled data (input \u2192 output pairs)',         'examples': ['Classification', 'Regression'],         'use_cases': ['Spam detection', 'House price prediction', 'Image recognition']     },     ' Unsupervised Learning': {         'description': 'Finding patterns in unlabeled data',         'examples': ['Clustering', 'Dimensionality Reduction'],         'use_cases': ['Customer segmentation', 'Anomaly detection', 'Data compression']     },     ' Reinforcement Learning': {         'description': 'Learning through trial and error with rewards',         'examples': ['Q-Learning', 'Policy Gradients'],         'use_cases': ['Game playing (AlphaGo)', 'Robotics', 'Self-driving cars']     } }  for paradigm, info in paradigms.items():     print(f\"\\n{paradigm}\")     print(f\"   {info['description']}\")     print(f\"   Examples: {', '.join(info['examples'])}\")     print(f\"   Use Cases: {', '.join(info['use_cases'])}\") In\u00a0[\u00a0]: Copied! <pre># Generate synthetic house data\nnp.random.seed(42)\n\n# House sizes (square feet)\nsizes = np.random.randint(500, 3500, 100).reshape(-1, 1)\n\n# Prices (with some realistic noise)\n# True relationship: Price = 150 * Size + 50000 + noise\nprices = 150 * sizes + 50000 + np.random.randn(100, 1) * 20000\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    sizes, prices, test_size=0.2, random_state=42\n)\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")\nprint(f\"\\nSample data:\")\nprint(f\"Size: {X_train[0][0]:.0f} sq ft \u2192 Price: ${y_train[0][0]:,.0f}\")\n</pre> # Generate synthetic house data np.random.seed(42)  # House sizes (square feet) sizes = np.random.randint(500, 3500, 100).reshape(-1, 1)  # Prices (with some realistic noise) # True relationship: Price = 150 * Size + 50000 + noise prices = 150 * sizes + 50000 + np.random.randn(100, 1) * 20000  # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(     sizes, prices, test_size=0.2, random_state=42 )  print(f\"Training samples: {len(X_train)}\") print(f\"Testing samples: {len(X_test)}\") print(f\"\\nSample data:\") print(f\"Size: {X_train[0][0]:.0f} sq ft \u2192 Price: ${y_train[0][0]:,.0f}\") In\u00a0[\u00a0]: Copied! <pre># Visualize the data\nplt.figure(figsize=(12, 6))\n\nplt.scatter(X_train, y_train, alpha=0.6, c='#3b82f6', \n           edgecolors='white', s=80, label='Training Data')\nplt.scatter(X_test, y_test, alpha=0.6, c='#f59e0b', \n           edgecolors='white', s=80, label='Test Data')\n\nplt.xlabel('House Size (sq ft)', fontsize=13, fontweight='bold')\nplt.ylabel('Price ($)', fontsize=13, fontweight='bold')\nplt.title('House Prices vs Size', fontsize=15, fontweight='bold', pad=20)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\" Our goal: Find the best line that fits this data!\")\n</pre> # Visualize the data plt.figure(figsize=(12, 6))  plt.scatter(X_train, y_train, alpha=0.6, c='#3b82f6',             edgecolors='white', s=80, label='Training Data') plt.scatter(X_test, y_test, alpha=0.6, c='#f59e0b',             edgecolors='white', s=80, label='Test Data')  plt.xlabel('House Size (sq ft)', fontsize=13, fontweight='bold') plt.ylabel('Price ($)', fontsize=13, fontweight='bold') plt.title('House Prices vs Size', fontsize=15, fontweight='bold', pad=20) plt.legend(fontsize=11) plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  print(\" Our goal: Find the best line that fits this data!\") In\u00a0[\u00a0]: Copied! <pre># Create and train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Get the learned parameters\nslope = model.coef_[0][0]\nintercept = model.intercept_[0]\n\nprint(\" Model Training Complete!\\n\")\nprint(f\"Learned equation: Price = {slope:.2f} \u00d7 Size + {intercept:,.0f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"  \u2022 Base price: ${intercept:,.0f}\")\nprint(f\"  \u2022 Price per sq ft: ${slope:.2f}\")\nprint(f\"\\n This means each additional square foot adds ${slope:.2f} to the price!\")\n</pre> # Create and train the model model = LinearRegression() model.fit(X_train, y_train)  # Get the learned parameters slope = model.coef_[0][0] intercept = model.intercept_[0]  print(\" Model Training Complete!\\n\") print(f\"Learned equation: Price = {slope:.2f} \u00d7 Size + {intercept:,.0f}\") print(f\"\\nInterpretation:\") print(f\"  \u2022 Base price: ${intercept:,.0f}\") print(f\"  \u2022 Price per sq ft: ${slope:.2f}\") print(f\"\\n This means each additional square foot adds ${slope:.2f} to the price!\") In\u00a0[\u00a0]: Copied! <pre># Evaluate the model\ntrain_score = model.score(X_train, y_train)\ntest_score = model.score(X_test, y_test)\n\nprint(f\"Model Performance (R\u00b2 Score):\")\nprint(f\"  Training: {train_score:.4f}\")\nprint(f\"  Testing:  {test_score:.4f}\")\nprint(f\"\\n A score close to 1.0 means excellent predictions!\")\n</pre> # Evaluate the model train_score = model.score(X_train, y_train) test_score = model.score(X_test, y_test)  print(f\"Model Performance (R\u00b2 Score):\") print(f\"  Training: {train_score:.4f}\") print(f\"  Testing:  {test_score:.4f}\") print(f\"\\n A score close to 1.0 means excellent predictions!\") In\u00a0[\u00a0]: Copied! <pre># Visualize predictions\nplt.figure(figsize=(12, 6))\n\n# Plot data points\nplt.scatter(X_train, y_train, alpha=0.6, c='#3b82f6', \n           edgecolors='white', s=80, label='Training Data')\nplt.scatter(X_test, y_test, alpha=0.6, c='#f59e0b', \n           edgecolors='white', s=80, label='Test Data')\n\n# Plot the learned line\nX_line = np.linspace(sizes.min(), sizes.max(), 100).reshape(-1, 1)\ny_line = model.predict(X_line)\nplt.plot(X_line, y_line, 'r-', linewidth=3, label='Learned Model', alpha=0.8)\n\nplt.xlabel('House Size (sq ft)', fontsize=13, fontweight='bold')\nplt.ylabel('Price ($)', fontsize=13, fontweight='bold')\nplt.title('Linear Regression: Predictions vs Actual', fontsize=15, fontweight='bold', pad=20)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</pre> # Visualize predictions plt.figure(figsize=(12, 6))  # Plot data points plt.scatter(X_train, y_train, alpha=0.6, c='#3b82f6',             edgecolors='white', s=80, label='Training Data') plt.scatter(X_test, y_test, alpha=0.6, c='#f59e0b',             edgecolors='white', s=80, label='Test Data')  # Plot the learned line X_line = np.linspace(sizes.min(), sizes.max(), 100).reshape(-1, 1) y_line = model.predict(X_line) plt.plot(X_line, y_line, 'r-', linewidth=3, label='Learned Model', alpha=0.8)  plt.xlabel('House Size (sq ft)', fontsize=13, fontweight='bold') plt.ylabel('Price ($)', fontsize=13, fontweight='bold') plt.title('Linear Regression: Predictions vs Actual', fontsize=15, fontweight='bold', pad=20) plt.legend(fontsize=11) plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Predict prices for new houses\nnew_houses = np.array([[1000], [1500], [2000], [2500], [3000]])\npredicted_prices = model.predict(new_houses)\n\nprint(\" Price Predictions for New Houses:\\n\")\nprint(\"Size (sq ft)  \u2192  Predicted Price\")\nprint(\"\" * 40)\nfor size, price in zip(new_houses, predicted_prices):\n    print(f\"{size[0]:&gt;6,} sq ft  \u2192  ${price[0]:&gt;12,.0f}\")\n</pre> # Predict prices for new houses new_houses = np.array([[1000], [1500], [2000], [2500], [3000]]) predicted_prices = model.predict(new_houses)  print(\" Price Predictions for New Houses:\\n\") print(\"Size (sq ft)  \u2192  Predicted Price\") print(\"\" * 40) for size, price in zip(new_houses, predicted_prices):     print(f\"{size[0]:&gt;6,} sq ft  \u2192  ${price[0]:&gt;12,.0f}\")"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#what-youll-learn","title":"What You'll Learn\u00b6","text":"<ul> <li>What is Artificial Intelligence?</li> <li>The relationship between AI, ML, and DL</li> <li>Key ML paradigms (Supervised, Unsupervised, Reinforcement)</li> <li>Build a simple linear regression model</li> <li>The GenAI landscape in 2026</li> </ul>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#setup","title":"Setup\u00b6","text":"<p>First, let's install and import the libraries we'll need.</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#1-what-is-artificial-intelligence","title":"1. What is Artificial Intelligence?\u00b6","text":"<p>Artificial Intelligence (AI) is the field of computer science focused on creating systems that can perform tasks that typically require human intelligence.</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#the-ai-hierarchy","title":"The AI Hierarchy\u00b6","text":"<p>Think of AI as a set of nested concepts:</p> <pre><code>\n   Artificial Intelligence (AI)       \u2190 Broad field: Any intelligent behavior\n    \n     Machine Learning (ML)           \u2190 Learning from data\n        \n       Deep Learning (DL)          \u2190 Neural networks with many layers\n            \n        GenAI / LLMs             \u2190 Generative models (GPT, Claude, etc.)\n            \n        \n    \n\n</code></pre> <p>Key Insight: All Deep Learning is Machine Learning, all Machine Learning is AI, but not all AI is Machine Learning!</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#2-machine-learning-paradigms","title":"2. Machine Learning Paradigms\u00b6","text":"<p>Machine Learning can be categorized into three main paradigms based on how the model learns:</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#3-your-first-ml-model-linear-regression","title":"3. Your First ML Model: Linear Regression\u00b6","text":"<p>Let's build a simple supervised learning model to predict house prices based on size. This is a classic regression problem.</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#the-problem","title":"The Problem\u00b6","text":"<p>Given the size of a house (in square feet), can we predict its price?</p> <p>We'll use the formula: Price = m \u00d7 Size + b</p> <p>Where:</p> <ul> <li>m = slope (how much price increases per square foot)</li> <li>b = intercept (base price)</li> </ul>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#training-the-model","title":"Training the Model\u00b6","text":"<p>Now let's train a linear regression model to learn the relationship between size and price.</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#making-predictions","title":"Making Predictions\u00b6","text":"<p>Now let's use our trained model to predict prices for new houses!</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#4-from-ml-to-deep-learning","title":"4. From ML to Deep Learning\u00b6","text":"<p>Linear regression is simple, but what if the relationship isn't linear? That's where Deep Learning comes in!</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#key-differences","title":"Key Differences\u00b6","text":"Aspect Traditional ML Deep Learning Model Simple (linear, trees) Neural networks with many layers Features Manual engineering Automatic learning Data Needed Works with small data Needs large datasets Complexity Simple patterns Complex patterns Examples Linear regression, SVM CNNs, Transformers, LLMs"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#neural-network-visualization","title":"Neural Network Visualization\u00b6","text":"<pre><code>Input Layer    Hidden Layers    Output Layer\n                                    \n               \n                                          \n                       \n</code></pre> <p>Each connection has a weight that the network learns during training!</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#5-the-genai-landscape-2026","title":"5. The GenAI Landscape (2026)\u00b6","text":"<p>Today's Generative AI ecosystem is built on deep learning, specifically Transformer architectures.</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#popular-models","title":"Popular Models\u00b6","text":"Category Examples Key Features Large Language Models GPT-4, Claude, Gemini Text generation, reasoning, coding Small Language Models Phi-3, Gemma, Qwen Efficient, edge-deployable Multimodal Models GPT-4V, Gemini Pro Vision + Language Code Models Codex, StarCoder, CodeLlama Specialized for programming Open Source Llama 3, Mistral, Mixtral Community-driven, customizable"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#how-llms-work-simplified","title":"How LLMs Work (Simplified)\u00b6","text":"<ol> <li>Training: Learn patterns from billions of text examples</li> <li>Tokenization: Break text into pieces (tokens)</li> <li>Prediction: Predict the next token based on context</li> <li>Generation: Repeat to generate full responses</li> </ol> <p>We'll dive deeper into these concepts in later modules!</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>AI is the broad field; ML learns from data; DL uses neural networks</li> <li>Supervised learning uses labeled data (like our house price example)</li> <li>The core ML loop: Data \u2192 Model \u2192 Training \u2192 Predictions</li> <li>Deep Learning adds layers and complexity for harder problems</li> <li>LLMs are the frontier of GenAI, built on transformer architectures</li> </ol>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#next-steps","title":"Next Steps\u00b6","text":"<p>Continue to Chapter 2: Your First Neural Network to build a real neural network from scratch!</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-1-ai-ml-intro/#practice-exercise","title":"Practice Exercise\u00b6","text":"<p>Try modifying the code above to:</p> <ol> <li>Change the relationship between size and price</li> <li>Add more features (e.g., number of bedrooms)</li> <li>Experiment with different train/test splits</li> </ol> <p>\u00a9 2026 MadeForAI. Learn more at madeforai.github.io</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/","title":"Chapter 2: Your First Neural Network","text":"Your First Neural Network\u00b6              Open in Colab          <p>In the previous chapter, we built a simple linear model. Now, let's level up and build a real neural network from scratch! You'll understand how neurons work, how networks learn through backpropagation, and build a classifier for handwritten digits.</p> In\u00a0[\u00a0]: Copied! <pre># Install required packages (uncomment if running in Colab)\n# !pip install numpy matplotlib scikit-learn -q\n</pre> # Install required packages (uncomment if running in Colab) # !pip install numpy matplotlib scikit-learn -q In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Set style\nplt.style.use('seaborn-v0_8-darkgrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\" Setup complete!\")\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  # Set style plt.style.use('seaborn-v0_8-darkgrid') plt.rcParams['figure.figsize'] = (12, 6)  print(\" Setup complete!\") In\u00a0[\u00a0]: Copied! <pre>def sigmoid(x):\n    \"\"\"Sigmoid: Squashes values between 0 and 1\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef relu(x):\n    \"\"\"ReLU: Returns max(0, x)\"\"\"\n    return np.maximum(0, x)\n\ndef tanh(x):\n    \"\"\"Tanh: Squashes values between -1 and 1\"\"\"\n    return np.tanh(x)\n\n# Visualize activation functions\nx = np.linspace(-5, 5, 100)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Sigmoid\naxes[0].plot(x, sigmoid(x), 'b-', linewidth=2.5)\naxes[0].set_title('Sigmoid', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('\u03c3(x)')\naxes[0].grid(True, alpha=0.3)\naxes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n\n# ReLU\naxes[1].plot(x, relu(x), 'g-', linewidth=2.5)\naxes[1].set_title('ReLU', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('ReLU(x)')\naxes[1].grid(True, alpha=0.3)\naxes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n\n# Tanh\naxes[2].plot(x, tanh(x), 'r-', linewidth=2.5)\naxes[2].set_title('Tanh', fontsize=14, fontweight='bold')\naxes[2].set_xlabel('x')\naxes[2].set_ylabel('tanh(x)')\naxes[2].grid(True, alpha=0.3)\naxes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\" Activation Functions:\")\nprint(\"  \u2022 Sigmoid: Good for binary classification (0-1 output)\")\nprint(\"  \u2022 ReLU: Most popular for hidden layers (fast, simple)\")\nprint(\"  \u2022 Tanh: Similar to sigmoid but centered at 0\")\n</pre> def sigmoid(x):     \"\"\"Sigmoid: Squashes values between 0 and 1\"\"\"     return 1 / (1 + np.exp(-x))  def relu(x):     \"\"\"ReLU: Returns max(0, x)\"\"\"     return np.maximum(0, x)  def tanh(x):     \"\"\"Tanh: Squashes values between -1 and 1\"\"\"     return np.tanh(x)  # Visualize activation functions x = np.linspace(-5, 5, 100)  fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # Sigmoid axes[0].plot(x, sigmoid(x), 'b-', linewidth=2.5) axes[0].set_title('Sigmoid', fontsize=14, fontweight='bold') axes[0].set_xlabel('x') axes[0].set_ylabel('\u03c3(x)') axes[0].grid(True, alpha=0.3) axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3) axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)  # ReLU axes[1].plot(x, relu(x), 'g-', linewidth=2.5) axes[1].set_title('ReLU', fontsize=14, fontweight='bold') axes[1].set_xlabel('x') axes[1].set_ylabel('ReLU(x)') axes[1].grid(True, alpha=0.3) axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3) axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)  # Tanh axes[2].plot(x, tanh(x), 'r-', linewidth=2.5) axes[2].set_title('Tanh', fontsize=14, fontweight='bold') axes[2].set_xlabel('x') axes[2].set_ylabel('tanh(x)') axes[2].grid(True, alpha=0.3) axes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3) axes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)  plt.tight_layout() plt.show()  print(\" Activation Functions:\") print(\"  \u2022 Sigmoid: Good for binary classification (0-1 output)\") print(\"  \u2022 ReLU: Most popular for hidden layers (fast, simple)\") print(\"  \u2022 Tanh: Similar to sigmoid but centered at 0\") In\u00a0[\u00a0]: Copied! <pre>class SimpleNeuralNetwork:\n    \"\"\"A simple 2-layer neural network\"\"\"\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        # Initialize weights randomly\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n        self.b2 = np.zeros((1, output_size))\n        \n    def forward(self, X):\n        \"\"\"Forward pass through the network\"\"\"\n        # Hidden layer\n        self.z1 = np.dot(X, self.W1) + self.b1\n        self.a1 = relu(self.z1)\n        \n        # Output layer\n        self.z2 = np.dot(self.a1, self.W2) + self.b2\n        self.a2 = self.softmax(self.z2)\n        \n        return self.a2\n    \n    def softmax(self, x):\n        \"\"\"Softmax activation for multi-class classification\"\"\"\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    \n    def backward(self, X, y, learning_rate=0.01):\n        \"\"\"Backward pass (gradient descent)\"\"\"\n        m = X.shape[0]\n        \n        # Output layer gradients\n        dz2 = self.a2 - y\n        dW2 = np.dot(self.a1.T, dz2) / m\n        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n        \n        # Hidden layer gradients\n        dz1 = np.dot(dz2, self.W2.T) * (self.z1 &gt; 0)  # ReLU derivative\n        dW1 = np.dot(X.T, dz1) / m\n        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n        \n        # Update weights\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1\n    \n    def compute_loss(self, y_true, y_pred):\n        \"\"\"Cross-entropy loss\"\"\"\n        m = y_true.shape[0]\n        log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])\n        return np.sum(log_likelihood) / m\n    \n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        output = self.forward(X)\n        return np.argmax(output, axis=1)\n\nprint(\" Neural Network class defined!\")\n</pre> class SimpleNeuralNetwork:     \"\"\"A simple 2-layer neural network\"\"\"          def __init__(self, input_size, hidden_size, output_size):         # Initialize weights randomly         self.W1 = np.random.randn(input_size, hidden_size) * 0.01         self.b1 = np.zeros((1, hidden_size))         self.W2 = np.random.randn(hidden_size, output_size) * 0.01         self.b2 = np.zeros((1, output_size))              def forward(self, X):         \"\"\"Forward pass through the network\"\"\"         # Hidden layer         self.z1 = np.dot(X, self.W1) + self.b1         self.a1 = relu(self.z1)                  # Output layer         self.z2 = np.dot(self.a1, self.W2) + self.b2         self.a2 = self.softmax(self.z2)                  return self.a2          def softmax(self, x):         \"\"\"Softmax activation for multi-class classification\"\"\"         exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))         return exp_x / np.sum(exp_x, axis=1, keepdims=True)          def backward(self, X, y, learning_rate=0.01):         \"\"\"Backward pass (gradient descent)\"\"\"         m = X.shape[0]                  # Output layer gradients         dz2 = self.a2 - y         dW2 = np.dot(self.a1.T, dz2) / m         db2 = np.sum(dz2, axis=0, keepdims=True) / m                  # Hidden layer gradients         dz1 = np.dot(dz2, self.W2.T) * (self.z1 &gt; 0)  # ReLU derivative         dW1 = np.dot(X.T, dz1) / m         db1 = np.sum(dz1, axis=0, keepdims=True) / m                  # Update weights         self.W2 -= learning_rate * dW2         self.b2 -= learning_rate * db2         self.W1 -= learning_rate * dW1         self.b1 -= learning_rate * db1          def compute_loss(self, y_true, y_pred):         \"\"\"Cross-entropy loss\"\"\"         m = y_true.shape[0]         log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])         return np.sum(log_likelihood) / m          def predict(self, X):         \"\"\"Make predictions\"\"\"         output = self.forward(X)         return np.argmax(output, axis=1)  print(\" Neural Network class defined!\") In\u00a0[\u00a0]: Copied! <pre># Load digits dataset (8x8 images)\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n# Normalize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# One-hot encode labels\ny_onehot = np.eye(10)[y]\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_onehot, test_size=0.2, random_state=42\n)\n\nprint(f\"Dataset loaded!\")\nprint(f\"  Training samples: {X_train.shape[0]}\")\nprint(f\"  Test samples: {X_test.shape[0]}\")\nprint(f\"  Input features: {X_train.shape[1]} (8x8 pixels)\")\nprint(f\"  Output classes: {y_train.shape[1]} (digits 0-9)\")\n</pre> # Load digits dataset (8x8 images) digits = load_digits() X, y = digits.data, digits.target  # Normalize features scaler = StandardScaler() X = scaler.fit_transform(X)  # One-hot encode labels y_onehot = np.eye(10)[y]  # Split data X_train, X_test, y_train, y_test = train_test_split(     X, y_onehot, test_size=0.2, random_state=42 )  print(f\"Dataset loaded!\") print(f\"  Training samples: {X_train.shape[0]}\") print(f\"  Test samples: {X_test.shape[0]}\") print(f\"  Input features: {X_train.shape[1]} (8x8 pixels)\") print(f\"  Output classes: {y_train.shape[1]} (digits 0-9)\") In\u00a0[\u00a0]: Copied! <pre># Visualize some examples\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\naxes = axes.ravel()\n\nfor i in range(10):\n    axes[i].imshow(digits.images[i], cmap='gray')\n    axes[i].set_title(f'Label: {digits.target[i]}', fontsize=12, fontweight='bold')\n    axes[i].axis('off')\n\nplt.suptitle('Sample Handwritten Digits', fontsize=15, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> # Visualize some examples fig, axes = plt.subplots(2, 5, figsize=(12, 5)) axes = axes.ravel()  for i in range(10):     axes[i].imshow(digits.images[i], cmap='gray')     axes[i].set_title(f'Label: {digits.target[i]}', fontsize=12, fontweight='bold')     axes[i].axis('off')  plt.suptitle('Sample Handwritten Digits', fontsize=15, fontweight='bold', y=1.02) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Create network\nnn = SimpleNeuralNetwork(input_size=64, hidden_size=32, output_size=10)\n\n# Training parameters\nepochs = 100\nlearning_rate = 0.1\nlosses = []\n\nprint(\" Training started...\\n\")\n\nfor epoch in range(epochs):\n    # Forward pass\n    output = nn.forward(X_train)\n    \n    # Compute loss\n    loss = nn.compute_loss(y_train, output)\n    losses.append(loss)\n    \n    # Backward pass\n    nn.backward(X_train, y_train, learning_rate)\n    \n    # Print progress\n    if (epoch + 1) % 20 == 0:\n        train_pred = nn.predict(X_train)\n        train_acc = np.mean(train_pred == y_train.argmax(axis=1))\n        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {train_acc:.4f}\")\n\nprint(\"\\n Training complete!\")\n</pre> # Create network nn = SimpleNeuralNetwork(input_size=64, hidden_size=32, output_size=10)  # Training parameters epochs = 100 learning_rate = 0.1 losses = []  print(\" Training started...\\n\")  for epoch in range(epochs):     # Forward pass     output = nn.forward(X_train)          # Compute loss     loss = nn.compute_loss(y_train, output)     losses.append(loss)          # Backward pass     nn.backward(X_train, y_train, learning_rate)          # Print progress     if (epoch + 1) % 20 == 0:         train_pred = nn.predict(X_train)         train_acc = np.mean(train_pred == y_train.argmax(axis=1))         print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {train_acc:.4f}\")  print(\"\\n Training complete!\") In\u00a0[\u00a0]: Copied! <pre># Plot training loss\nplt.figure(figsize=(10, 5))\nplt.plot(losses, linewidth=2, color='#3b82f6')\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Loss', fontsize=12, fontweight='bold')\nplt.title('Training Loss Over Time', fontsize=14, fontweight='bold', pad=15)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\" Loss decreased from {:.4f} to {:.4f}\".format(losses[0], losses[-1]))\n</pre> # Plot training loss plt.figure(figsize=(10, 5)) plt.plot(losses, linewidth=2, color='#3b82f6') plt.xlabel('Epoch', fontsize=12, fontweight='bold') plt.ylabel('Loss', fontsize=12, fontweight='bold') plt.title('Training Loss Over Time', fontsize=14, fontweight='bold', pad=15) plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  print(\" Loss decreased from {:.4f} to {:.4f}\".format(losses[0], losses[-1])) In\u00a0[\u00a0]: Copied! <pre># Test accuracy\ntest_pred = nn.predict(X_test)\ntest_acc = np.mean(test_pred == y_test.argmax(axis=1))\n\nprint(f\"\\n Final Results:\")\nprint(f\"  Test Accuracy: {test_acc:.2%}\")\nprint(f\"\\n Our neural network correctly classifies {test_acc:.1%} of handwritten digits!\")\n</pre> # Test accuracy test_pred = nn.predict(X_test) test_acc = np.mean(test_pred == y_test.argmax(axis=1))  print(f\"\\n Final Results:\") print(f\"  Test Accuracy: {test_acc:.2%}\") print(f\"\\n Our neural network correctly classifies {test_acc:.1%} of handwritten digits!\") In\u00a0[\u00a0]: Copied! <pre># Visualize predictions\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\naxes = axes.ravel()\n\n# Get first 10 test samples\ntest_indices = range(10)\n\nfor i, idx in enumerate(test_indices):\n    # Reshape for visualization\n    image = X_test[idx].reshape(8, 8)\n    \n    # Get prediction\n    pred = test_pred[idx]\n    true = y_test[idx].argmax()\n    \n    # Plot\n    axes[i].imshow(image, cmap='gray')\n    color = 'green' if pred == true else 'red'\n    axes[i].set_title(f'Pred: {pred} | True: {true}', \n                     fontsize=11, fontweight='bold', color=color)\n    axes[i].axis('off')\n\nplt.suptitle('Model Predictions (Green=Correct, Red=Wrong)', \n            fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> # Visualize predictions fig, axes = plt.subplots(2, 5, figsize=(12, 5)) axes = axes.ravel()  # Get first 10 test samples test_indices = range(10)  for i, idx in enumerate(test_indices):     # Reshape for visualization     image = X_test[idx].reshape(8, 8)          # Get prediction     pred = test_pred[idx]     true = y_test[idx].argmax()          # Plot     axes[i].imshow(image, cmap='gray')     color = 'green' if pred == true else 'red'     axes[i].set_title(f'Pred: {pred} | True: {true}',                       fontsize=11, fontweight='bold', color=color)     axes[i].axis('off')  plt.suptitle('Model Predictions (Green=Correct, Red=Wrong)',              fontsize=14, fontweight='bold', y=1.02) plt.tight_layout() plt.show()"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#what-youll-learn","title":"What You'll Learn\u00b6","text":"<ul> <li>How artificial neurons work</li> <li>Activation functions and why they matter</li> <li>Building a neural network from scratch</li> <li>Training with gradient descent</li> <li>Classifying handwritten digits (MNIST)</li> </ul>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#setup","title":"Setup\u00b6","text":""},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#1-understanding-artificial-neurons","title":"1. Understanding Artificial Neurons\u00b6","text":"<p>An artificial neuron is inspired by biological neurons in the brain. Here's how it works:</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#the-neuron-formula","title":"The Neuron Formula\u00b6","text":"<pre><code>Input \u2192 [Weighted Sum] \u2192 [Activation Function] \u2192 Output\n</code></pre> <p>Mathematically:</p> <p>$$y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)$$</p> <p>Where:</p> <ul> <li>x = inputs</li> <li>w = weights (learned parameters)</li> <li>b = bias (learned parameter)</li> <li>f = activation function (adds non-linearity)</li> </ul>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#visual-representation","title":"Visual Representation\u00b6","text":"<pre><code>x\u2081 w\u2081\nx\u2082 w\u2082\nx\u2083 w\u2083 \u03a3  f(\u00b7)  output\n   ...         \u2191\nx\u2099 w\u2099      b (bias)\n</code></pre>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#2-activation-functions","title":"2. Activation Functions\u00b6","text":"<p>Activation functions introduce non-linearity, allowing neural networks to learn complex patterns.</p> <p>Let's implement and visualize the most common ones:</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#3-building-a-neural-network-from-scratch","title":"3. Building a Neural Network from Scratch\u00b6","text":"<p>Let's build a simple 2-layer neural network to classify handwritten digits!</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#network-architecture","title":"Network Architecture\u00b6","text":"<pre><code>Input (64 pixels) \u2192 Hidden Layer (32 neurons) \u2192 Output (10 classes)\n</code></pre>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#4-loading-the-mnist-digits-dataset","title":"4. Loading the MNIST Digits Dataset\u00b6","text":"<p>We'll use the classic MNIST dataset of handwritten digits (0-9).</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#5-training-the-neural-network","title":"5. Training the Neural Network\u00b6","text":"<p>Now let's train our network!</p>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#6-evaluating-the-model","title":"6. Evaluating the Model\u00b6","text":""},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>Neurons combine inputs with weights, add bias, and apply activation functions</li> <li>Activation functions (ReLU, Sigmoid, Tanh) add non-linearity</li> <li>Forward pass: Data flows through layers to make predictions</li> <li>Backward pass: Gradients flow back to update weights</li> <li>Training loop: Forward \u2192 Loss \u2192 Backward \u2192 Update (repeat!)</li> </ol>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#whats-next","title":"What's Next?\u00b6","text":"<p>You've built a neural network from scratch! In the next modules, you'll learn:</p> <ul> <li>How modern frameworks (PyTorch, TensorFlow) simplify this</li> <li>Deeper networks with more layers</li> <li>Convolutional networks for images</li> <li>Transformer architectures for language</li> </ul>"},{"location":"paths/understanding-ai/1-foundations/chapter-2-first-neural-network/#practice-exercises","title":"Practice Exercises\u00b6","text":"<p>Try these challenges:</p> <ol> <li>Add another hidden layer to the network</li> <li>Experiment with different learning rates</li> <li>Try different activation functions</li> <li>Increase the hidden layer size</li> </ol> <p>\u00a9 2026 MadeForAI. Learn more at madeforai.github.io</p>"}]}