{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 - Your First Neural Network\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/paths/understanding-ai/module-1/chapter-2-first-neural-network.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "In the previous chapter, we built a simple linear model. Now, let's level up and build a real **neural network** from scratch! You'll understand how neurons work, how networks learn through backpropagation, and build a classifier for handwritten digits.\n",
    "\n",
    "##  What You'll Learn\n",
    "\n",
    "- How artificial neurons work\n",
    "- Activation functions and why they matter\n",
    "- Building a neural network from scratch\n",
    "- Training with gradient descent\n",
    "- Classifying handwritten digits (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in Colab)\n",
    "# !pip install numpy matplotlib scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\" Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Understanding Artificial Neurons\n",
    "\n",
    "An artificial neuron is inspired by biological neurons in the brain. Here's how it works:\n",
    "\n",
    "### The Neuron Formula\n",
    "\n",
    "```\n",
    "Input \u2192 [Weighted Sum] \u2192 [Activation Function] \u2192 Output\n",
    "```\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)$$\n",
    "\n",
    "Where:\n",
    "- **x** = inputs\n",
    "- **w** = weights (learned parameters)\n",
    "- **b** = bias (learned parameter)\n",
    "- **f** = activation function (adds non-linearity)\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "x\u2081 w\u2081\n",
    "x\u2082 w\u2082\n",
    "x\u2083 w\u2083 \u03a3  f(\u00b7)  output\n",
    "   ...         \u2191\n",
    "x\u2099 w\u2099      b (bias)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing neural networks to learn complex patterns.\n",
    "\n",
    "Let's implement and visualize the most common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid: Squashes values between 0 and 1\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU: Returns max(0, x)\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh: Squashes values between -1 and 1\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(x, sigmoid(x), 'b-', linewidth=2.5)\n",
    "axes[0].set_title('Sigmoid', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('\u03c3(x)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "axes[1].plot(x, relu(x), 'g-', linewidth=2.5)\n",
    "axes[1].set_title('ReLU', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('ReLU(x)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "axes[2].plot(x, tanh(x), 'r-', linewidth=2.5)\n",
    "axes[2].set_title('Tanh', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('tanh(x)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Activation Functions:\")\n",
    "print(\"  \u2022 Sigmoid: Good for binary classification (0-1 output)\")\n",
    "print(\"  \u2022 ReLU: Most popular for hidden layers (fast, simple)\")\n",
    "print(\"  \u2022 Tanh: Similar to sigmoid but centered at 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Neural Network from Scratch\n",
    "\n",
    "Let's build a simple 2-layer neural network to classify handwritten digits!\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "```\n",
    "Input (64 pixels) \u2192 Hidden Layer (32 neurons) \u2192 Output (10 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"A simple 2-layer neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation for multi-class classification\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"Backward pass (gradient descent)\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dz1 = np.dot(dz2, self.W2.T) * (self.z1 > 0)  # ReLU derivative\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])\n",
    "        return np.sum(log_likelihood) / m\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "print(\" Neural Network class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading the MNIST Digits Dataset\n",
    "\n",
    "We'll use the classic MNIST dataset of handwritten digits (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset (8x8 images)\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_onehot = np.eye(10)[y]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded!\")\n",
    "print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test.shape[0]}\")\n",
    "print(f\"  Input features: {X_train.shape[1]} (8x8 pixels)\")\n",
    "print(f\"  Output classes: {y_train.shape[1]} (digits 0-9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(digits.images[i], cmap='gray')\n",
    "    axes[i].set_title(f'Label: {digits.target[i]}', fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Handwritten Digits', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Neural Network\n",
    "\n",
    "Now let's train our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network\n",
    "nn = SimpleNeuralNetwork(input_size=64, hidden_size=32, output_size=10)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "losses = []\n",
    "\n",
    "print(\" Training started...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output = nn.forward(X_train)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = nn.compute_loss(y_train, output)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    nn.backward(X_train, y_train, learning_rate)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        train_pred = nn.predict(X_train)\n",
    "        train_acc = np.mean(train_pred == y_train.argmax(axis=1))\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "print(\"\\n Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2, color='#3b82f6')\n",
    "plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Loss decreased from {:.4f} to {:.4f}\".format(losses[0], losses[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "test_pred = nn.predict(X_test)\n",
    "test_acc = np.mean(test_pred == y_test.argmax(axis=1))\n",
    "\n",
    "print(f\"\\n Final Results:\")\n",
    "print(f\"  Test Accuracy: {test_acc:.2%}\")\n",
    "print(f\"\\n Our neural network correctly classifies {test_acc:.1%} of handwritten digits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Get first 10 test samples\n",
    "test_indices = range(10)\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    # Reshape for visualization\n",
    "    image = X_test[idx].reshape(8, 8)\n",
    "    \n",
    "    # Get prediction\n",
    "    pred = test_pred[idx]\n",
    "    true = y_test[idx].argmax()\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].imshow(image, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    axes[i].set_title(f'Pred: {pred} | True: {true}', \n",
    "                     fontsize=11, fontweight='bold', color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Model Predictions (Green=Correct, Red=Wrong)', \n",
    "            fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Key Takeaways\n",
    "\n",
    "1. **Neurons** combine inputs with weights, add bias, and apply activation functions\n",
    "2. **Activation functions** (ReLU, Sigmoid, Tanh) add non-linearity\n",
    "3. **Forward pass**: Data flows through layers to make predictions\n",
    "4. **Backward pass**: Gradients flow back to update weights\n",
    "5. **Training loop**: Forward \u2192 Loss \u2192 Backward \u2192 Update (repeat!)\n",
    "\n",
    "##  What's Next?\n",
    "\n",
    "You've built a neural network from scratch! In the next modules, you'll learn:\n",
    "- How modern frameworks (PyTorch, TensorFlow) simplify this\n",
    "- Deeper networks with more layers\n",
    "- Convolutional networks for images\n",
    "- Transformer architectures for language\n",
    "\n",
    "---\n",
    "\n",
    "##  Practice Exercises\n",
    "\n",
    "Try these challenges:\n",
    "1. Add another hidden layer to the network\n",
    "2. Experiment with different learning rates\n",
    "3. Try different activation functions\n",
    "4. Increase the hidden layer size\n",
    "\n",
    "---\n",
    "\n",
    "*\u00a9 2026 MadeForAI. Learn more at [madeforai.github.io](https://madeforai.github.io/madeforai)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}