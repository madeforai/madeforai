{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 - Your First AI Model\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/understanding-ai/module-1/1.4-first-ai-model.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Build a complete machine learning model from scratch‚Äîdata to deployment in under 30 minutes.**\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- **Load and explore real data**: Understand your dataset before building models\n",
    "- **Data preprocessing**: Handle missing values, encode categories, and scale features\n",
    "- **Train multiple models**: Try different algorithms and compare performance\n",
    "- **Evaluate properly**: Use metrics that actually matter for your problem\n",
    "- **The complete ML workflow**: From raw data to trained model\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "30-35 minutes\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic Python and pandas knowledge\n",
    "- Understanding of machine learning concepts (Chapter 1.3)\n",
    "- Enthusiasm to build your first model! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Our Mission: Predict Customer Churn\n",
    "\n",
    "**The Business Problem:**\n",
    "\n",
    "You work for a telecom company. Customers are leaving (churning), and it costs 5x more to acquire new customers than to retain existing ones. Your job: **build a model that predicts which customers are likely to churn so the company can intervene early.**\n",
    "\n",
    "**Why This Matters:**\n",
    "- Churn prediction is used by Netflix, Spotify, banks, and virtually every subscription business\n",
    "- It's a classic **binary classification** problem (churn: Yes/No)\n",
    "- Perfect first project‚Äîreal business value, clean dataset, measurable impact\n",
    "\n",
    "**What We'll Build:**\n",
    "1. Load and explore customer data\n",
    "2. Clean and prepare features\n",
    "3. Train multiple classification models\n",
    "4. Compare performance using proper metrics\n",
    "5. Select the best model and interpret results\n",
    "\n",
    "Let's dive in! üèä‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install required packages\n",
    "# Uncomment if running in Google Colab\n",
    "# !pip install numpy pandas matplotlib seaborn scikit-learn plotly -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(\"üéØ Ready to build your first AI model!\")\n",
    "print(\"\\nüíº Project: Telecom Customer Churn Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Load and Explore the Data\n",
    "\n",
    "**Golden Rule:** Never start building models without understanding your data first!\n",
    "\n",
    "We'll use a synthetic telecom churn dataset with realistic customer attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic telecom customer data (in real projects, you'd load from CSV/database)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate realistic customer data\n",
    "data = pd.DataFrame({\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 70, n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "    'tenure_months': np.random.randint(1, 73, n_samples),  # 0-6 years\n",
    "    'monthly_charges': np.random.uniform(20, 120, n_samples),\n",
    "    'total_charges': None,  # Will calculate from tenure and monthly\n",
    "    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples, p=[0.5, 0.3, 0.2]),\n",
    "    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples, p=[0.3, 0.5, 0.2]),\n",
    "    'online_security': np.random.choice(['Yes', 'No', 'No internet service'], n_samples, p=[0.3, 0.5, 0.2]),\n",
    "    'tech_support': np.random.choice(['Yes', 'No', 'No internet service'], n_samples, p=[0.25, 0.55, 0.2]),\n",
    "    'paperless_billing': np.random.choice(['Yes', 'No'], n_samples, p=[0.6, 0.4]),\n",
    "    'payment_method': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], \n",
    "                                       n_samples, p=[0.35, 0.2, 0.25, 0.2])\n",
    "})\n",
    "\n",
    "# Calculate total charges\n",
    "data['total_charges'] = data['tenure_months'] * data['monthly_charges'] + np.random.normal(0, 100, n_samples)\n",
    "data.loc[data['tenure_months'] == 0, 'total_charges'] = data.loc[data['tenure_months'] == 0, 'monthly_charges']\n",
    "\n",
    "# Create churn label (target variable)\n",
    "# Higher churn probability for: short tenure, month-to-month contracts, high charges, electronic check\n",
    "churn_prob = 0.1  # Base churn rate\n",
    "churn_prob += (data['tenure_months'] < 12) * 0.3  # New customers more likely to churn\n",
    "churn_prob += (data['contract_type'] == 'Month-to-month') * 0.25\n",
    "churn_prob += (data['monthly_charges'] > 80) * 0.15\n",
    "churn_prob += (data['payment_method'] == 'Electronic check') * 0.2\n",
    "churn_prob += (data['online_security'] == 'No') * 0.1\n",
    "churn_prob += (data['tech_support'] == 'No') * 0.1\n",
    "\n",
    "data['churn'] = (np.random.random(n_samples) < churn_prob).astype(int)\n",
    "\n",
    "# Introduce some missing values (realistic scenario)\n",
    "missing_idx = np.random.choice(data.index, size=int(0.02 * n_samples), replace=False)\n",
    "data.loc[missing_idx, 'total_charges'] = np.nan\n",
    "\n",
    "print(\"üìÅ Dataset loaded successfully!\\n\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {data.shape[1]}\")\n",
    "print(f\"Rows: {data.shape[0]}\\n\")\n",
    "print(\"First few rows:\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"üìã Dataset Information:\\n\")\n",
    "print(data.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìä Summary Statistics:\\n\")\n",
    "display(data.describe())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüîç Missing Values:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüéØ Target Variable Distribution:\")\n",
    "churn_counts = data['churn'].value_counts()\n",
    "print(f\"Not Churned (0): {churn_counts[0]} ({churn_counts[0]/len(data)*100:.1f}%)\")\n",
    "print(f\"Churned (1): {churn_counts[1]} ({churn_counts[1]/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Churn distribution\n",
    "churn_counts.plot(kind='bar', ax=axes[0, 0], color=['#10b981', '#ef4444'], alpha=0.7, edgecolor='white', linewidth=2)\n",
    "axes[0, 0].set_title('Churn Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Churn (0=No, 1=Yes)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Customers', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_xticklabels(['Not Churned', 'Churned'], rotation=0)\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Tenure vs Churn\n",
    "data.groupby('churn')['tenure_months'].plot(kind='hist', ax=axes[0, 1], alpha=0.6, bins=20, legend=True)\n",
    "axes[0, 1].set_title('Tenure Distribution by Churn', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Tenure (months)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].legend(['No Churn', 'Churn'])\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Monthly Charges vs Churn\n",
    "data.groupby('churn')['monthly_charges'].plot(kind='hist', ax=axes[0, 2], alpha=0.6, bins=20, legend=True)\n",
    "axes[0, 2].set_title('Monthly Charges by Churn', fontsize=13, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Monthly Charges ($)', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].legend(['No Churn', 'Churn'])\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Contract Type vs Churn\n",
    "contract_churn = pd.crosstab(data['contract_type'], data['churn'], normalize='index') * 100\n",
    "contract_churn.plot(kind='bar', ax=axes[1, 0], color=['#10b981', '#ef4444'], alpha=0.7, edgecolor='white', linewidth=2)\n",
    "axes[1, 0].set_title('Churn Rate by Contract Type', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Contract Type', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Percentage (%)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].legend(['No Churn', 'Churn'], loc='upper right')\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Internet Service vs Churn\n",
    "internet_churn = pd.crosstab(data['internet_service'], data['churn'], normalize='index') * 100\n",
    "internet_churn.plot(kind='bar', ax=axes[1, 1], color=['#10b981', '#ef4444'], alpha=0.7, edgecolor='white', linewidth=2)\n",
    "axes[1, 1].set_title('Churn Rate by Internet Service', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Internet Service', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Percentage (%)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].legend(['No Churn', 'Churn'], loc='upper right')\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Payment Method vs Churn  \n",
    "payment_churn = pd.crosstab(data['payment_method'], data['churn'], normalize='index') * 100\n",
    "payment_churn.plot(kind='bar', ax=axes[1, 2], color=['#10b981', '#ef4444'], alpha=0.7, edgecolor='white', linewidth=2)\n",
    "axes[1, 2].set_title('Churn Rate by Payment Method', fontsize=13, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Payment Method', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].set_ylabel('Percentage (%)', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].legend(['No Churn', 'Churn'], loc='upper right')\n",
    "axes[1, 2].set_xticklabels(axes[1, 2].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Observations:\")\n",
    "print(\"   ‚Üí Customers with shorter tenure churn more often\")\n",
    "print(\"   ‚Üí Month-to-month contracts have highest churn rate\")\n",
    "print(\"   ‚Üí Electronic check payments correlate with higher churn\")\n",
    "print(\"   ‚Üí Fiber optic customers churn more than DSL customers\")\n",
    "print(\"\\nüí° These patterns will help our model predict churn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 2: Data Preprocessing\n",
    "\n",
    "Raw data is messy. Before training models, we need to:\n",
    "1. **Handle missing values**\n",
    "2. **Encode categorical variables** (convert text to numbers)\n",
    "3. **Scale numerical features** (standardize ranges)\n",
    "4. **Split into train/validation/test sets**\n",
    "\n",
    "Let's do this systematically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df = data.copy()\n",
    "\n",
    "print(\"üßπ Starting Data Preprocessing...\\n\")\n",
    "\n",
    "# 1. Handle missing values\n",
    "print(\"Step 1: Handling missing values\")\n",
    "print(f\"   Missing values before: {df['total_charges'].isnull().sum()}\")\n",
    "# Fill missing total_charges with median\n",
    "df['total_charges'].fillna(df['total_charges'].median(), inplace=True)\n",
    "print(f\"   Missing values after: {df['total_charges'].isnull().sum()}\")\n",
    "print(\"   ‚úÖ Missing values handled\\n\")\n",
    "\n",
    "# 2. Drop customer_id (not useful for prediction)\n",
    "df = df.drop('customer_id', axis=1)\n",
    "print(\"Step 2: Removed customer_id column\\n\")\n",
    "\n",
    "# 3. Encode categorical variables\n",
    "print(\"Step 3: Encoding categorical variables\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols.remove('churn') if 'churn' in categorical_cols else None\n",
    "\n",
    "print(f\"   Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Use Label Encoding (convert categories to numbers)\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "print(\"   ‚úÖ Categorical variables encoded\\n\")\n",
    "\n",
    "print(\"üìä Preprocessed Data:\")\n",
    "display(df.head())\n",
    "print(f\"\\n‚ú® Data preprocessing complete! Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split features and target\n",
    "print(\"\\nüì¶ Preparing Features and Target...\\n\")\n",
    "\n",
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"\\nFeature names: {list(X.columns)}\")\n",
    "\n",
    "# 5. Train-Test Split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  # stratify keeps class balance\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data Split:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "# 6. Scale numerical features\n",
    "print(f\"\\nüîß Scaling numerical features...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data, transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for better readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "print(\"   ‚úÖ Features scaled (mean=0, std=1)\")\n",
    "print(\"\\nüéØ Data is ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Train Multiple Models\n",
    "\n",
    "We'll train 5 different classification algorithms and compare them:\n",
    "\n",
    "1. **Logistic Regression**: Simple, interpretable, fast (linear model)\n",
    "2. **Decision Tree**: Easy to visualize, handles non-linearity\n",
    "3. **Random Forest**: Ensemble of trees, robust to overfitting\n",
    "4. **Gradient Boosting**: Powerful, often wins competitions\n",
    "5. **K-Nearest Neighbors**: Simple, no training phase\n",
    "\n",
    "**Why try multiple models?**\n",
    "- No single algorithm is always best\n",
    "- Different models capture different patterns\n",
    "- Comparing helps you understand the problem better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Training Multiple Classification Models...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "# Train each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    trained_models[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ {name} trained successfully!\")\n",
    "    print(f\"      Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ All models trained successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Evaluate and Compare Models\n",
    "\n",
    "**Important:** Different metrics tell different stories!\n",
    "\n",
    "- **Accuracy**: Overall correctness (can be misleading with imbalanced data)\n",
    "- **Precision**: Of predicted churners, how many actually churned? (avoid false alarms)\n",
    "- **Recall**: Of actual churners, how many did we catch? (don't miss churners)\n",
    "- **F1-Score**: Balance between precision and recall\n",
    "- **ROC-AUC**: Overall ability to discriminate between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"üìä MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "display(results_df.round(3))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Visualize comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3b82f6', '#10b981', '#f59e0b', '#8b5cf6']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Sort by metric\n",
    "    sorted_df = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    bars = ax.barh(sorted_df['Model'], sorted_df[metric], \n",
    "                   color=color, alpha=0.7, edgecolor='white', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "               f'{width:.3f}',\n",
    "               ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nüèÜ Best Models by Metric:\")\n",
    "print(\"=\"*60)\n",
    "for metric in metrics:\n",
    "    best_model = results_df.loc[results_df[metric].idxmax(), 'Model']\n",
    "    best_value = results_df[metric].max()\n",
    "    print(f\"{metric:12s}: {best_model:25s} ({best_value:.3f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Comparison (for models with probability predictions)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for name, model_data in trained_models.items():\n",
    "    if model_data['probabilities'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, model_data['probabilities'])\n",
    "        roc_auc = roc_auc_score(y_test, model_data['probabilities'])\n",
    "        ax.plot(fpr, tpr, linewidth=2.5, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Plot random classifier baseline\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_title('ROC Curves: Model Comparison\\n(Closer to top-left = Better)', \n",
    "            fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà ROC-AUC Interpretation:\")\n",
    "print(\"   ‚Üí 0.5 = Random guessing (no better than coin flip)\")\n",
    "print(\"   ‚Üí 0.7-0.8 = Acceptable\")\n",
    "print(\"   ‚Üí 0.8-0.9 = Excellent\")\n",
    "print(\"   ‚Üí 0.9+ = Outstanding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model\n",
    "best_f1_model = results_df.loc[results_df['F1-Score'].idxmax(), 'Model']\n",
    "best_predictions = trained_models[best_f1_model]['predictions']\n",
    "\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True, \n",
    "           xticklabels=['Not Churn', 'Churn'],\n",
    "           yticklabels=['Not Churn', 'Churn'],\n",
    "           cbar_kws={'label': 'Count'},\n",
    "           ax=ax, annot_kws={'size': 14, 'weight': 'bold'})\n",
    "\n",
    "ax.set_xlabel('Predicted', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Actual', fontsize=13, fontweight='bold')\n",
    "ax.set_title(f'Confusion Matrix: {best_f1_model}\\n(Best F1-Score Model)', \n",
    "            fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display confusion matrix metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nüìä Confusion Matrix Breakdown ({best_f1_model}):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Negatives (TN):  {tn:4d} ‚Üê Correctly predicted NOT churn\")\n",
    "print(f\"False Positives (FP): {fp:4d} ‚Üê Incorrectly predicted churn\")\n",
    "print(f\"False Negatives (FN): {fn:4d} ‚Üê Missed churners (dangerous!)\")\n",
    "print(f\"True Positives (TP):  {tp:4d} ‚Üê Correctly predicted churn\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüí° Business Impact:\")\n",
    "print(f\"   ‚Üí We correctly identified {tp} churners (can intervene!)\")\n",
    "print(f\"   ‚Üí We missed {fn} churners (they'll leave unnoticed)\")\n",
    "print(f\"   ‚Üí We had {fp} false alarms (wasted retention efforts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for best model\n",
    "print(f\"\\nüìã DETAILED CLASSIFICATION REPORT: {best_f1_model}\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, best_predictions, \n",
    "                          target_names=['Not Churn', 'Churn'],\n",
    "                          digits=3))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Interpret Results & Feature Importance\n",
    "\n",
    "**The Big Question:** Which factors most influence churn?\n",
    "\n",
    "Understanding feature importance helps:\n",
    "- Build trust in the model\n",
    "- Guide business strategy\n",
    "- Debug model behavior\n",
    "- Comply with regulations (explainability)\n",
    "\n",
    "Let's examine feature importance from our best tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from tree-based models\n",
    "tree_models = ['Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, model_name in enumerate(tree_models):\n",
    "    model = trained_models[model_name]['model']\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    bars = ax.barh(importances['Feature'], importances['Importance'], \n",
    "                   alpha=0.7, edgecolor='white', linewidth=2)\n",
    "    \n",
    "    # Color bars by importance\n",
    "    colors = plt.cm.RdYlGn(importances['Importance'] / importances['Importance'].max())\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    ax.set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{model_name}\\nFeature Importance', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get top 5 features from Random Forest (typically most reliable)\n",
    "rf_model = trained_models['Random Forest']['model']\n",
    "rf_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüîç TOP 5 MOST IMPORTANT FEATURES (Random Forest):\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in rf_importances.head(5).iterrows():\n",
    "    print(f\"{row['Feature']:25s}: {row['Importance']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüí° Business Insights:\")\n",
    "print(\"   ‚Üí Focus retention efforts on customers with short tenure\")\n",
    "print(\"   ‚Üí Monthly contracts are a churn risk‚Äîencourage longer terms\")\n",
    "print(\"   ‚Üí High monthly charges predict churn‚Äîconsider pricing strategy\")\n",
    "print(\"   ‚Üí Offer online security/tech support to at-risk customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Making Predictions on New Customers\n",
    "\n",
    "The whole point of building a model is to use it! Let's demonstrate how to:\n",
    "1. Take a new customer's data\n",
    "2. Preprocess it the same way as training data\n",
    "3. Make a churn prediction\n",
    "4. Get a probability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some hypothetical new customers\n",
    "new_customers = pd.DataFrame([\n",
    "    {\n",
    "        'age': 25,\n",
    "        'gender': 'Female',\n",
    "        'tenure_months': 3,  # NEW customer\n",
    "        'monthly_charges': 85.0,  # HIGH charges\n",
    "        'total_charges': 255.0,\n",
    "        'contract_type': 'Month-to-month',  # RISKY\n",
    "        'internet_service': 'Fiber optic',\n",
    "        'online_security': 'No',  # NO security\n",
    "        'tech_support': 'No',  # NO support\n",
    "        'paperless_billing': 'Yes',\n",
    "        'payment_method': 'Electronic check'  # RISKY payment\n",
    "    },\n",
    "    {\n",
    "        'age': 45,\n",
    "        'gender': 'Male',\n",
    "        'tenure_months': 48,  # LOYAL customer\n",
    "        'monthly_charges': 45.0,  # LOW charges\n",
    "        'total_charges': 2160.0,\n",
    "        'contract_type': 'Two year',  # STABLE contract\n",
    "        'internet_service': 'DSL',\n",
    "        'online_security': 'Yes',  # HAS security\n",
    "        'tech_support': 'Yes',  # HAS support\n",
    "        'paperless_billing': 'No',\n",
    "        'payment_method': 'Bank transfer'  # STABLE payment\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"üÜï New Customers to Predict:\\n\")\n",
    "display(new_customers)\n",
    "\n",
    "# Preprocess new data (same as training data)\n",
    "new_customers_processed = new_customers.copy()\n",
    "\n",
    "# Encode categorical variables using the same encoders\n",
    "for col in categorical_cols:\n",
    "    new_customers_processed[col] = label_encoders[col].transform(new_customers_processed[col])\n",
    "\n",
    "# Scale features\n",
    "new_customers_scaled = scaler.transform(new_customers_processed)\n",
    "\n",
    "# Make predictions with best model\n",
    "best_model = trained_models[best_f1_model]['model']\n",
    "predictions = best_model.predict(new_customers_scaled)\n",
    "probabilities = best_model.predict_proba(new_customers_scaled)[:, 1]\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÆ CHURN PREDICTIONS:\")\n",
    "print(\"=\"*80)\n",
    "for idx, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "    risk_level = \"HIGH RISK\" if prob > 0.7 else \"MEDIUM RISK\" if prob > 0.4 else \"LOW RISK\"\n",
    "    color = \"üî¥\" if prob > 0.7 else \"üü°\" if prob > 0.4 else \"üü¢\"\n",
    "    \n",
    "    print(f\"\\nCustomer {idx + 1}:\")\n",
    "    print(f\"  Prediction: {'WILL CHURN' if pred == 1 else 'WILL STAY'}\")\n",
    "    print(f\"  Churn Probability: {prob:.1%}\")\n",
    "    print(f\"  Risk Level: {color} {risk_level}\")\n",
    "    \n",
    "    if prob > 0.5:\n",
    "        print(f\"  ‚ö†Ô∏è  Action: Immediate retention intervention recommended!\")\n",
    "        print(f\"     ‚Üí Offer contract incentives\")\n",
    "        print(f\"     ‚Üí Add online security/tech support\")\n",
    "        print(f\"     ‚Üí Consider pricing adjustment\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Action: Standard customer care, monitor monthly\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: Build Your Own Customer Profile\n",
    "\n",
    "**Objective:** Create a customer profile and predict their churn probability\n",
    "\n",
    "**Task:**  \n",
    "Modify the customer data below to create a hypothetical customer. Then run the prediction code to see if they're likely to churn.\n",
    "\n",
    "**Experiment with:**\n",
    "- Different tenure lengths (1-72 months)\n",
    "- Contract types (Month-to-month, One year, Two year)\n",
    "- Service combinations\n",
    "- Monthly charge levels ($20-$120)\n",
    "\n",
    "**Questions to explore:**\n",
    "1. What combination creates the highest churn risk?\n",
    "2. How much does contract type affect the prediction?\n",
    "3. Can you create a customer with >90% churn probability?\n",
    "4. Can you create a customer with <10% churn probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own customer profile here\n",
    "my_customer = pd.DataFrame([{\n",
    "    'age': 30,  # Modify these values!\n",
    "    'gender': 'Female',\n",
    "    'tenure_months': 6,\n",
    "    'monthly_charges': 75.0,\n",
    "    'total_charges': 450.0,\n",
    "    'contract_type': 'Month-to-month',\n",
    "    'internet_service': 'Fiber optic',\n",
    "    'online_security': 'No',\n",
    "    'tech_support': 'No',\n",
    "    'paperless_billing': 'Yes',\n",
    "    'payment_method': 'Electronic check'\n",
    "}])\n",
    "\n",
    "# Preprocess and predict (same code as above)\n",
    "my_customer_processed = my_customer.copy()\n",
    "for col in categorical_cols:\n",
    "    my_customer_processed[col] = label_encoders[col].transform(my_customer_processed[col])\n",
    "my_customer_scaled = scaler.transform(my_customer_processed)\n",
    "\n",
    "my_prediction = best_model.predict(my_customer_scaled)[0]\n",
    "my_probability = best_model.predict_proba(my_customer_scaled)[0, 1]\n",
    "\n",
    "print(\"\\nüîÆ YOUR CUSTOMER PREDICTION:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Churn Prediction: {'WILL CHURN' if my_prediction == 1 else 'WILL STAY'}\")\n",
    "print(f\"Churn Probability: {my_probability:.1%}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Your observations here:\n",
    "# What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Improve the Model\n",
    "\n",
    "**Objective:** Try to beat the current best model's F1-score\n",
    "\n",
    "**Ideas to experiment with:**\n",
    "1. **Try different hyperparameters**:\n",
    "   - Change `max_depth` for Decision Tree\n",
    "   - Adjust `n_estimators` for Random Forest\n",
    "   - Modify `n_neighbors` for KNN\n",
    "\n",
    "2. **Feature engineering**:\n",
    "   - Create new features (e.g., `charges_per_month = total_charges / tenure_months`)\n",
    "   - Try polynomial features\n",
    "   - Remove less important features\n",
    "\n",
    "3. **Handle class imbalance**:\n",
    "   - Try `class_weight='balanced'` in models\n",
    "   - Use SMOTE (Synthetic Minority Over-sampling)\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint: Start Here</summary>\n",
    "\n",
    "Try training a Random Forest with more trees:\n",
    "```python\n",
    "improved_rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "improved_rf.fit(X_train_scaled, y_train)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experimentation here!\n",
    "# Try to improve the model performance\n",
    "\n",
    "# Example starter code:\n",
    "# improved_model = RandomForestClassifier(\n",
    "#     n_estimators=200,  # More trees\n",
    "#     max_depth=10,      # Deeper trees\n",
    "#     min_samples_split=5,\n",
    "#     random_state=42\n",
    "# )\n",
    "# improved_model.fit(X_train_scaled, y_train)\n",
    "# improved_pred = improved_model.predict(X_test_scaled)\n",
    "# print(f\"Improved F1-Score: {f1_score(y_test, improved_pred):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "Congratulations! You've built your first complete machine learning model! Let's recap:\n",
    "\n",
    "- ‚úÖ **Data Exploration**: Always understand your data before modeling\n",
    "- ‚úÖ **Preprocessing Pipeline**: Handle missing values, encode categories, scale features\n",
    "- ‚úÖ **Model Selection**: Try multiple algorithms, no free lunch theorem\n",
    "- ‚úÖ **Proper Evaluation**: Use train-test split, multiple metrics, confusion matrix\n",
    "- ‚úÖ **Business Context**: Metrics mean nothing without business interpretation\n",
    "- ‚úÖ **Feature Importance**: Understand what drives predictions\n",
    "- ‚úÖ **Deployment Ready**: Know how to make predictions on new data\n",
    "\n",
    "### üíº Real-World Impact:\n",
    "\n",
    "This exact workflow powers:\n",
    "- Netflix predicting if you'll cancel\n",
    "- Banks detecting fraudulent transactions\n",
    "- Healthcare diagnosing diseases\n",
    "- E-commerce personalizing recommendations\n",
    "\n",
    "**You just built the same foundation used by billion-dollar AI systems!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Learning\n",
    "\n",
    "**Recommended Reading:**\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html) - Official guide to every algorithm\n",
    "- [Kaggle Learn](https://www.kaggle.com/learn/intro-to-machine-learning) - Interactive ML tutorials\n",
    "- [ML Mastery](https://machinelearningmastery.com/) - Practical ML tutorials\n",
    "\n",
    "**Practice Datasets:**\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) - 500+ datasets\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets) - Real-world data\n",
    "- [Scikit-learn Built-in Datasets](https://scikit-learn.org/stable/datasets.html) - Ready to use\n",
    "\n",
    "**Competitions (Apply Your Skills):**\n",
    "- [Kaggle Competitions](https://www.kaggle.com/competitions) - Compete with data scientists worldwide\n",
    "- [DrivenData](https://www.drivendata.org/competitions/) - Competitions for social good\n",
    "\n",
    "**Advanced Topics:**\n",
    "- Hyperparameter tuning with GridSearchCV/RandomizedSearchCV\n",
    "- Cross-validation strategies\n",
    "- Handling imbalanced datasets\n",
    "- Model interpretability (SHAP, LIME)\n",
    "- Feature engineering techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚û°Ô∏è What's Next?\n",
    "\n",
    "You've completed Module 1: AI Foundations! üéâ\n",
    "\n",
    "**You've learned:**\n",
    "- What AI is and the current landscape\n",
    "- How machines learn (supervised, unsupervised, reinforcement)\n",
    "- Core ML concepts (loss functions, gradient descent, train-test split)\n",
    "- How to build a complete ML model end-to-end\n",
    "\n",
    "**Next in Module 2: Machine Learning Fundamentals**\n",
    "\n",
    "You'll dive deeper into:\n",
    "- **2.1 - Supervised Learning Essentials**: Master regression and classification\n",
    "- **2.2 - Classification vs Regression**: When to use which and why\n",
    "- **2.3 - Unsupervised Learning & Clustering**: Discover hidden patterns\n",
    "- **2.4 - Model Evaluation & Metrics**: Beyond accuracy to real performance\n",
    "\n",
    "Ready to level up? Start **[Module 2: Machine Learning Fundamentals](../module-2/2.1-supervised-learning.ipynb)**! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Feedback & Community\n",
    "\n",
    "**Questions?** Join our [Discord community](https://discord.gg/madeforai)\n",
    "\n",
    "**Found a bug?** [Open an issue on GitHub](https://github.com/madeforai/madeforai/issues)\n",
    "\n",
    "**Want to contribute?** Check our [contribution guide](https://github.com/madeforai/madeforai/blob/main/CONTRIBUTING.md)\n",
    "\n",
    "**Share your success!** Tweet your first model with #MadeForAI\n",
    "\n",
    "---\n",
    "\n",
    "### üéâ Congratulations!\n",
    "\n",
    "You're no longer a complete beginner‚Äîyou're a practicing AI engineer!\n",
    "\n",
    "**Keep building, keep learning, keep creating!** üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
