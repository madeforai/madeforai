{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Model Evaluation & Metrics: Measuring What Matters\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/understanding-ai/module-2/2.4-model-evaluation.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Master the art of measuring model performance‚Äîbecause a model is only as good as your ability to evaluate it.**\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- **Confusion matrices**: Understanding true/false positives and negatives\n",
    "- **Core metrics**: Accuracy, precision, recall, F1-score‚Äîwhen to use what\n",
    "- **ROC curves & AUC**: Visualizing classifier performance across thresholds\n",
    "- **Cross-validation**: Properly assessing model generalization\n",
    "- **Bias-variance tradeoff**: Overfitting vs underfitting explained\n",
    "- **Business metrics**: Choosing the right metric for your problem\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "40-45 minutes\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Completed Chapter 2.3 (Unsupervised Learning)\n",
    "- Understanding of classification and regression\n",
    "- Basic probability concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Evaluation Paradox\n",
    "\n",
    "**Scenario**: You've built two models for detecting spam emails.\n",
    "\n",
    "**Model A**: 95% accuracy  \n",
    "**Model B**: 88% accuracy\n",
    "\n",
    "**Question**: Which is better?\n",
    "\n",
    "**Your answer**: \"Obviously Model A!\"\n",
    "\n",
    "**Reality**: **Maybe not.** Here's why:\n",
    "\n",
    "Imagine your dataset:\n",
    "- 95% legitimate emails\n",
    "- 5% spam emails\n",
    "\n",
    "**Model A (the \"lazy\" model)**:\n",
    "- Predicts EVERYTHING as \"legitimate\"\n",
    "- Accuracy: 95% ‚úÖ\n",
    "- Spam caught: 0% ‚ùå\n",
    "- **Completely useless!**\n",
    "\n",
    "**Model B (the \"smart\" model)**:\n",
    "- Actually tries to detect spam\n",
    "- Accuracy: 88% ‚úÖ\n",
    "- Spam caught: 85% ‚úÖ\n",
    "- **Actually useful!**\n",
    "\n",
    "**The Lesson**: **Accuracy alone is often meaningless!**\n",
    "\n",
    "This chapter teaches you to:\n",
    "1. ‚úÖ Choose the RIGHT metric for YOUR problem\n",
    "2. ‚úÖ Understand the tradeoffs between different metrics\n",
    "3. ‚úÖ Properly validate your models\n",
    "4. ‚úÖ Communicate model performance to stakeholders\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create an infographic showing the model evaluation paradox.\n",
    "Style: Professional, slightly humorous educational diagram.\n",
    "Left side: 'Model A' showing 95% accuracy badge (shiny gold) but a broken spam filter icon (spam going through).\n",
    "Right side: 'Model B' showing 88% accuracy badge (silver) but working spam filter (spam being blocked).\n",
    "Center: Large question mark with 'Which is better?'\n",
    "Bottom: Reveal showing Model A labels everything as 'Not Spam', Model B actually detects spam.\n",
    "Include text: 'Accuracy isn't everything!'\n",
    "Color scheme: Red for spam, green for legitimate, gold/silver for badges.\n",
    "Format: Comparison layout, 16:9 ratio.\" -->\n",
    "\n",
    "Let's dive into the metrics that matter! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import libraries\n",
    "# Uncomment if running in Google Colab\n",
    "# !pip install numpy pandas matplotlib seaborn scikit-learn plotly -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, cross_validate,\n",
    "    learning_curve, validation_curve, KFold, StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve,\n",
    "    average_precision_score, matthews_corrcoef,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Better defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(\"üìò Module 2.4: Model Evaluation & Metrics\")\n",
    "print(\"üìä Ready to master performance measurement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 1: The Confusion Matrix - Foundation of Classification Metrics\n",
    "\n",
    "### Understanding the 2√ó2 Grid\n",
    "\n",
    "Every classification metric starts from the **confusion matrix**:\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "                 Negative  Positive\n",
    "Actual Negative     TN        FP     (False Positive = Type I Error)\n",
    "Actual Positive     FN        TP     (False Negative = Type II Error)\n",
    "```\n",
    "\n",
    "**The Four Outcomes**:\n",
    "\n",
    "1. **True Positive (TP)**: Predicted positive, actually positive ‚úÖ\n",
    "   - Example: Detected spam that IS spam\n",
    "\n",
    "2. **True Negative (TN)**: Predicted negative, actually negative ‚úÖ\n",
    "   - Example: Legitimate email marked as legitimate\n",
    "\n",
    "3. **False Positive (FP)**: Predicted positive, actually negative ‚ùå (Type I Error)\n",
    "   - Example: Legitimate email marked as spam\n",
    "   - **Business cost**: User misses important email!\n",
    "\n",
    "4. **False Negative (FN)**: Predicted negative, actually positive ‚ùå (Type II Error)\n",
    "   - Example: Spam email reaches inbox\n",
    "   - **Business cost**: User sees unwanted spam!\n",
    "\n",
    "**Critical Insight**: Different problems care about different errors!\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a detailed confusion matrix diagram with medical diagnosis example.\n",
    "Style: Educational medical infographic.\n",
    "2x2 grid labeled clearly:\n",
    "- Top-left (TN): Healthy person correctly diagnosed as healthy. Icon: Happy person with green checkmark.\n",
    "- Top-right (FP): Healthy person incorrectly diagnosed as sick. Icon: Worried person (false alarm). Red X.\n",
    "- Bottom-left (FN): Sick person incorrectly diagnosed as healthy. Icon: Sick person sent home. Red X with warning symbol.\n",
    "- Bottom-right (TP): Sick person correctly diagnosed as sick. Icon: Sick person getting treatment. Green checkmark.\n",
    "Axis labels: 'Predicted' (top), 'Actual' (left).\n",
    "Include percentages and counts in each cell.\n",
    "Highlight FN as 'Most Dangerous' in medical context.\n",
    "Color scheme: Green for correct, red for errors, medical blue background.\n",
    "Format: Square, clear labels, professional medical theme.\" -->\n",
    "\n",
    "Let's build a confusion matrix from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate imbalanced classification data (like spam detection)\n",
    "# 95% class 0 (legitimate), 5% class 1 (spam)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    weights=[0.95, 0.05],  # Imbalanced!\n",
    "    flip_y=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"üìä Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "print(f\"\\nClass distribution (test set):\")\n",
    "print(f\"  Class 0 (Legitimate): {np.sum(y_test == 0)} ({np.sum(y_test == 0)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Class 1 (Spam): {np.sum(y_test == 1)} ({np.sum(y_test == 1)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"\\n‚ö†Ô∏è This is HIGHLY imbalanced - accuracy will be misleading!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train two models for comparison\n",
    "\n",
    "# Model 1: Dummy \"always predict majority class\"\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_model = DummyClassifier(strategy='most_frequent')\n",
    "dummy_model.fit(X_train_scaled, y_train)\n",
    "y_pred_dummy = dummy_model.predict(X_test_scaled)\n",
    "\n",
    "# Model 2: Actual logistic regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Visualize confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Dummy model confusion matrix\n",
    "cm_dummy = confusion_matrix(y_test, y_pred_dummy)\n",
    "sns.heatmap(cm_dummy, annot=True, fmt='d', cmap='Reds', ax=axes[0],\n",
    "           cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Dummy Model (Always Predicts Legitimate)\\n' + \n",
    "                 f'Accuracy: {accuracy_score(y_test, y_pred_dummy):.1%}',\n",
    "                 fontsize=14, fontweight='bold', color='red')\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_xticklabels(['Legitimate', 'Spam'])\n",
    "axes[0].set_yticklabels(['Legitimate', 'Spam'])\n",
    "\n",
    "# Logistic regression confusion matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "           cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('Logistic Regression\\n' + \n",
    "                 f'Accuracy: {accuracy_score(y_test, y_pred_lr):.1%}',\n",
    "                 fontsize=14, fontweight='bold', color='green')\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_xticklabels(['Legitimate', 'Spam'])\n",
    "axes[1].set_yticklabels(['Legitimate', 'Spam'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed breakdown\n",
    "print(\"\\nüìä Confusion Matrix Breakdown (Logistic Regression):\")\n",
    "print(\"=\"*60)\n",
    "tn, fp, fn, tp = cm_lr.ravel()\n",
    "print(f\"True Negatives (TN):  {tn:4d} - Correctly identified legitimate emails\")\n",
    "print(f\"False Positives (FP): {fp:4d} - Legitimate emails marked as spam (BAD!)\")\n",
    "print(f\"False Negatives (FN): {fn:4d} - Spam emails that got through (BAD!)\")\n",
    "print(f\"True Positives (TP):  {tp:4d} - Correctly caught spam emails\")\n",
    "print(f\"\\nüí° Notice: High accuracy doesn't mean good spam detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 2: Core Classification Metrics\n",
    "\n",
    "### The Metric Trinity: Precision, Recall, F1\n",
    "\n",
    "From the confusion matrix, we derive three essential metrics:\n",
    "\n",
    "#### 1. Accuracy\n",
    "**Formula**: `(TP + TN) / (TP + TN + FP + FN)`\n",
    "\n",
    "**Meaning**: Proportion of correct predictions\n",
    "\n",
    "**When to use**: Balanced datasets only!\n",
    "\n",
    "**When NOT to use**: Imbalanced data (like our spam example)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Precision (Positive Predictive Value)\n",
    "**Formula**: `TP / (TP + FP)`\n",
    "\n",
    "**Question answered**: \"Of all emails I marked as spam, how many were actually spam?\"\n",
    "\n",
    "**Focus**: Minimizing false positives\n",
    "\n",
    "**High precision means**: When I say it's spam, I'm probably right\n",
    "\n",
    "**Use when**: False positives are costly (e.g., medical diagnosis)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Recall (Sensitivity, True Positive Rate)\n",
    "**Formula**: `TP / (TP + FN)`\n",
    "\n",
    "**Question answered**: \"Of all actual spam emails, how many did I catch?\"\n",
    "\n",
    "**Focus**: Minimizing false negatives\n",
    "\n",
    "**High recall means**: I catch most of the spam\n",
    "\n",
    "**Use when**: False negatives are costly (e.g., cancer screening)\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. F1-Score (Harmonic Mean of Precision and Recall)\n",
    "**Formula**: `2 √ó (Precision √ó Recall) / (Precision + Recall)`\n",
    "\n",
    "**Meaning**: Balanced measure of precision and recall\n",
    "\n",
    "**Use when**: You care about both false positives AND false negatives\n",
    "\n",
    "**Range**: 0 to 1 (higher is better)\n",
    "\n",
    "---\n",
    "\n",
    "### The Precision-Recall Tradeoff\n",
    "\n",
    "**Critical insight**: You can't maximize both!\n",
    "\n",
    "**Increase recall** (catch more spam):\n",
    "- ‚Üí More false positives (legitimate emails marked as spam)\n",
    "- ‚Üí Lower precision\n",
    "\n",
    "**Increase precision** (be more sure about spam):\n",
    "- ‚Üí Miss more spam (higher false negatives)\n",
    "- ‚Üí Lower recall\n",
    "\n",
    "**F1 score** balances this tradeoff!\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a seesaw/balance diagram showing precision-recall tradeoff.\n",
    "Style: Conceptual illustration with clear metaphor.\n",
    "Center: Fulcrum labeled 'Classification Threshold'\n",
    "Left side: Weight labeled 'PRECISION' with icon of magnifying glass (being precise/selective). Arrow pointing up.\n",
    "Right side: Weight labeled 'RECALL' with icon of wide net (catching everything). Arrow pointing down.\n",
    "Show three positions:\n",
    "1. Balanced (F1 optimal) - seesaw level\n",
    "2. High Precision - left side down, catches fewer but more accurate\n",
    "3. High Recall - right side down, catches more but less accurate\n",
    "Include annotations: 'Moving threshold changes the balance'\n",
    "Color scheme: Blue for precision, orange for recall, green for balanced.\n",
    "Format: Horizontal layout showing the tradeoff concept.\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics for our models\n",
    "\n",
    "def print_detailed_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Print comprehensive classification metrics\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä {model_name} - Detailed Metrics\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"\\nüéØ Overall Performance:\")\n",
    "    print(f\"  Accuracy:  {acc:.1%} ‚Üê Proportion of correct predictions\")\n",
    "    print(f\"  Precision: {prec:.1%} ‚Üê Of predicted spam, how many were actually spam?\")\n",
    "    print(f\"  Recall:    {rec:.1%} ‚Üê Of actual spam, how many did we catch?\")\n",
    "    print(f\"  F1-Score:  {f1:.3f} ‚Üê Harmonic mean of precision & recall\")\n",
    "    \n",
    "    # Confusion matrix breakdown\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\nüìã Confusion Matrix Breakdown:\")\n",
    "    print(f\"  True Negatives:  {tn:4d}\")\n",
    "    print(f\"  False Positives: {fp:4d} ‚Üê Legitimate emails marked as spam\")\n",
    "    print(f\"  False Negatives: {fn:4d} ‚Üê Spam emails that got through\")\n",
    "    print(f\"  True Positives:  {tp:4d}\")\n",
    "    \n",
    "    # Derived metrics\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    print(f\"\\nüîç Additional Metrics:\")\n",
    "    print(f\"  Specificity (True Negative Rate): {specificity:.1%}\")\n",
    "    print(f\"    ‚Üí Of legitimate emails, how many were correctly identified?\")\n",
    "    print(f\"  False Positive Rate: {fp/(fp+tn):.1%} ‚Üê Should be LOW\")\n",
    "    print(f\"  False Negative Rate: {fn/(fn+tp):.1%} ‚Üê Should be LOW\")\n",
    "\n",
    "# Compare both models\n",
    "print_detailed_metrics(y_test, y_pred_dummy, \"Dummy Model (Baseline)\")\n",
    "print_detailed_metrics(y_test, y_pred_lr, \"Logistic Regression\")\n",
    "\n",
    "print(f\"\\n\\nüí° Key Insight:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"The dummy model has HIGH accuracy ({accuracy_score(y_test, y_pred_dummy):.1%})\")\n",
    "print(f\"but ZERO recall (catches no spam)!\")\n",
    "print(f\"\\nLogistic Regression has slightly lower accuracy ({accuracy_score(y_test, y_pred_lr):.1%})\")\n",
    "print(f\"but much better recall ({recall_score(y_test, y_pred_lr):.1%}) - actually useful!\")\n",
    "print(f\"\\n‚≠ê This is why accuracy alone is misleading on imbalanced data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Tradeoff: Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_lr)\n",
    "avg_precision = average_precision_score(y_test, y_proba_lr)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Precision-Recall curve\n",
    "axes[0].plot(recall, precision, linewidth=2.5, color='blue', label=f'AP = {avg_precision:.3f}')\n",
    "axes[0].fill_between(recall, precision, alpha=0.2, color='blue')\n",
    "axes[0].set_xlabel('Recall (Sensitivity)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Precision (Positive Predictive Value)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Precision-Recall Curve', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[0].legend(loc='best', fontsize=12)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Add annotations\n",
    "axes[0].annotate('High Precision\\nLow Recall', xy=(0.2, 0.9), fontsize=11,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "axes[0].annotate('Low Precision\\nHigh Recall', xy=(0.8, 0.3), fontsize=11,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# Threshold vs Metrics\n",
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-8)\n",
    "axes[1].plot(thresholds, precision[:-1], linewidth=2.5, label='Precision', color='blue')\n",
    "axes[1].plot(thresholds, recall[:-1], linewidth=2.5, label='Recall', color='orange')\n",
    "axes[1].plot(thresholds, f1_scores, linewidth=2.5, label='F1-Score', color='green', linestyle='--')\n",
    "\n",
    "# Find optimal threshold (max F1)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "axes[1].axvline(x=optimal_threshold, color='red', linestyle=':', linewidth=2,\n",
    "               label=f'Optimal Threshold = {optimal_threshold:.3f}')\n",
    "\n",
    "axes[1].set_xlabel('Classification Threshold', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "axes[1].set_title('Metrics vs Decision Threshold', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[1].legend(loc='best', fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Optimal Operating Point:\")\n",
    "print(f\"={'='*60}\")\n",
    "print(f\"Best threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"At this threshold:\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision[optimal_idx]:.1%}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall[optimal_idx]:.1%}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_scores[optimal_idx]:.3f}\")\n",
    "print(f\"\\nüí° Adjust threshold based on business needs!\")\n",
    "print(f\"   Lower threshold ‚Üí Higher recall (catch more spam)\")\n",
    "print(f\"   Higher threshold ‚Üí Higher precision (fewer false alarms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 3: ROC Curves & AUC - The Gold Standard\n",
    "\n",
    "### Understanding ROC (Receiver Operating Characteristic)\n",
    "\n",
    "ROC curves plot:\n",
    "- **X-axis**: False Positive Rate (FPR) = FP / (FP + TN)\n",
    "- **Y-axis**: True Positive Rate (TPR) = TP / (TP + FN) = Recall\n",
    "\n",
    "**ROC curve shows**: Model performance across ALL possible thresholds\n",
    "\n",
    "**AUC (Area Under Curve)**:\n",
    "- **Range**: 0 to 1\n",
    "- **1.0**: Perfect classifier\n",
    "- **0.5**: Random guessing (diagonal line)\n",
    "- **< 0.5**: Worse than random (you're doing something backwards!)\n",
    "\n",
    "**Interpretation**:\n",
    "- AUC = probability that model ranks random positive higher than random negative\n",
    "- **0.9-1.0**: Excellent\n",
    "- **0.8-0.9**: Good\n",
    "- **0.7-0.8**: Fair\n",
    "- **0.6-0.7**: Poor\n",
    "- **0.5-0.6**: Fail\n",
    "\n",
    "**Why ROC/AUC?**\n",
    "‚úÖ Threshold-independent\n",
    "‚úÖ Works well with imbalanced data\n",
    "‚úÖ Single number for model comparison\n",
    "‚úÖ Widely understood in industry\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create an educational diagram explaining ROC curves with multiple classifier examples.\n",
    "Style: Professional statistical visualization.\n",
    "Main plot: ROC space with FPR (0-1) on X-axis, TPR (0-1) on Y-axis.\n",
    "Show 4 curves:\n",
    "1. Perfect classifier (L-shaped, hugging top-left corner) - labeled 'AUC = 1.0 (Perfect)'\n",
    "2. Good classifier (smooth curve above diagonal) - labeled 'AUC = 0.85 (Good)'\n",
    "3. Random classifier (diagonal line from (0,0) to (1,1)) - labeled 'AUC = 0.5 (Random)'\n",
    "4. Poor classifier (below diagonal) - labeled 'AUC = 0.3 (Inverted)'\n",
    "Shade the area under the good classifier curve.\n",
    "Add annotations: 'Better models hug the top-left corner'\n",
    "Include interpretative box explaining AUC score ranges.\n",
    "Color scheme: Green for perfect, blue for good, gray for random, red for poor.\n",
    "Format: Professional academic style, 16:9 ratio.\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot ROC curves for all models\n",
    "colors = ['blue', 'green', 'orange']\n",
    "for (name, model), color in zip(models.items(), colors):\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get probabilities\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_test_scaled)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(fpr, tpr, linewidth=2.5, color=color, \n",
    "            label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "# Plot diagonal (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate (Recall/Sensitivity)', fontsize=13, fontweight='bold')\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=15, fontweight='bold', pad=15)\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Perfect Classifier\\n(AUC = 1.0)', \n",
    "            xy=(0, 1), xytext=(0.3, 0.9),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Model Comparison via AUC:\")\n",
    "print(\"=\"*60)\n",
    "for name, model in models.items():\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"{name:20s}: AUC = {auc:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"  ‚Ä¢ Closer to top-left corner = better model\")\n",
    "print(\"  ‚Ä¢ AUC is threshold-independent metric\")\n",
    "print(\"  ‚Ä¢ Use AUC to compare models, then tune threshold for deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 4: Cross-Validation - Proper Model Assessment\n",
    "\n",
    "### The Problem with Single Train/Test Split\n",
    "\n",
    "**Scenario**: You split your data once, train model, get 90% accuracy.\n",
    "\n",
    "**Questions**:\n",
    "- Was this a lucky split?\n",
    "- Would performance hold on different data?\n",
    "- Did you overfit to this particular test set?\n",
    "\n",
    "**Answer**: You don't know! One split is not enough.\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "**The solution**: Test on multiple different splits!\n",
    "\n",
    "**Process** (5-fold example):\n",
    "1. Split data into 5 equal folds\n",
    "2. Train on folds 1-4, test on fold 5\n",
    "3. Train on folds 1-3 & 5, test on fold 4\n",
    "4. Train on folds 1-2 & 4-5, test on fold 3\n",
    "5. Train on folds 2-5, test on fold 1\n",
    "6. Train on folds 1 & 3-5, test on fold 2\n",
    "\n",
    "**Result**: 5 different performance scores ‚Üí average & standard deviation\n",
    "\n",
    "**Benefits**:\n",
    "‚úÖ More robust performance estimate\n",
    "‚úÖ Every sample is used for both training and testing\n",
    "‚úÖ Reduces variance in performance estimate\n",
    "‚úÖ Detects overfitting\n",
    "\n",
    "**Stratified K-Fold**: Maintains class distribution in each fold (important for imbalanced data!)\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a diagram showing 5-fold cross-validation process.\n",
    "Style: Educational flowchart with clear visual representation.\n",
    "Show dataset as horizontal bar divided into 5 equal segments (Fold 1-5).\n",
    "Display 5 iterations vertically:\n",
    "Iteration 1: Folds 1-4 in blue (training), Fold 5 in orange (testing). Arrow to 'Score 1'\n",
    "Iteration 2: Folds 1-3,5 in blue, Fold 4 in orange. Arrow to 'Score 2'\n",
    "... and so on for all 5 iterations\n",
    "Bottom: Show aggregation of 5 scores into 'Mean Score ¬± Std Dev'\n",
    "Include annotations: 'Each fold used exactly once for testing'\n",
    "Color scheme: Blue for training, orange for testing, green for final result.\n",
    "Add icons showing model training and evaluation at each step.\n",
    "Format: Vertical flow diagram, clear progression.\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate cross-validation\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Use Random Forest for this demo\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "\n",
    "# Regular K-Fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_regular = cross_val_score(rf_model, X_train_scaled, y_train, \n",
    "                                   cv=kfold, scoring='accuracy')\n",
    "\n",
    "# Stratified K-Fold (better for imbalanced data)\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_stratified = cross_val_score(rf_model, X_train_scaled, y_train, \n",
    "                                      cv=skfold, scoring='accuracy')\n",
    "\n",
    "# Multiple metrics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "cv_results = cross_validate(rf_model, X_train_scaled, y_train, \n",
    "                           cv=skfold, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print(\"\\nüìä Cross-Validation Results (5-Fold):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRegular K-Fold Accuracy:\")\n",
    "print(f\"  Scores: {[f'{s:.3f}' for s in cv_scores_regular]}\")\n",
    "print(f\"  Mean:   {cv_scores_regular.mean():.3f} ¬± {cv_scores_regular.std():.3f}\")\n",
    "\n",
    "print(f\"\\nStratified K-Fold Accuracy:\")\n",
    "print(f\"  Scores: {[f'{s:.3f}' for s in cv_scores_stratified]}\")\n",
    "print(f\"  Mean:   {cv_scores_stratified.mean():.3f} ¬± {cv_scores_stratified.std():.3f}\")\n",
    "\n",
    "print(f\"\\nüìà Multiple Metrics (Stratified K-Fold):\")\n",
    "print(f\"=\"*70)\n",
    "for metric in scoring:\n",
    "    test_scores = cv_results[f'test_{metric}']\n",
    "    train_scores = cv_results[f'train_{metric}']\n",
    "    print(f\"\\n{metric.upper():12s}:\")\n",
    "    print(f\"  Train: {train_scores.mean():.3f} ¬± {train_scores.std():.3f}\")\n",
    "    print(f\"  Test:  {test_scores.mean():.3f} ¬± {test_scores.std():.3f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    gap = train_scores.mean() - test_scores.mean()\n",
    "    if gap > 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  Large train-test gap ({gap:.3f}) ‚Üí Possible overfitting!\")\n",
    "\n",
    "# Visualize CV scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Box plot of scores\n",
    "metrics_data = [cv_results[f'test_{m}'] for m in scoring]\n",
    "axes[0].boxplot(metrics_data, labels=[m.upper() for m in scoring])\n",
    "axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Cross-Validation Score Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].set_xticklabels([m.upper().replace('_', ' ') for m in scoring], rotation=15, ha='right')\n",
    "\n",
    "# Train vs Test comparison\n",
    "x_pos = np.arange(len(scoring))\n",
    "train_means = [cv_results[f'train_{m}'].mean() for m in scoring]\n",
    "test_means = [cv_results[f'test_{m}'].mean() for m in scoring]\n",
    "\n",
    "axes[1].bar(x_pos - 0.2, train_means, 0.4, label='Train', alpha=0.8, color='skyblue')\n",
    "axes[1].bar(x_pos + 0.2, test_means, 0.4, label='Test', alpha=0.8, color='lightcoral')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([m.upper().replace('_', ' ') for m in scoring], rotation=15, ha='right')\n",
    "axes[1].set_ylabel('Mean Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Train vs Test Performance', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Standard deviation tells us score stability\")\n",
    "print(\"  ‚Ä¢ Large train-test gap indicates overfitting\")\n",
    "print(\"  ‚Ä¢ Always use stratified CV for classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 5: Bias-Variance Tradeoff - Understanding Model Complexity\n",
    "\n",
    "### The Fundamental Tradeoff\n",
    "\n",
    "**Every model** faces this dilemma:\n",
    "\n",
    "#### High Bias (Underfitting)\n",
    "- Model too simple\n",
    "- Doesn't capture patterns\n",
    "- Poor on training AND test data\n",
    "- Example: Linear model for non-linear data\n",
    "\n",
    "#### High Variance (Overfitting)\n",
    "- Model too complex\n",
    "- Memorizes training data (including noise)\n",
    "- Great on training, poor on test data\n",
    "- Example: Deep decision tree on small dataset\n",
    "\n",
    "#### The Sweet Spot\n",
    "- Model complexity just right\n",
    "- Captures true patterns, ignores noise\n",
    "- Good on both training and test data\n",
    "\n",
    "**Mathematically**:\n",
    "```\n",
    "Total Error = Bias¬≤ + Variance + Irreducible Error\n",
    "```\n",
    "\n",
    "**The Tradeoff**:\n",
    "- Decrease bias ‚Üí Increase variance\n",
    "- Decrease variance ‚Üí Increase bias\n",
    "- Goal: Minimize total error\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a comprehensive bias-variance tradeoff diagram.\n",
    "Style: Professional statistical illustration with clear zones.\n",
    "Main plot: U-shaped curve showing Total Error vs Model Complexity.\n",
    "Show three curves:\n",
    "1. Bias (decreasing from left to right) - red dashed line\n",
    "2. Variance (increasing from left to right) - blue dashed line  \n",
    "3. Total Error (U-shaped, sum of bias and variance) - solid black line\n",
    "Mark optimal complexity point at the minimum of U-curve.\n",
    "Three annotated zones:\n",
    "- Left: 'Underfitting' (high bias, low variance) - simple model struggling with complex data\n",
    "- Center: 'Just Right' (balanced) - model fitting data appropriately\n",
    "- Right: 'Overfitting' (low bias, high variance) - complex model memorizing noise\n",
    "Include small scatter plots in each zone showing example fits.\n",
    "Color scheme: Red for bias, blue for variance, green for optimal zone.\n",
    "Format: Wide horizontal layout with clear annotations.\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate bias-variance tradeoff with learning curves\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_learning_curve(estimator, X, y, title, ylim=None):\n",
    "    \"\"\"\n",
    "    Generate learning curve plot showing training and validation scores\n",
    "    vs training set size.\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=5, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='accuracy',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,\n",
    "                    alpha=0.2, color='blue')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,\n",
    "                    alpha=0.2, color='orange')\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', linewidth=2.5,\n",
    "            label='Training score')\n",
    "    plt.plot(train_sizes, val_mean, 'o-', color='orange', linewidth=2.5,\n",
    "            label='Cross-validation score')\n",
    "    \n",
    "    plt.xlabel('Training Set Size', fontsize=13, fontweight='bold')\n",
    "    plt.ylabel('Accuracy Score', fontsize=13, fontweight='bold')\n",
    "    plt.title(title, fontsize=15, fontweight='bold', pad=15)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "    \n",
    "    # Analyze the gap\n",
    "    final_gap = train_mean[-1] - val_mean[-1]\n",
    "    if final_gap > 0.1:\n",
    "        plt.text(train_sizes[-1]*0.5, train_mean[-1]*0.95,\n",
    "                f'‚ö†Ô∏è High Variance\\n(Gap: {final_gap:.2%})',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "                fontsize=11, fontweight='bold')\n",
    "    elif val_mean[-1] < 0.7:\n",
    "        plt.text(train_sizes[-1]*0.5, val_mean[-1]*1.1,\n",
    "                f'‚ö†Ô∏è High Bias\\n(Low score: {val_mean[-1]:.2%})',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7),\n",
    "                fontsize=11, fontweight='bold')\n",
    "    else:\n",
    "        plt.text(train_sizes[-1]*0.5, val_mean[-1]*1.05,\n",
    "                f'‚úÖ Good Balance\\n(Gap: {final_gap:.2%})',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "                fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return train_mean, val_mean, final_gap\n",
    "\n",
    "# Compare three models with different complexity\n",
    "print(\"\\nüî¨ Analyzing Bias-Variance Tradeoff...\\n\")\n",
    "\n",
    "# High bias model (too simple)\n",
    "simple_model = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "train_m, val_m, gap = plot_learning_curve(\n",
    "    simple_model, X_train_scaled, y_train,\n",
    "    'High Bias: Shallow Decision Tree (max_depth=2)'\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Simple Model - Train: {train_m[-1]:.3f}, Val: {val_m[-1]:.3f}, Gap: {gap:.3f}\")\n",
    "print(\"‚Üí Low training score indicates UNDERFITTING (high bias)\\n\")\n",
    "\n",
    "# Balanced model\n",
    "balanced_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "train_m, val_m, gap = plot_learning_curve(\n",
    "    balanced_model, X_train_scaled, y_train,\n",
    "    'Balanced: Medium Decision Tree (max_depth=5)'\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Balanced Model - Train: {train_m[-1]:.3f}, Val: {val_m[-1]:.3f}, Gap: {gap:.3f}\")\n",
    "print(\"‚Üí Small gap between train and validation indicates GOOD FIT\\n\")\n",
    "\n",
    "# High variance model (too complex)\n",
    "complex_model = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "train_m, val_m, gap = plot_learning_curve(\n",
    "    complex_model, X_train_scaled, y_train,\n",
    "    'High Variance: Deep Decision Tree (max_depth=20)'\n",
    ")\n",
    "plt.show()\n",
    "print(f\"Complex Model - Train: {train_m[-1]:.3f}, Val: {val_m[-1]:.3f}, Gap: {gap:.3f}\")\n",
    "print(\"‚Üí Large gap indicates OVERFITTING (high variance)\\n\")\n",
    "\n",
    "print(\"\\nüí° Learning Curve Interpretation:\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìà Both curves converge at high level ‚Üí Good model\")\n",
    "print(\"üìâ Both curves converge at low level ‚Üí Underfitting (add complexity)\")\n",
    "print(\"üìä Large gap between curves ‚Üí Overfitting (reduce complexity/add data)\")\n",
    "print(\"üîÑ Curves haven't converged ‚Üí Need more data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 6: Business Metric Selection - Choosing What Matters\n",
    "\n",
    "### The Business Context Matters!\n",
    "\n",
    "**Same problem, different metrics based on business needs:**\n",
    "\n",
    "#### Case 1: Medical Diagnosis (Cancer Screening)\n",
    "**Priority**: Don't miss actual cancers (minimize False Negatives)\n",
    "\n",
    "**Best metric**: **Recall (Sensitivity)**\n",
    "- Better to have false alarms than miss cancer\n",
    "- False positives ‚Üí extra tests (acceptable)\n",
    "- False negatives ‚Üí missed treatment (catastrophic)\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 2: Spam Detection\n",
    "**Priority**: Don't mark legitimate emails as spam (minimize False Positives)\n",
    "\n",
    "**Best metric**: **Precision**\n",
    "- Missing some spam is annoying\n",
    "- Blocking important email is unacceptable\n",
    "- Better to let spam through than block legitimate email\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 3: Fraud Detection\n",
    "**Priority**: Balance both (catch fraud without annoying customers)\n",
    "\n",
    "**Best metric**: **F1-Score** or **Precision-Recall AUC**\n",
    "- Need to catch fraud (recall)\n",
    "- Can't flag legitimate transactions (precision)\n",
    "- Both matter equally\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 4: Recommendation Systems\n",
    "**Priority**: User engagement and satisfaction\n",
    "\n",
    "**Best metric**: **Business KPIs**\n",
    "- Click-through rate (CTR)\n",
    "- Time on platform\n",
    "- Revenue per user\n",
    "- Traditional ML metrics are secondary!\n",
    "\n",
    "### The Decision Framework\n",
    "\n",
    "| Scenario | False Positive Cost | False Negative Cost | Best Metric |\n",
    "|----------|-------------------|-------------------|-------------|\n",
    "| Cancer screening | Low (extra tests) | **Very High** (death) | **Recall** |\n",
    "| Spam filter | **High** (missed email) | Low (see spam) | **Precision** |\n",
    "| Fraud detection | Medium (customer friction) | Medium (lost money) | **F1-Score** |\n",
    "| Loan approval | Low (missed profit) | **High** (default loss) | **Precision** |\n",
    "| Marketing campaign | Low (wasted ad spend) | Low (missed sale) | **Balanced** |\n",
    "\n",
    "**Key Questions to Ask**:\n",
    "1. What's the cost of a false positive?\n",
    "2. What's the cost of a false negative?\n",
    "3. Which error is more acceptable?\n",
    "4. What does the business actually care about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate metric selection for different scenarios\n",
    "\n",
    "def evaluate_for_scenario(y_true, y_pred, y_proba, scenario_name, primary_metric):\n",
    "    \"\"\"\n",
    "    Evaluate model focusing on the metric that matters for the scenario\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìã Scenario: {scenario_name}\")\n",
    "    print(f\"üéØ Primary Metric: {primary_metric}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Print with emphasis on primary metric\n",
    "    metrics = {\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': auc\n",
    "    }\n",
    "    \n",
    "    for metric_name, value in metrics.items():\n",
    "        marker = \"‚≠ê\" if metric_name == primary_metric else \"  \"\n",
    "        print(f\"{marker} {metric_name:12s}: {value:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Error Analysis:\")\n",
    "    print(f\"   False Positives: {fp:4d}\")\n",
    "    print(f\"   False Negatives: {fn:4d}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Train model once\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate for different scenarios\n",
    "scenarios = [\n",
    "    (\"Cancer Screening (Minimize Missed Cases)\", \"Recall\"),\n",
    "    (\"Spam Filter (Minimize False Alarms)\", \"Precision\"),\n",
    "    (\"Fraud Detection (Balance Both Errors)\", \"F1-Score\"),\n",
    "    (\"General Classification\", \"ROC-AUC\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for scenario, metric in scenarios:\n",
    "    result = evaluate_for_scenario(y_test, y_pred, y_proba, scenario, metric)\n",
    "    results.append(result)\n",
    "\n",
    "print(f\"\\n\\nüí° Key Takeaway:\")\n",
    "print(f\"={'='*70}\")\n",
    "print(f\"The SAME model can be 'good' or 'bad' depending on:\")\n",
    "print(f\"  1. What metric you prioritize\")\n",
    "print(f\"  2. The business context and costs\")\n",
    "print(f\"  3. Your tolerance for different error types\")\n",
    "print(f\"\\n‚≠ê Always align metrics with business objectives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: Real-World Metric Selection\n",
    "\n",
    "**Objective**: Practice choosing the right metric for different business problems\n",
    "\n",
    "**Scenarios**: For each scenario below, identify:\n",
    "1. The primary metric to optimize\n",
    "2. Why that metric matters most\n",
    "3. What threshold adjustment might be needed\n",
    "\n",
    "**Scenario A: Email Phishing Detection**\n",
    "- Detect phishing emails in corporate inbox\n",
    "- False positive: Legitimate email blocked (might miss important business communication)\n",
    "- False negative: Phishing email delivered (employee might get compromised)\n",
    "\n",
    "**Scenario B: Credit Card Fraud Real-Time Detection**\n",
    "- Block suspicious transactions instantly\n",
    "- False positive: Legitimate purchase declined (customer frustrated)\n",
    "- False negative: Fraudulent charge goes through (bank loses money)\n",
    "\n",
    "**Scenario C: Automated Resume Screening**\n",
    "- Filter candidates for interviews\n",
    "- False positive: Unqualified candidate interviewed (wasted time)\n",
    "- False negative: Qualified candidate rejected (lose talent)\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint: Think about business impact</summary>\n",
    "\n",
    "Consider:\n",
    "- Which error is more costly?\n",
    "- Can you recover from the error?\n",
    "- What's the user experience impact?\n",
    "</details>\n",
    "\n",
    "**Your Analysis**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your analysis\n",
    "my_analysis = {\n",
    "    'Scenario A - Phishing Detection': {\n",
    "        'Primary Metric': '',  # Precision, Recall, F1, etc.\n",
    "        'Reasoning': '',\n",
    "        'Threshold Adjustment': ''  # Higher, lower, or balanced?\n",
    "    },\n",
    "    'Scenario B - Fraud Detection': {\n",
    "        'Primary Metric': '',\n",
    "        'Reasoning': '',\n",
    "        'Threshold Adjustment': ''\n",
    "    },\n",
    "    'Scenario C - Resume Screening': {\n",
    "        'Primary Metric': '',\n",
    "        'Reasoning': '',\n",
    "        'Threshold Adjustment': ''\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print your analysis\n",
    "for scenario, analysis in my_analysis.items():\n",
    "    print(f\"\\n{scenario}\")\n",
    "    print(\"=\"*60)\n",
    "    for key, value in analysis.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Cross-Validation Deep Dive\n",
    "\n",
    "**Objective**: Master cross-validation and detect overfitting\n",
    "\n",
    "**Task**:\n",
    "1. Create a dataset with 500 samples\n",
    "2. Train three models with different complexities:\n",
    "   - Simple: DecisionTreeClassifier(max_depth=3)\n",
    "   - Medium: DecisionTreeClassifier(max_depth=8)\n",
    "   - Complex: DecisionTreeClassifier(max_depth=None)\n",
    "3. Use 5-fold cross-validation to evaluate each model\n",
    "4. Plot training vs validation scores for each model\n",
    "5. Identify which model is:\n",
    "   - Underfitting\n",
    "   - Well-fitted\n",
    "   - Overfitting\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint: Detecting overfitting</summary>\n",
    "\n",
    "Overfitting indicators:\n",
    "- High training score, low validation score\n",
    "- Large gap between training and validation\n",
    "- Validation score decreases with model complexity\n",
    "</details>\n",
    "\n",
    "**Bonus Challenge**: \n",
    "- Create learning curves for each model\n",
    "- Determine if more data would help each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Implement cross-validation comparison\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "You've mastered model evaluation and metrics!\n",
    "\n",
    "- ‚úÖ **Confusion Matrix Fundamentals**:\n",
    "  - TP, TN, FP, FN form the foundation of all classification metrics\n",
    "  - Understanding error types is crucial for metric selection\n",
    "  - Visualize confusion matrices to understand model behavior\n",
    "\n",
    "- ‚úÖ **Classification Metrics**:\n",
    "  - **Accuracy**: Only useful for balanced datasets\n",
    "  - **Precision**: Minimize false positives (\"When I say yes, I'm right\")\n",
    "  - **Recall**: Minimize false negatives (\"I catch most of the positives\")\n",
    "  - **F1-Score**: Harmonic mean balancing precision and recall\n",
    "  - Each metric serves a different business need!\n",
    "\n",
    "- ‚úÖ **ROC Curves & AUC**:\n",
    "  - Threshold-independent evaluation\n",
    "  - AUC summarizes performance across all thresholds\n",
    "  - Perfect for comparing models objectively\n",
    "  - Use precision-recall curves for imbalanced data\n",
    "\n",
    "- ‚úÖ **Cross-Validation**:\n",
    "  - Single train/test split is unreliable\n",
    "  - K-fold CV provides robust performance estimates\n",
    "  - Stratified CV maintains class distribution\n",
    "  - Always report mean ¬± standard deviation\n",
    "\n",
    "- ‚úÖ **Bias-Variance Tradeoff**:\n",
    "  - Underfitting (high bias): Model too simple\n",
    "  - Overfitting (high variance): Model too complex\n",
    "  - Learning curves reveal which problem you have\n",
    "  - Balance comes from proper model complexity\n",
    "\n",
    "- ‚úÖ **Business Metric Selection**:\n",
    "  - **Context matters more than the metric itself**\n",
    "  - Align metrics with business costs and objectives\n",
    "  - Different scenarios need different metrics\n",
    "  - Always ask: \"What does the business actually care about?\"\n",
    "\n",
    "### ü§î The Big Picture:\n",
    "\n",
    "**Model Evaluation Workflow**:\n",
    "1. ‚úÖ Understand the business problem and costs\n",
    "2. ‚úÖ Choose appropriate metrics (not just accuracy!)\n",
    "3. ‚úÖ Use cross-validation for robust estimates\n",
    "4. ‚úÖ Check for overfitting/underfitting with learning curves\n",
    "5. ‚úÖ Tune decision threshold based on business needs\n",
    "6. ‚úÖ Monitor performance on production data\n",
    "\n",
    "**Remember**: \n",
    "> \"A model optimizing the wrong metric is worse than no model at all!\"\n",
    "\n",
    "**Always**:\n",
    "- Question if accuracy is meaningful for your data\n",
    "- Understand the cost of different errors\n",
    "- Use multiple metrics to get the complete picture\n",
    "- Validate properly before deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Learning\n",
    "\n",
    "**Recommended Reading**:\n",
    "- [Scikit-learn Metrics Guide](https://scikit-learn.org/stable/modules/model_evaluation.html) - Comprehensive documentation\n",
    "- [ROC Curves Explained](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) - Google's ML Crash Course\n",
    "- [Cross-Validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html) - Official scikit-learn tutorial\n",
    "\n",
    "**Video Tutorials**:\n",
    "- [StatQuest: ROC and AUC](https://www.youtube.com/watch?v=4jRBRDbJemM) - Crystal clear explanation\n",
    "- [Precision and Recall](https://www.youtube.com/watch?v=jJ7ff7Gcq34) - Visual walkthrough\n",
    "- [Cross-Validation](https://www.youtube.com/watch?v=fSytzGwwBVw) - Practical demonstration\n",
    "\n",
    "**Deep Dives**:\n",
    "- [Imbalanced Learning](https://imbalanced-learn.org/stable/) - Handling imbalanced datasets\n",
    "- [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) - Alternative to F1\n",
    "- [Calibration](https://scikit-learn.org/stable/modules/calibration.html) - Probability calibration\n",
    "\n",
    "**Interactive Tools**:\n",
    "- [ROC Curve Visualizer](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/) - Interactive demo\n",
    "- [Confusion Matrix Calculator](https://www.machinelearningplus.com/statistics/confusion-matrix-explained/) - Hands-on tool\n",
    "\n",
    "**Research Papers**:\n",
    "- [The Relationship Between Precision-Recall and ROC Curves](https://www.biostat.wisc.edu/~page/rocpr.pdf) - When to use which\n",
    "- [A survey of cross-validation procedures](https://arxiv.org/abs/0907.4728) - Advanced techniques\n",
    "\n",
    "**Case Studies**:\n",
    "- [Netflix Prize](https://www.netflixprize.com/) - Real-world evaluation challenges\n",
    "- [Kaggle Evaluation Metrics](https://www.kaggle.com/learn/intro-to-machine-learning) - Competition metrics\n",
    "\n",
    "**Tools & Libraries**:\n",
    "- [Yellowbrick](https://www.scikit-yb.org/) - Visual diagnostic tools\n",
    "- [PyCaret](https://pycaret.org/) - Automated model evaluation\n",
    "- [MLflow](https://mlflow.org/) - Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚û°Ô∏è What's Next?\n",
    "\n",
    "üéâ **Congratulations!** You've completed **Module 2: Machine Learning Fundamentals**!\n",
    "\n",
    "You've mastered:\n",
    "- ‚úÖ Supervised learning (classification & regression)\n",
    "- ‚úÖ Unsupervised learning (clustering)\n",
    "- ‚úÖ Model evaluation & selection\n",
    "- ‚úÖ Core ML concepts and best practices\n",
    "\n",
    "**In Module 3: Neural Networks Demystified**, you'll discover:\n",
    "\n",
    "**Chapter 3.1 - Biological Neurons**:\n",
    "- How the brain inspired artificial neural networks\n",
    "- From biological neurons to mathematical models\n",
    "- The perceptron: First artificial neuron\n",
    "\n",
    "**Chapter 3.2 - Building Your First Neural Network**:\n",
    "- Implementing neural networks from scratch\n",
    "- Understanding forward propagation\n",
    "- PyTorch basics and tensor operations\n",
    "\n",
    "**Chapter 3.3 - Activation Functions & Backpropagation**:\n",
    "- Why networks need non-linearity\n",
    "- The mathematics of learning (chain rule!)\n",
    "- Gradient descent and optimization\n",
    "\n",
    "**Chapter 3.4 - Deep Learning Architectures**:\n",
    "- CNNs for computer vision\n",
    "- RNNs for sequences\n",
    "- Modern architectures and design patterns\n",
    "\n",
    "From classical ML to deep learning‚Äîthe journey continues! üß†\n",
    "\n",
    "Ready to dive into neural networks? Open **[Chapter 3.1 - Biological Neurons](../module-3/3.1-biological-neurons.ipynb)**!\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Feedback & Community\n",
    "\n",
    "**Questions?** Join our [Discord community](https://discord.gg/madeforai)\n",
    "\n",
    "**Found a bug?** [Open an issue on GitHub](https://github.com/madeforai/madeforai/issues)\n",
    "\n",
    "**Share your evaluation insights!** Tweet with #MadeForAI\n",
    "\n",
    "**Keep learning!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
