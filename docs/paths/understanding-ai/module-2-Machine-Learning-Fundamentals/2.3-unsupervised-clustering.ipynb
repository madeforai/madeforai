{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 - Unsupervised Learning & Clustering: Finding Hidden Patterns\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/understanding-ai/module-2/2.3-unsupervised-clustering.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Discover patterns without labels‚Äîmaster clustering algorithms that find structure in the wild.**\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- **Unsupervised learning fundamentals**: When you don't have labels and why that's powerful\n",
    "- **K-Means clustering**: The workhorse algorithm for grouping similar data\n",
    "- **DBSCAN**: Density-based clustering for complex, non-spherical shapes\n",
    "- **Hierarchical clustering**: Building cluster trees to understand data structure\n",
    "- **Evaluation metrics**: How to measure cluster quality without ground truth\n",
    "- **Real-world applications**: Customer segmentation, anomaly detection, data exploration\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "40-45 minutes\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Completed Chapter 2.2 (Classification vs Regression)\n",
    "- Understanding of supervised learning concepts\n",
    "- Basic familiarity with distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î The World Without Labels\n",
    "\n",
    "Imagine you're exploring a massive dataset from a new e-commerce platform. You have:\n",
    "- Customer behavior data\n",
    "- Purchase patterns\n",
    "- Browsing history\n",
    "\n",
    "**But here's the catch**: Nobody has labeled this data. No \"high-value customer\" tags, no \"product category\" classifications.\n",
    "\n",
    "**Your mission**: Find meaningful groups of similar customers to personalize marketing.\n",
    "\n",
    "This is **unsupervised learning**‚Äîdiscovering structure without guidance.\n",
    "\n",
    "### Why Unsupervised Learning?\n",
    "\n",
    "**Reality check**: Most real-world data is unlabeled.\n",
    "\n",
    "| Scenario | Labeling Challenge |\n",
    "|----------|--------------------|\n",
    "| **Medical imaging** | Requires expert radiologists (expensive!) |\n",
    "| **Customer data** | Behavior emerges over time (can't pre-label) |\n",
    "| **Anomaly detection** | Anomalies haven't happened yet |\n",
    "| **Data exploration** | You don't know what patterns exist |\n",
    "\n",
    "**Unsupervised learning says**: \"Let the data speak for itself.\"\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a comparison illustration showing supervised vs unsupervised learning.\n",
    "Style: Modern, clean infographic.\n",
    "Left side (Supervised): Labeled data points with clear categories (red circles, blue squares), connected with arrows to predictions. Show 'Training Labels' box feeding into the model.\n",
    "Right side (Unsupervised): Unlabeled gray points naturally clustering into groups with dotted boundaries around them. Show 'No Labels' with a question mark.\n",
    "Center: Arrow showing transformation from unlabeled to discovered clusters.\n",
    "Color scheme: Blue/orange gradient.\n",
    "Include title: 'Supervised vs Unsupervised Learning'\n",
    "Format: Horizontal comparison, 16:9 ratio.\" -->\n",
    "\n",
    "**The Core Challenge**: How do we measure success when there's no \"correct answer\" to compare against?\n",
    "\n",
    "**Answer**: We evaluate based on:\n",
    "1. **Cohesion**: How similar are items within a cluster?\n",
    "2. **Separation**: How distinct are different clusters?\n",
    "3. **Business value**: Do clusters correspond to actionable insights?\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import libraries\n",
    "# Uncomment if running in Google Colab\n",
    "# !pip install numpy pandas matplotlib seaborn scikit-learn plotly scipy -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, \n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up better figure defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(\"üìò Module 2.3: Unsupervised Learning & Clustering\")\n",
    "print(\"üîç Ready to discover hidden patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 1: K-Means Clustering - The Classic Algorithm\n",
    "\n",
    "### How K-Means Works\n",
    "\n",
    "K-Means is like organizing a party:\n",
    "\n",
    "1. **Choose K hosts** (cluster centers) randomly\n",
    "2. **Each guest** sits with their nearest host\n",
    "3. **Hosts move** to the center of their group\n",
    "4. **Repeat** until hosts stop moving\n",
    "\n",
    "**Mathematically**:\n",
    "- Minimize within-cluster sum of squares (WCSS)\n",
    "- Each point assigned to nearest centroid\n",
    "- Centroids updated as mean of assigned points\n",
    "\n",
    "**Key Insight**: K-Means assumes spherical clusters of similar size. When this assumption breaks, K-Means struggles!\n",
    "\n",
    "### The Algorithm in 4 Steps\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a 4-panel sequential illustration showing K-Means algorithm steps.\n",
    "Style: Educational diagram with clean geometry.\n",
    "Panel 1 (Step 1): Random scattered data points (gray) with 3 randomly placed centroids (red stars). Title: '1. Initialize K centroids'\n",
    "Panel 2 (Step 2): Points colored by nearest centroid (3 different colors). Dotted lines connecting points to nearest centroid. Title: '2. Assign points to nearest centroid'\n",
    "Panel 3 (Step 3): Centroids moved to center of their colored groups. Arrows showing movement. Title: '3. Update centroids to cluster means'\n",
    "Panel 4 (Step 4): Final stable clusters with well-separated groups. Title: '4. Repeat until convergence'\n",
    "Color scheme: Blue, orange, green for the 3 clusters.\n",
    "Include legend showing centroid and data point symbols.\n",
    "Format: 2x2 grid layout, square overall aspect ratio.\" -->\n",
    "\n",
    "Let's implement this from scratch to understand it deeply!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data: Customer segments based on purchase behavior\n",
    "# Features: Average purchase value and purchase frequency\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=300,\n",
    "    centers=4,\n",
    "    n_features=2,\n",
    "    cluster_std=0.60,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Scale the data (important for distance-based algorithms!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Visualize raw data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.6, c='gray', s=50)\n",
    "plt.title('Unlabeled Customer Data', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Average Purchase Value ($)', fontsize=12)\n",
    "plt.ylabel('Purchase Frequency (per month)', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.6, c=y_true, cmap='husl', s=50)\n",
    "plt.title('True Groups (Unknown in Real World!)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Average Purchase Value ($)', fontsize=12)\n",
    "plt.ylabel('Purchase Frequency (per month)', fontsize=12)\n",
    "plt.colorbar(label='True Cluster')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Notice: We can see patterns by eye, but K-Means will find them mathematically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing K: The Elbow Method\n",
    "\n",
    "**The hardest question**: How many clusters should we have?\n",
    "\n",
    "**The Elbow Method**:\n",
    "1. Try different values of K (e.g., 2-10)\n",
    "2. Calculate inertia (within-cluster sum of squares) for each K\n",
    "3. Plot inertia vs K\n",
    "4. Look for the \"elbow\"‚Äîwhere adding more clusters gives diminishing returns\n",
    "\n",
    "**Think of it like**: Adding more party hosts. Initially, each new host dramatically improves the party (less crowding). Eventually, adding hosts barely helps.\n",
    "\n",
    "The \"elbow\" is where the benefit plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Elbow Method: Finding optimal K\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    # Inertia: sum of squared distances to nearest cluster center\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Silhouette score: measures how similar points are to their own cluster\n",
    "    # vs other clusters (range: -1 to 1, higher is better)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Visualize the elbow\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Elbow curve (Inertia)\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2.5, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Inertia (WCSS)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Elbow Method: Inertia', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Optimal K=4')\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Plot 2: Silhouette score\n",
    "axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2.5, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=13, fontweight='bold')\n",
    "axes[1].set_title('Silhouette Analysis', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Peak at K=4')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis:\")\n",
    "print(f\"‚Ä¢ Elbow appears around K=4 (inertia curve flattens)\")\n",
    "print(f\"‚Ä¢ Silhouette score peaks at K=4 ({silhouette_scores[2]:.3f})\")\n",
    "print(f\"‚Ä¢ Both methods agree: K=4 is optimal!\")\n",
    "print(\"\\nüí° Tip: When methods disagree, consider business context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying K-Means with Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with K=4\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Transform centroids back to original scale for interpretation\n",
    "centroids_original = scaler.inverse_transform(centroids)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='husl', \n",
    "                     alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.scatter(centroids_original[:, 0], centroids_original[:, 1], \n",
    "           c='red', marker='X', s=300, edgecolors='black', linewidth=2,\n",
    "           label='Centroids')\n",
    "plt.title('K-Means Clustering Result (K=4)', fontsize=15, fontweight='bold', pad=15)\n",
    "plt.xlabel('Average Purchase Value ($)', fontsize=12)\n",
    "plt.ylabel('Purchase Frequency (per month)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show confusion-style comparison with true labels\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, clusters)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title('Cluster Assignments vs True Labels', fontsize=15, fontweight='bold', pad=15)\n",
    "plt.xlabel('K-Means Cluster', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret cluster centers\n",
    "print(\"\\nüéØ Customer Segment Insights:\")\n",
    "print(\"=\"*60)\n",
    "for i, center in enumerate(centroids_original):\n",
    "    print(f\"\\nüìä Cluster {i}:\")\n",
    "    print(f\"   ‚Ä¢ Average Purchase: ${center[0]:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Purchase Frequency: {center[1]:.2f} times/month\")\n",
    "    print(f\"   ‚Ä¢ Size: {np.sum(clusters == i)} customers\")\n",
    "    \n",
    "    # Business interpretation\n",
    "    if center[0] > X[:, 0].mean() and center[1] > X[:, 1].mean():\n",
    "        print(f\"   üíé **Premium Frequent Buyers** - High value, high engagement\")\n",
    "    elif center[0] > X[:, 0].mean() and center[1] < X[:, 1].mean():\n",
    "        print(f\"   üéØ **Occasional Big Spenders** - High value, low frequency\")\n",
    "    elif center[0] < X[:, 0].mean() and center[1] > X[:, 1].mean():\n",
    "        print(f\"   üîÑ **Frequent Small Buyers** - Low value, high engagement\")\n",
    "    else:\n",
    "        print(f\"   üò¥ **Low Engagement** - Consider re-engagement campaigns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 2: DBSCAN - Density-Based Clustering\n",
    "\n",
    "### When K-Means Fails\n",
    "\n",
    "K-Means has a fatal flaw: it assumes spherical clusters!\n",
    "\n",
    "**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** solves this by:\n",
    "- Finding regions of high density\n",
    "- Connecting nearby dense regions\n",
    "- Marking sparse points as noise/outliers\n",
    "\n",
    "**Key Parameters**:\n",
    "- `eps` (epsilon): Maximum distance between two points to be neighbors\n",
    "- `min_samples`: Minimum points to form a dense region\n",
    "\n",
    "**Advantages over K-Means**:\n",
    "‚úÖ Handles arbitrary shapes (moons, rings, spirals)\n",
    "‚úÖ Automatically detects outliers\n",
    "‚úÖ No need to specify number of clusters\n",
    "\n",
    "**Disadvantages**:\n",
    "‚ùå Sensitive to parameter tuning\n",
    "‚ùå Struggles with varying densities\n",
    "‚ùå Doesn't scale as well to very high dimensions\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a side-by-side comparison showing K-Means vs DBSCAN on non-spherical data.\n",
    "Style: Educational comparison diagram.\n",
    "Left panel: Two crescent moon shapes interlocked. K-Means clustering result showing incorrect vertical split (failing to capture moon shapes). Title: 'K-Means Fails on Non-Spherical Data'\n",
    "Right panel: Same moon shapes correctly clustered by DBSCAN, each moon in different color. Outliers shown as black dots. Title: 'DBSCAN Handles Complex Shapes'\n",
    "Both panels show cluster boundaries with dotted lines.\n",
    "Color scheme: Blue and orange for clusters, black for outliers.\n",
    "Include legend explaining core points, border points, and noise.\n",
    "Format: Side-by-side, 16:9 ratio.\" -->\n",
    "\n",
    "Let's see DBSCAN in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex-shaped data that K-Means can't handle\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "X_circles, _ = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "X_moons_scaled = StandardScaler().fit_transform(X_moons)\n",
    "X_circles_scaled = StandardScaler().fit_transform(X_circles)\n",
    "\n",
    "datasets = [\n",
    "    (X_moons_scaled, X_moons, 'Two Moons'),\n",
    "    (X_circles_scaled, X_circles, 'Concentric Circles')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "for idx, (X_scaled, X_orig, title) in enumerate(datasets):\n",
    "    # Original data\n",
    "    axes[idx, 0].scatter(X_orig[:, 0], X_orig[:, 1], alpha=0.6, c='gray', s=30)\n",
    "    axes[idx, 0].set_title(f'{title}\\n(Unlabeled)', fontsize=13, fontweight='bold')\n",
    "    axes[idx, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # K-Means (will fail!)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "    axes[idx, 1].scatter(X_orig[:, 0], X_orig[:, 1], c=kmeans_labels, \n",
    "                        cmap='husl', alpha=0.6, s=30)\n",
    "    axes[idx, 1].set_title(f'K-Means (K=2)\\n‚ùå Poor Results', \n",
    "                           fontsize=13, fontweight='bold', color='red')\n",
    "    axes[idx, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # DBSCAN (will succeed!)\n",
    "    dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "    dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    # Separate core points from noise\n",
    "    core_samples_mask = np.zeros_like(dbscan_labels, dtype=bool)\n",
    "    core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "    \n",
    "    # Plot DBSCAN results\n",
    "    unique_labels = set(dbscan_labels)\n",
    "    colors = plt.cm.husl(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Noise points (outliers)\n",
    "            col = 'black'\n",
    "            marker = 'x'\n",
    "        else:\n",
    "            marker = 'o'\n",
    "        \n",
    "        class_member_mask = (dbscan_labels == k)\n",
    "        xy = X_orig[class_member_mask & core_samples_mask]\n",
    "        axes[idx, 2].scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker,\n",
    "                            s=30, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        xy = X_orig[class_member_mask & ~core_samples_mask]\n",
    "        axes[idx, 2].scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker,\n",
    "                            s=15, alpha=0.3)\n",
    "    \n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = list(dbscan_labels).count(-1)\n",
    "    \n",
    "    axes[idx, 2].set_title(f'DBSCAN\\n‚úÖ Clusters: {n_clusters}, Noise: {n_noise}', \n",
    "                          fontsize=13, fontweight='bold', color='green')\n",
    "    axes[idx, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Observations:\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ DBSCAN correctly identifies complex shapes\")\n",
    "print(\"‚úÖ Automatically detects and removes outliers (noise)\")\n",
    "print(\"‚ùå K-Means forces spherical clusters, misses the pattern\")\n",
    "print(\"\\nüí° Lesson: Choose your algorithm based on data shape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 3: Hierarchical Clustering - Building Cluster Trees\n",
    "\n",
    "### The Family Tree of Data\n",
    "\n",
    "Hierarchical clustering builds a tree (dendrogram) showing how data points group at different scales.\n",
    "\n",
    "**Two Approaches**:\n",
    "\n",
    "1. **Agglomerative (Bottom-Up)**:\n",
    "   - Start: Each point is its own cluster\n",
    "   - Repeat: Merge the two closest clusters\n",
    "   - End: All points in one cluster\n",
    "\n",
    "2. **Divisive (Top-Down)**:\n",
    "   - Start: All points in one cluster\n",
    "   - Repeat: Split clusters recursively\n",
    "   - End: Each point is its own cluster\n",
    "\n",
    "**Linkage Methods** (how to measure cluster distance):\n",
    "- **Single**: Minimum distance between any two points\n",
    "- **Complete**: Maximum distance between any two points\n",
    "- **Average**: Average distance between all pairs\n",
    "- **Ward**: Minimizes within-cluster variance (most common)\n",
    "\n",
    "**Advantages**:\n",
    "‚úÖ No need to specify K upfront\n",
    "‚úÖ Produces hierarchy showing relationships\n",
    "‚úÖ Deterministic (same result every time)\n",
    "\n",
    "**Disadvantages**:\n",
    "‚ùå Computationally expensive (O(n¬≥) or O(n¬≤) with optimization)\n",
    "‚ùå Can't undo merges (greedy)\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create an educational diagram showing hierarchical clustering dendrogram.\n",
    "Style: Professional scientific illustration.\n",
    "Top: Dendrogram (tree diagram) showing hierarchical merging of clusters. Vertical axis labeled 'Distance', horizontal axis showing data point labels.\n",
    "Different height levels with horizontal lines indicating merge points.\n",
    "Bottom: Corresponding data points at the leaf level, connected to tree above.\n",
    "Show cutoff line (dashed red) at different heights resulting in 2, 3, and 4 clusters.\n",
    "Color-code clusters with consistent colors from dendrogram to data points.\n",
    "Include annotations explaining: 'Cutting the tree at different heights gives different number of clusters'\n",
    "Color scheme: Professional blue-green gradient for tree, multicolor for clusters.\n",
    "Format: Vertical layout optimized for understanding tree structure.\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our original customer data\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Create the dendrogram\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "dendrogram(linkage_matrix, \n",
    "          truncate_mode='lastp',  # Show only the last p merged clusters\n",
    "          p=30,  # Show last 30 merges\n",
    "          leaf_rotation=90,\n",
    "          leaf_font_size=10)\n",
    "plt.title('Hierarchical Clustering Dendrogram', fontsize=15, fontweight='bold', pad=15)\n",
    "plt.xlabel('Sample Index or (Cluster Size)', fontsize=12)\n",
    "plt.ylabel('Distance (Ward Linkage)', fontsize=12)\n",
    "plt.axhline(y=6, color='red', linestyle='--', linewidth=2, label='Cut at 4 clusters')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Apply hierarchical clustering with 4 clusters\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "hierarchical_labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=hierarchical_labels, \n",
    "                     cmap='husl', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.title('Hierarchical Clustering Result (K=4)', fontsize=15, fontweight='bold', pad=15)\n",
    "plt.xlabel('Average Purchase Value ($)', fontsize=12)\n",
    "plt.ylabel('Purchase Frequency (per month)', fontsize=12)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Dendrogram Interpretation:\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚Ä¢ Height of merge = dissimilarity between clusters\")\n",
    "print(\"‚Ä¢ Cut the tree at different heights ‚Üí different K\")\n",
    "print(\"‚Ä¢ We cut at height ~6 to get 4 distinct clusters\")\n",
    "print(\"\\nüí° Pro Tip: Use the dendrogram to choose optimal K!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 4: Evaluating Clustering Quality\n",
    "\n",
    "### The Challenge: No Ground Truth\n",
    "\n",
    "Unlike supervised learning, we don't have labels to check against. So how do we know if clustering is good?\n",
    "\n",
    "### Internal Validation Metrics\n",
    "\n",
    "**1. Silhouette Score** (Range: -1 to 1, higher is better)\n",
    "- Measures how similar a point is to its own cluster vs. other clusters\n",
    "- Formula: `s = (b - a) / max(a, b)`\n",
    "  - `a` = average distance to points in same cluster\n",
    "  - `b` = average distance to points in nearest other cluster\n",
    "- **Interpretation**:\n",
    "  - Close to +1: Well clustered\n",
    "  - Close to 0: On cluster boundary\n",
    "  - Negative: Probably in wrong cluster\n",
    "\n",
    "**2. Davies-Bouldin Index** (Lower is better)\n",
    "- Ratio of within-cluster to between-cluster distances\n",
    "- Simpler than silhouette, less computationally expensive\n",
    "\n",
    "**3. Calinski-Harabasz Index** (Higher is better)\n",
    "- Ratio of between-cluster to within-cluster variance\n",
    "- Also called Variance Ratio Criterion\n",
    "\n",
    "**‚ö†Ô∏è Important**: Metrics can disagree! Use multiple metrics + domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three clustering methods\n",
    "methods = {\n",
    "    'K-Means': clusters,\n",
    "    'DBSCAN (on blobs)': DBSCAN(eps=0.5, min_samples=5).fit_predict(X_scaled),\n",
    "    'Hierarchical': hierarchical_labels\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nüéØ Clustering Method Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, labels in methods.items():\n",
    "    # Skip if all points are noise (DBSCAN edge case)\n",
    "    if len(set(labels)) <= 1:\n",
    "        continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_scaled, labels)\n",
    "    calinski = calinski_harabasz_score(X_scaled, labels)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    results.append({\n",
    "        'Method': name,\n",
    "        'Clusters': n_clusters,\n",
    "        'Silhouette': silhouette,\n",
    "        'Davies-Bouldin': davies_bouldin,\n",
    "        'Calinski-Harabasz': calinski\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüìä Metric Interpretation:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Silhouette Score: Higher is better (max = 1.0)\")\n",
    "print(\"Davies-Bouldin Index: Lower is better (min = 0.0)\")\n",
    "print(\"Calinski-Harabasz Index: Higher is better\")\n",
    "\n",
    "# Visualize silhouette analysis for K-Means\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Silhouette plot\n",
    "silhouette_vals = silhouette_samples(X_scaled, clusters)\n",
    "y_lower = 10\n",
    "\n",
    "for i in range(4):\n",
    "    cluster_silhouette_vals = silhouette_vals[clusters == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    size_cluster = cluster_silhouette_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster\n",
    "    \n",
    "    color = cm.husl(float(i) / 4)\n",
    "    axes[0].fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                         0, cluster_silhouette_vals,\n",
    "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    \n",
    "    axes[0].text(-0.05, y_lower + 0.5 * size_cluster, str(i), \n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "axes[0].set_xlabel('Silhouette Coefficient', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Cluster', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Silhouette Plot for K-Means', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[0].axvline(x=silhouette_score(X_scaled, clusters), color='red', \n",
    "               linestyle='--', linewidth=2, label=f'Average: {silhouette_score(X_scaled, clusters):.3f}')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Metric comparison bar chart\n",
    "x_pos = np.arange(len(results_df))\n",
    "axes[1].bar(x_pos - 0.2, results_df['Silhouette'], 0.4, \n",
    "           label='Silhouette (‚Üë)', alpha=0.8, color='skyblue')\n",
    "axes[1].bar(x_pos + 0.2, 1 / (1 + results_df['Davies-Bouldin']), 0.4, \n",
    "           label='1/(1+DB) (‚Üë)', alpha=0.8, color='lightcoral')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(results_df['Method'], rotation=15, ha='right')\n",
    "axes[1].set_ylabel('Normalized Score', fontsize=12)\n",
    "axes[1].set_title('Method Comparison (Higher is Better)', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Best Practice: Combine metrics with visual inspection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: Real-World Anomaly Detection\n",
    "\n",
    "**Objective**: Use clustering to detect anomalies in credit card transactions\n",
    "\n",
    "**Scenario**: You have transaction data with features:\n",
    "- Transaction amount\n",
    "- Time of day\n",
    "- Merchant category\n",
    "\n",
    "**Task**:\n",
    "1. Generate synthetic transaction data with some outliers\n",
    "2. Apply DBSCAN to identify normal vs. anomalous transactions\n",
    "3. Visualize the results and calculate the anomaly rate\n",
    "4. Bonus: Try different `eps` values and see how it affects detection\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 1: Generating data</summary>\n",
    "\n",
    "```python\n",
    "# Normal transactions\n",
    "normal_transactions = np.random.normal([50, 12], [20, 3], (200, 2))\n",
    "\n",
    "# Anomalies (unusual amounts/times)\n",
    "anomalies = np.random.uniform([200, 1], [500, 23], (20, 2))\n",
    "\n",
    "# Combine\n",
    "X_transactions = np.vstack([normal_transactions, anomalies])\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint 2: DBSCAN parameters</summary>\n",
    "\n",
    "Start with `eps=0.5` and `min_samples=5`. Points labeled as -1 are anomalies!\n",
    "</details>\n",
    "\n",
    "**Expected Output**: \n",
    "- A scatter plot showing normal transactions in clusters and anomalies as black crosses\n",
    "- Anomaly detection rate (should be close to 10% if you used the hint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Detect anomalies in transaction data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Customer Segmentation Strategy\n",
    "\n",
    "**Objective**: Build a complete customer segmentation pipeline\n",
    "\n",
    "**Dataset**: Generate synthetic e-commerce data with:\n",
    "- Recency: Days since last purchase\n",
    "- Frequency: Number of purchases\n",
    "- Monetary: Total amount spent\n",
    "\n",
    "This is called **RFM Analysis** in marketing!\n",
    "\n",
    "**Tasks**:\n",
    "1. Generate RFM data for 500 customers\n",
    "2. Scale the features (important!)\n",
    "3. Use the elbow method to find optimal K\n",
    "4. Apply K-Means clustering\n",
    "5. Profile each cluster and name them (e.g., \"VIP Customers\", \"At Risk\", etc.)\n",
    "6. Create actionable marketing recommendations for each segment\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint: Generating RFM data</summary>\n",
    "\n",
    "```python\n",
    "recency = np.random.exponential(30, 500)  # Days since last purchase\n",
    "frequency = np.random.poisson(5, 500)  # Number of purchases\n",
    "monetary = np.random.gamma(100, 5, 500)  # Total spent\n",
    "\n",
    "rfm_data = np.column_stack([recency, frequency, monetary])\n",
    "```\n",
    "</details>\n",
    "\n",
    "**Challenge**: \n",
    "- Profile clusters: Calculate mean RFM values for each cluster\n",
    "- Business naming: Give clusters business-friendly names\n",
    "- Marketing actions: Suggest specific campaigns for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Build complete customer segmentation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "You've mastered unsupervised learning and clustering!\n",
    "\n",
    "- ‚úÖ **Unsupervised Learning Fundamentals**:\n",
    "  - No labels required‚Äîdiscover patterns autonomously\n",
    "  - Most real-world data is unlabeled\n",
    "  - Evaluation is trickier but doable with internal metrics\n",
    "\n",
    "- ‚úÖ **K-Means Clustering**:\n",
    "  - Fast, simple, and widely used\n",
    "  - Assumes spherical clusters of similar size\n",
    "  - Use elbow method + silhouette score to choose K\n",
    "  - Perfect for customer segmentation and data exploration\n",
    "\n",
    "- ‚úÖ **DBSCAN**:\n",
    "  - Handles arbitrary shapes and automatically detects outliers\n",
    "  - No need to specify K upfront\n",
    "  - Requires careful parameter tuning (eps, min_samples)\n",
    "  - Ideal for anomaly detection and complex patterns\n",
    "\n",
    "- ‚úÖ **Hierarchical Clustering**:\n",
    "  - Produces interpretable dendrograms\n",
    "  - Deterministic results (no randomness)\n",
    "  - Computationally expensive for large datasets\n",
    "  - Great for understanding data structure at multiple scales\n",
    "\n",
    "- ‚úÖ **Evaluation Metrics**:\n",
    "  - Silhouette Score: Measures cluster cohesion and separation\n",
    "  - Davies-Bouldin Index: Cluster compactness ratio\n",
    "  - Calinski-Harabasz: Variance ratio\n",
    "  - Always use multiple metrics + visual inspection\n",
    "\n",
    "### ü§î The Big Picture:\n",
    "\n",
    "**Algorithm Selection Guide**:\n",
    "\n",
    "| Use Case | Best Algorithm | Why? |\n",
    "|----------|---------------|------|\n",
    "| **Customer segmentation** | K-Means | Fast, interpretable, works with business metrics |\n",
    "| **Anomaly detection** | DBSCAN | Automatically identifies outliers |\n",
    "| **Complex shapes** | DBSCAN | Doesn't assume spherical clusters |\n",
    "| **Understanding hierarchy** | Hierarchical | Provides multi-scale view |\n",
    "| **Large datasets** | K-Means | Scales better than hierarchical |\n",
    "\n",
    "**When to use clustering**:\n",
    "1. ‚úÖ Exploratory data analysis\n",
    "2. ‚úÖ Customer/user segmentation\n",
    "3. ‚úÖ Anomaly and outlier detection\n",
    "4. ‚úÖ Feature engineering for supervised learning\n",
    "5. ‚úÖ Image segmentation in computer vision\n",
    "6. ‚úÖ Document clustering in NLP\n",
    "\n",
    "**Remember**: Clustering is an art as much as a science. Domain expertise is crucial for interpreting results! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Learning\n",
    "\n",
    "**Recommended Reading**:\n",
    "- [Scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html) - Comprehensive documentation\n",
    "- [StatQuest: K-Means](https://www.youtube.com/watch?v=4b5d3muPQmA) - Excellent visual explanation\n",
    "- [DBSCAN Visualized](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/) - Interactive demo\n",
    "\n",
    "**Deep Dives**:\n",
    "- [Clustering Validation](https://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation) - Understanding evaluation metrics\n",
    "- [Advanced Clustering](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods) - HDBSCAN, Spectral Clustering, and more\n",
    "- [Curse of Dimensionality](https://www.youtube.com/watch?v=QZ0DtNFdDko) - Why clustering gets harder in high dimensions\n",
    "\n",
    "**Practical Applications**:\n",
    "- [Customer Segmentation with Python](https://www.kaggle.com/code/fabiendaniel/customer-segmentation) - Real retail data\n",
    "- [Anomaly Detection Tutorial](https://scikit-learn.org/stable/auto_examples/applications/plot_outlier_detection_wine.html) - Wine quality dataset\n",
    "- [Image Compression with K-Means](https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html) - Color quantization\n",
    "\n",
    "**Research Papers** (for the curious):\n",
    "- [Original K-Means](https://web.stanford.edu/~hastie/Papers/gap.pdf) - Gap statistic for estimating K\n",
    "- [DBSCAN Paper](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf) - The original algorithm\n",
    "- [Silhouette Method](https://www.sciencedirect.com/science/article/pii/0377042787901257) - Clustering validation\n",
    "\n",
    "**Tools & Libraries**:\n",
    "- [HDBSCAN](https://hdbscan.readthedocs.io/) - Improved DBSCAN for varying densities\n",
    "- [Yellowbrick](https://www.scikit-yb.org/en/latest/api/cluster/) - Visualization for clustering\n",
    "- [PyClustering](https://pyclustering.github.io/) - Additional clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚û°Ô∏è What's Next?\n",
    "\n",
    "Congratulations! You've completed Module 2: Machine Learning Fundamentals!\n",
    "\n",
    "**In Chapter 2.4 - Model Evaluation & Metrics**, you'll master:\n",
    "\n",
    "**Coming up**:\n",
    "- **Deep dive into metrics**: Precision, recall, F1, ROC-AUC‚Äîwhen to use what\n",
    "- **Cross-validation**: Properly assessing model performance\n",
    "- **Overfitting & underfitting**: The bias-variance tradeoff\n",
    "- **Confusion matrices**: Understanding your model's mistakes\n",
    "- **Real-world case studies**: Choosing the right metric for your business problem\n",
    "\n",
    "From patterns to performance‚Äîmeasuring what matters! üìä\n",
    "\n",
    "Ready to become a model evaluation expert? Open **[Chapter 2.4 - Model Evaluation](2.4-model-evaluation.ipynb)**!\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Feedback & Community\n",
    "\n",
    "**Questions?** Join our [Discord community](https://discord.gg/madeforai)\n",
    "\n",
    "**Found a bug?** [Open an issue on GitHub](https://github.com/madeforai/madeforai/issues)\n",
    "\n",
    "**Share your clustering projects!** Tweet with #MadeForAI\n",
    "\n",
    "**Keep exploring!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
