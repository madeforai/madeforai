{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 - Classification vs Regression: Choosing Your Weapon\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/understanding-ai/module-2/2.2-classification-vs-regression.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Master the art of choosing between regression and classification‚Äîthe first and most important decision in any ML project.**\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- **Decision framework**: How to choose between regression and classification\n",
    "- **When algorithms overlap**: Models that can do both (and why)\n",
    "- **Converting between types**: Turning regression into classification and vice versa\n",
    "- **Advanced techniques**: Polynomial regression, multi-class classification, class imbalance\n",
    "- **Real-world scenarios**: Case studies that will test your judgment\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "35-40 minutes\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Completed Chapter 2.1 (Supervised Learning Essentials)\n",
    "- Understanding of regression and classification basics\n",
    "- Familiarity with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Million-Dollar Question\n",
    "\n",
    "Imagine you're at a new job. Your boss walks in with a dataset and asks:\n",
    "\n",
    "> \"Build a model to predict customer behavior.\"\n",
    "\n",
    "**Your first question should be:** *\"Is this a regression or classification problem?\"*\n",
    "\n",
    "Get this wrong, and everything else fails. Get it right, and you're 50% of the way there.\n",
    "\n",
    "**Why This Matters:**\n",
    "- ‚ö†Ô∏è Wrong choice = wrong metrics, wrong algorithms, wrong results\n",
    "- ‚úÖ Right choice = clear path to solution, appropriate evaluation, business value\n",
    "\n",
    "**The Core Distinction:**\n",
    "\n",
    "| Aspect | Regression | Classification |\n",
    "|--------|-----------|----------------|\n",
    "| **Output Type** | Continuous number | Discrete category |\n",
    "| **Example Output** | $250,000, 23.5¬∞C, 1.8m | Spam, Cat, Approved |\n",
    "| **Question Answered** | \"How much?\" | \"Which one?\" |\n",
    "| **Infinite Possibilities?** | Yes (any value in range) | No (fixed set of classes) |\n",
    "\n",
    "**But here's where it gets tricky...**\n",
    "\n",
    "Some problems can be framed EITHER way! We'll explore these edge cases today. ü§î\n",
    "\n",
    "Let's build the ultimate decision framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import libraries\n",
    "# Uncomment if running in Google Colab\n",
    "# !pip install numpy pandas matplotlib seaborn scikit-learn plotly -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, KBinsDiscretizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(\"üìò Module 2.2: Classification vs Regression\")\n",
    "print(\"üéØ Let's master the art of problem formulation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 1: The Ultimate Decision Framework\n",
    "\n",
    "### üîç Step 1: Examine Your Target Variable\n",
    "\n",
    "**The target variable (y) determines EVERYTHING.**\n",
    "\n",
    "Ask yourself these questions:\n",
    "\n",
    "#### Question 1: Is the output a number or a category?\n",
    "\n",
    "**If number:**\n",
    "- Can it take ANY value in a range? ‚Üí **Regression**\n",
    "  - Examples: $0-$1M, 0-100¬∞C, 0.1-10.0 meters\n",
    "\n",
    "**If category:**\n",
    "- Fixed set of distinct classes? ‚Üí **Classification**\n",
    "  - Examples: {Yes, No}, {Cat, Dog, Bird}, {Low, Medium, High}\n",
    "\n",
    "#### Question 2: Could you measure it with increasing precision?\n",
    "\n",
    "**Regression indicators:**\n",
    "- \"Approximately $250K\" ‚Üí could be $249,567.89\n",
    "- Can zoom in infinitely (like zooming into a number line)\n",
    "\n",
    "**Classification indicators:**\n",
    "- \"It's either a cat OR a dog\" ‚Üí no middle ground\n",
    "- Discrete buckets, can't be \"between\" classes\n",
    "\n",
    "#### Question 3: What does your business care about?\n",
    "\n",
    "**Regression when:**\n",
    "- Business needs specific quantity (\"How much revenue will we make?\")\n",
    "- Optimization requires precise values (\"What's the optimal price?\")\n",
    "\n",
    "**Classification when:**\n",
    "- Business needs decisions/actions (\"Should we approve this loan?\")\n",
    "- Outcomes are inherently categorical (\"Will customer churn?\")\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a decision tree flowchart for choosing regression vs classification.\n",
    "Style: Clean, modern, professional flowchart.\n",
    "Start node: 'What is your target variable?'\n",
    "Branch 1: 'Continuous number?' ‚Üí leads to 'Can take any value in range?' ‚Üí Yes ‚Üí 'REGRESSION'\n",
    "Branch 2: 'Discrete category?' ‚Üí leads to 'Fixed set of classes?' ‚Üí Yes ‚Üí 'CLASSIFICATION'\n",
    "Middle branch: 'Not sure?' ‚Üí leads to 'Ask: Can you measure with increasing precision?'\n",
    "- If yes ‚Üí REGRESSION\n",
    "- If no ‚Üí CLASSIFICATION\n",
    "Color scheme: Blue for regression path, Orange for classification path, Gray for decision nodes.\n",
    "Include icons: calculator for regression, labels for classification.\n",
    "Format: Vertical flowchart, 16:9 aspect ratio.\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Step 2: Consider the Gray Areas\n",
    "\n",
    "Some problems can be formulated EITHER way. Here's how to decide:\n",
    "\n",
    "#### Scenario A: Predicting Age\n",
    "\n",
    "**As Regression:**\n",
    "- Predict exact age: 23.5 years, 47.2 years\n",
    "- Use case: Insurance pricing, personalized healthcare\n",
    "- **Choose if:** You need precise age for calculations\n",
    "\n",
    "**As Classification:**\n",
    "- Predict age group: Child, Teen, Adult, Senior\n",
    "- Use case: Content recommendations, marketing segments\n",
    "- **Choose if:** You're making category-based decisions\n",
    "\n",
    "#### Scenario B: Customer Lifetime Value (CLV)\n",
    "\n",
    "**As Regression:**\n",
    "- Predict exact revenue: $1,245.67 over 3 years\n",
    "- Use case: Revenue forecasting, budget allocation\n",
    "- **Choose if:** You need precise financial projections\n",
    "\n",
    "**As Classification:**\n",
    "- Predict value tier: Low ($0-$100), Medium ($100-$1000), High ($1000+)\n",
    "- Use case: Customer segmentation, tiered service\n",
    "- **Choose if:** Actions are tier-based anyway\n",
    "\n",
    "**The Decision Rule:**\n",
    "> If your downstream action is the same for a range of values, **use classification**. If every dollar/unit matters, **use regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Same data, two formulations\n",
    "\n",
    "# Generate student test score data\n",
    "np.random.seed(42)\n",
    "n_students = 500\n",
    "\n",
    "# Features: study hours, previous GPA, attendance rate\n",
    "study_hours = np.random.uniform(0, 50, n_students)\n",
    "prev_gpa = np.random.uniform(2.0, 4.0, n_students)\n",
    "attendance = np.random.uniform(0.5, 1.0, n_students)\n",
    "\n",
    "# Generate test scores (0-100)\n",
    "test_scores = (\n",
    "    30 + \n",
    "    1.2 * study_hours + \n",
    "    12 * prev_gpa + \n",
    "    25 * attendance +\n",
    "    np.random.normal(0, 8, n_students)\n",
    ")\n",
    "test_scores = np.clip(test_scores, 0, 100)  # Ensure 0-100 range\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'study_hours': study_hours,\n",
    "    'previous_gpa': prev_gpa,\n",
    "    'attendance_rate': attendance,\n",
    "    'test_score': test_scores\n",
    "})\n",
    "\n",
    "# FORMULATION 1: Regression (exact score)\n",
    "df['score_continuous'] = df['test_score']\n",
    "\n",
    "# FORMULATION 2: Classification (grade categories)\n",
    "def score_to_grade(score):\n",
    "    if score >= 90: return 'A'\n",
    "    elif score >= 80: return 'B'\n",
    "    elif score >= 70: return 'C'\n",
    "    elif score >= 60: return 'D'\n",
    "    else: return 'F'\n",
    "\n",
    "df['grade_category'] = df['test_score'].apply(score_to_grade)\n",
    "\n",
    "print(\"üìä Same Data, Two Formulations:\\n\")\n",
    "print(\"Sample of 5 students:\")\n",
    "print(\"=\"*80)\n",
    "display(df[['study_hours', 'previous_gpa', 'attendance_rate', \n",
    "           'score_continuous', 'grade_category']].head())\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìà Formulation 1 - REGRESSION:\")\n",
    "print(f\"   Target: test_score (continuous)\")\n",
    "print(f\"   Range: {df['score_continuous'].min():.1f} to {df['score_continuous'].max():.1f}\")\n",
    "print(f\"   Goal: Predict EXACT score\")\n",
    "print(f\"   Use case: Precise performance forecasting\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Formulation 2 - CLASSIFICATION:\")\n",
    "print(f\"   Target: grade (categorical)\")\n",
    "print(f\"   Classes: {df['grade_category'].unique()}\")\n",
    "print(f\"   Goal: Predict letter GRADE\")\n",
    "print(f\"   Use case: Pass/fail decisions, student grouping\")\n",
    "\n",
    "# Visualize both formulations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Regression view\n",
    "ax1.scatter(df['study_hours'], df['score_continuous'], \n",
    "           alpha=0.5, s=60, color='#3b82f6', edgecolors='white', linewidth=1)\n",
    "ax1.set_xlabel('Study Hours', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Test Score (Continuous)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Regression Formulation\\n(Predict Exact Score)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Classification view\n",
    "grade_colors = {'A': '#10b981', 'B': '#3b82f6', 'C': '#f59e0b', 'D': '#ef4444', 'F': '#8b5cf6'}\n",
    "for grade in ['A', 'B', 'C', 'D', 'F']:\n",
    "    mask = df['grade_category'] == grade\n",
    "    ax2.scatter(df[mask]['study_hours'], df[mask]['score_continuous'],\n",
    "               alpha=0.6, s=60, color=grade_colors[grade], \n",
    "               label=f'Grade {grade}', edgecolors='white', linewidth=1)\n",
    "\n",
    "ax2.set_xlabel('Study Hours', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Test Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Classification Formulation\\n(Predict Grade Category)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add grade boundaries\n",
    "for grade_cutoff, grade_label in [(90, 'A'), (80, 'B'), (70, 'C'), (60, 'D')]:\n",
    "    ax2.axhline(y=grade_cutoff, color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   ‚Üí SAME underlying data, DIFFERENT problem formulations\")\n",
    "print(\"   ‚Üí Regression: Sensitive to every point difference\")\n",
    "print(\"   ‚Üí Classification: Only cares about crossing grade boundaries\")\n",
    "print(\"   ‚Üí Choose based on what your application actually needs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Part 2: Converting Between Problem Types\n",
    "\n",
    "### üìä Regression ‚Üí Classification (Discretization)\n",
    "\n",
    "**Why convert?**\n",
    "- Simplify decision-making (\"High risk\" vs \"4.7% default probability\")\n",
    "- Match business processes (tier-based pricing, risk categories)\n",
    "- Improve interpretability for non-technical stakeholders\n",
    "\n",
    "**Two Approaches:**\n",
    "\n",
    "#### 1. Domain-Based Binning\n",
    "Use business knowledge to set thresholds\n",
    "```python\n",
    "# Example: Income categories\n",
    "if income < 30000: class = 'Low'\n",
    "elif income < 80000: class = 'Middle'\n",
    "else: class = 'High'\n",
    "```\n",
    "\n",
    "#### 2. Data-Driven Binning\n",
    "Use quantiles or equal-width bins\n",
    "```python\n",
    "# Example: Tertiles (33rd, 66th percentiles)\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è Warning:** You lose information! A regression model with 75 and 76 sees them as similar. After binning to \"Pass/Fail\" at 75, they're in different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate discretization methods\n",
    "\n",
    "# Use test score data\n",
    "scores = df['test_score'].values.reshape(-1, 1)\n",
    "\n",
    "# Method 1: Domain-based binning (using grade boundaries)\n",
    "domain_bins = np.array([0, 60, 70, 80, 90, 100])\n",
    "domain_labels = ['F', 'D', 'C', 'B', 'A']\n",
    "df['domain_based'] = pd.cut(df['test_score'], bins=domain_bins, labels=domain_labels)\n",
    "\n",
    "# Method 2: Equal-width binning\n",
    "equal_width = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "df['equal_width'] = equal_width.fit_transform(scores).astype(int)\n",
    "\n",
    "# Method 3: Quantile-based binning (equal number of samples per bin)\n",
    "quantile_based = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "df['quantile_based'] = quantile_based.fit_transform(scores).astype(int)\n",
    "\n",
    "# Visualize different binning strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Domain-based\n",
    "for idx, grade in enumerate(['A', 'B', 'C', 'D', 'F']):\n",
    "    mask = df['domain_based'] == grade\n",
    "    axes[0].scatter(df[mask]['study_hours'], df[mask]['test_score'],\n",
    "                   alpha=0.6, s=50, label=grade, edgecolors='white', linewidth=1)\n",
    "for boundary in [60, 70, 80, 90]:\n",
    "    axes[0].axhline(y=boundary, color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "axes[0].set_title('Domain-Based Binning\\n(Grade Boundaries)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Study Hours', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Test Score', fontsize=11, fontweight='bold')\n",
    "axes[0].legend(title='Grade', fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Equal-width\n",
    "for bin_num in range(5):\n",
    "    mask = df['equal_width'] == bin_num\n",
    "    axes[1].scatter(df[mask]['study_hours'], df[mask]['test_score'],\n",
    "                   alpha=0.6, s=50, label=f'Bin {bin_num}', edgecolors='white', linewidth=1)\n",
    "# Show bin edges\n",
    "bin_edges = equal_width.bin_edges_[0]\n",
    "for edge in bin_edges[1:-1]:\n",
    "    axes[1].axhline(y=edge, color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "axes[1].set_title('Equal-Width Binning\\n(Same Range per Bin)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Study Hours', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Test Score', fontsize=11, fontweight='bold')\n",
    "axes[1].legend(title='Bin', fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Quantile-based\n",
    "for bin_num in range(5):\n",
    "    mask = df['quantile_based'] == bin_num\n",
    "    axes[2].scatter(df[mask]['study_hours'], df[mask]['test_score'],\n",
    "                   alpha=0.6, s=50, label=f'Quantile {bin_num}', edgecolors='white', linewidth=1)\n",
    "# Show bin edges\n",
    "bin_edges_q = quantile_based.bin_edges_[0]\n",
    "for edge in bin_edges_q[1:-1]:\n",
    "    axes[2].axhline(y=edge, color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "axes[2].set_title('Quantile-Based Binning\\n(Equal Samples per Bin)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Study Hours', fontsize=11, fontweight='bold')\n",
    "axes[2].set_ylabel('Test Score', fontsize=11, fontweight='bold')\n",
    "axes[2].legend(title='Quantile', fontsize=9)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare bin distributions\n",
    "print(\"\\nüìä Bin Distributions:\\n\")\n",
    "print(\"Domain-Based (Grades):\")\n",
    "print(df['domain_based'].value_counts().sort_index())\n",
    "print(f\"\\nEqual-Width Bins:\")\n",
    "print(df['equal_width'].value_counts().sort_index())\n",
    "print(f\"\\nQuantile-Based Bins:\")\n",
    "print(df['quantile_based'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   ‚Üí Domain-based: Reflects grading standards, unequal class sizes\")\n",
    "print(\"   ‚Üí Equal-width: Regular intervals, but can have very different class sizes\")\n",
    "print(\"   ‚Üí Quantile-based: Balanced classes, but irregular intervals\")\n",
    "print(\"\\nüéØ Choose based on your use case!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè∑Ô∏è Classification ‚Üí Regression (Ordinal Encoding + Regression)\n",
    "\n",
    "**Why convert?**\n",
    "- Get more granular predictions\n",
    "- Leverage ordering in categories (Low < Medium < High)\n",
    "- When boundaries between classes are fuzzy\n",
    "\n",
    "**Example:**\n",
    "- Original: Predict {Low, Medium, High} customer satisfaction\n",
    "- Convert to: Low=1, Medium=2, High=3\n",
    "- Train regression model\n",
    "- Output: 2.7 (between Medium and High)\n",
    "\n",
    "**‚ö†Ô∏è Warning:** Only works for **ordinal** categories (with natural ordering)!\n",
    "- ‚úÖ Good: {Small, Medium, Large}, {Cold, Warm, Hot}\n",
    "- ‚ùå Bad: {Cat, Dog, Bird}, {Red, Blue, Green}\n",
    "\n",
    "**When NOT to do this:**\n",
    "- Nominal categories (no natural order)\n",
    "- When intermediate values are meaningless\n",
    "- When you need probability estimates for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Part 3: Algorithms That Do Both\n",
    "\n",
    "Some algorithms can handle BOTH regression and classification with minor modifications!\n",
    "\n",
    "### Decision Trees\n",
    "**Regression**: Predict average value in each leaf\n",
    "**Classification**: Predict majority class in each leaf\n",
    "\n",
    "### Random Forests\n",
    "**Regression**: Average predictions from all trees\n",
    "**Classification**: Vote among all trees\n",
    "\n",
    "### Neural Networks\n",
    "**Regression**: Linear output layer, MSE loss\n",
    "**Classification**: Softmax output layer, cross-entropy loss\n",
    "\n",
    "### Support Vector Machines (SVM)\n",
    "**Regression**: SVR (epsilon-insensitive loss)\n",
    "**Classification**: SVC (maximum margin classifier)\n",
    "\n",
    "**The difference is usually just:**\n",
    "1. Output layer/activation function\n",
    "2. Loss function\n",
    "3. Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: Decision Tree for both tasks\n",
    "\n",
    "# Prepare data\n",
    "X = df[['study_hours', 'previous_gpa', 'attendance_rate']]\n",
    "y_reg = df['score_continuous']  # Regression target\n",
    "y_clf = df['domain_based']  # Classification target\n",
    "\n",
    "# Train-test split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "dt_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg = dt_reg.predict(X_test_reg)\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_clf.fit(X_train_clf, y_train_clf)\n",
    "y_pred_clf = dt_clf.predict(X_test_clf)\n",
    "\n",
    "# Evaluate\n",
    "reg_r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "reg_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
    "\n",
    "clf_acc = accuracy_score(y_test_clf, y_pred_clf)\n",
    "clf_f1 = f1_score(y_test_clf, y_pred_clf, average='weighted')\n",
    "\n",
    "print(\"üå≤ Decision Tree: Same Algorithm, Two Tasks\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"REGRESSION TASK (Predict exact score):\")\n",
    "print(f\"   R¬≤ Score: {reg_r2:.4f}\")\n",
    "print(f\"   RMSE: {reg_rmse:.2f} points\")\n",
    "print(f\"   Interpretation: Predictions are off by ~{reg_rmse:.1f} points on average\")\n",
    "print(\"\\nCLASSIFICATION TASK (Predict grade):\")\n",
    "print(f\"   Accuracy: {clf_acc:.4f}\")\n",
    "print(f\"   F1-Score: {clf_f1:.4f}\")\n",
    "print(f\"   Interpretation: {clf_acc:.1%} of grade predictions are correct\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Regression predictions\n",
    "ax1.scatter(y_test_reg, y_pred_reg, alpha=0.6, s=60, color='#3b82f6', edgecolors='white', linewidth=1.5)\n",
    "ax1.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], \n",
    "        'r--', linewidth=3, label='Perfect Predictions')\n",
    "ax1.set_xlabel('Actual Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Predicted Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Regression: Decision Tree\\n(R¬≤ = {reg_r2:.3f})', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Classification confusion matrix\n",
    "cm = confusion_matrix(y_test_clf, y_pred_clf, labels=['A', 'B', 'C', 'D', 'F'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True,\n",
    "           xticklabels=['A', 'B', 'C', 'D', 'F'],\n",
    "           yticklabels=['A', 'B', 'C', 'D', 'F'],\n",
    "           cbar_kws={'label': 'Count'}, ax=ax2,\n",
    "           annot_kws={'size': 11, 'weight': 'bold'})\n",
    "ax2.set_xlabel('Predicted Grade', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Actual Grade', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Classification: Decision Tree\\n(Accuracy = {clf_acc:.3f})', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   ‚Üí Same algorithm structure (decision tree)\")\n",
    "print(\"   ‚Üí Different output mechanisms:\")\n",
    "print(\"      ‚Ä¢ Regression: Average of samples in leaf\")\n",
    "print(\"      ‚Ä¢ Classification: Majority vote in leaf\")\n",
    "print(\"   ‚Üí Both work well when properly tuned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 4: Advanced Topics\n",
    "\n",
    "### üé® Multi-Class Classification\n",
    "\n",
    "So far we've mostly discussed **binary classification** (2 classes). But what about:\n",
    "- Digit recognition: 10 classes (0-9)\n",
    "- Product categorization: 100+ classes\n",
    "- Medical diagnosis: Multiple disease types\n",
    "\n",
    "**Two Approaches:**\n",
    "\n",
    "#### 1. One-vs-Rest (OvR)\n",
    "- Train N binary classifiers (one per class)\n",
    "- Classifier 1: \"Class A\" vs \"Not Class A\"\n",
    "- Classifier 2: \"Class B\" vs \"Not Class B\"\n",
    "- ...\n",
    "- Prediction: Pick class with highest confidence\n",
    "\n",
    "#### 2. One-vs-One (OvO)\n",
    "- Train N√ó(N-1)/2 binary classifiers (one per pair)\n",
    "- Classifier 1: \"A vs B\"\n",
    "- Classifier 2: \"A vs C\"\n",
    "- Classifier 3: \"B vs C\"\n",
    "- ...\n",
    "- Prediction: Vote among all classifiers\n",
    "\n",
    "**Most algorithms handle this automatically!**\n",
    "- Logistic Regression ‚Üí Softmax (multinomial)\n",
    "- Decision Trees ‚Üí Natural multi-class support\n",
    "- Neural Networks ‚Üí Softmax output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class classification example\n",
    "\n",
    "# Create multi-class dataset (5 grades)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode grades as integers\n",
    "le = LabelEncoder()\n",
    "y_multiclass = le.fit_transform(df['domain_based'])\n",
    "\n",
    "# Train-test split\n",
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(\n",
    "    X, y_multiclass, test_size=0.2, random_state=42, stratify=y_multiclass\n",
    ")\n",
    "\n",
    "# Train multi-class classifier\n",
    "rf_mc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_mc.fit(X_train_mc, y_train_mc)\n",
    "\n",
    "# Predictions and probabilities\n",
    "y_pred_mc = rf_mc.predict(X_test_mc)\n",
    "y_proba_mc = rf_mc.predict_proba(X_test_mc)\n",
    "\n",
    "# Evaluate\n",
    "acc_mc = accuracy_score(y_test_mc, y_pred_mc)\n",
    "f1_mc = f1_score(y_test_mc, y_pred_mc, average='weighted')\n",
    "\n",
    "print(\"üé® Multi-Class Classification (5 Grades)\\n\")\n",
    "print(f\"Accuracy: {acc_mc:.4f}\")\n",
    "print(f\"Weighted F1-Score: {f1_mc:.4f}\")\n",
    "\n",
    "# Show detailed classification report\n",
    "print(\"\\nüìä Detailed Performance by Grade:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_mc, y_pred_mc, target_names=le.classes_))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm_mc = confusion_matrix(y_test_mc, y_pred_mc)\n",
    "sns.heatmap(cm_mc, annot=True, fmt='d', cmap='Blues', square=True,\n",
    "           xticklabels=le.classes_,\n",
    "           yticklabels=le.classes_,\n",
    "           cbar_kws={'label': 'Count'}, ax=ax,\n",
    "           annot_kws={'size': 13, 'weight': 'bold'})\n",
    "ax.set_xlabel('Predicted Grade', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Actual Grade', fontsize=13, fontweight='bold')\n",
    "ax.set_title(f'Multi-Class Confusion Matrix\\n(Accuracy = {acc_mc:.3f})', \n",
    "            fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show example predictions with probabilities\n",
    "print(\"\\nüéØ Example Predictions (with confidence):\")\n",
    "print(\"=\"*80)\n",
    "sample_indices = np.random.choice(len(X_test_mc), 5, replace=False)\n",
    "for idx in sample_indices:\n",
    "    actual = le.classes_[y_test_mc.iloc[idx]]\n",
    "    predicted = le.classes_[y_pred_mc[idx]]\n",
    "    probs = y_proba_mc[idx]\n",
    "    confidence = probs[y_pred_mc[idx]]\n",
    "    \n",
    "    print(f\"\\nStudent {idx}:\")\n",
    "    print(f\"   Actual: {actual}, Predicted: {predicted} (Confidence: {confidence:.1%})\")\n",
    "    print(f\"   All probabilities: \", end=\"\")\n",
    "    for grade, prob in zip(le.classes_, probs):\n",
    "        print(f\"{grade}={prob:.2f} \", end=\"\")\n",
    "    print()\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   ‚Üí Model outputs probability for EACH class\")\n",
    "print(\"   ‚Üí Picks class with highest probability\")\n",
    "print(\"   ‚Üí Confidence scores help assess prediction reliability\")\n",
    "print(\"   ‚Üí Some mistakes are 'close' (predicting B instead of A)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Handling Imbalanced Data\n",
    "\n",
    "**The Problem:**\n",
    "When one class dominates the dataset:\n",
    "- Fraud detection: 99.9% legitimate, 0.1% fraud\n",
    "- Disease screening: 95% healthy, 5% diseased\n",
    "- Spam: 80% normal emails, 20% spam\n",
    "\n",
    "**Why it's bad:**\n",
    "- Model can get 99% accuracy by always predicting majority class!\n",
    "- Minority class (often the important one) gets ignored\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "#### 1. Collect More Minority Data\n",
    "Best solution if possible, but often impractical\n",
    "\n",
    "#### 2. Class Weights\n",
    "Penalize minority class errors more heavily\n",
    "```python\n",
    "LogisticRegression(class_weight='balanced')\n",
    "```\n",
    "\n",
    "#### 3. Resampling\n",
    "**Oversampling:** Duplicate minority samples\n",
    "**Undersampling:** Remove majority samples\n",
    "**SMOTE:** Generate synthetic minority samples\n",
    "\n",
    "#### 4. Different Metrics\n",
    "Don't use accuracy! Use:\n",
    "- Precision, Recall, F1-Score\n",
    "- ROC-AUC\n",
    "- Precision-Recall curves\n",
    "\n",
    "#### 5. Anomaly Detection\n",
    "For extreme imbalance (99.9%+), treat as anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Part 5: Real-World Decision Scenarios\n",
    "\n",
    "Test your understanding! For each scenario, decide: **Regression or Classification?**\n",
    "\n",
    "### Scenario 1: E-Commerce Product Pricing\n",
    "**Context:** You have historical data on products sold (features: category, brand, reviews, etc.)\n",
    "**Task:** Build a model to set prices for new products\n",
    "\n",
    "<details>\n",
    "<summary>ü§î Think first, then click</summary>\n",
    "\n",
    "**Answer: REGRESSION**\n",
    "- Output: Price (continuous $)\n",
    "- Business needs: Exact dollar amount\n",
    "- Could be classification if you only cared about price tiers (Budget/Mid/Premium)\n",
    "</details>\n",
    "\n",
    "### Scenario 2: Employee Turnover Prediction\n",
    "**Context:** HR data on employees (tenure, satisfaction scores, salary, etc.)\n",
    "**Task:** Predict which employees will leave in the next 6 months\n",
    "\n",
    "<details>\n",
    "<summary>ü§î Think first, then click</summary>\n",
    "\n",
    "**Answer: CLASSIFICATION**\n",
    "- Output: Will leave (Yes/No) - binary\n",
    "- Business needs: Actionable categories\n",
    "- Could be regression if predicting \"days until departure\"\n",
    "</details>\n",
    "\n",
    "### Scenario 3: Restaurant Wait Time\n",
    "**Context:** Restaurant data (time of day, party size, day of week, etc.)\n",
    "**Task:** Inform customers how long they'll wait\n",
    "\n",
    "<details>\n",
    "<summary>ü§î Think first, then click</summary>\n",
    "\n",
    "**Answer: REGRESSION** (or Classification!)\n",
    "- Regression: Predict exact minutes (\"23 minutes\")\n",
    "- Classification: Predict time bracket (\"15-30 min\")\n",
    "- **Best choice:** Regression, then bin for display\n",
    "- Why: Gives flexibility; can always discretize later\n",
    "</details>\n",
    "\n",
    "### Scenario 4: Credit Card Default Risk\n",
    "**Context:** Customer financial data (income, debt, payment history, etc.)\n",
    "**Task:** Decide whether to approve credit increase\n",
    "\n",
    "<details>\n",
    "<summary>ü§î Think first, then click</summary>\n",
    "\n",
    "**Answer: CLASSIFICATION** (with probability)\n",
    "- Output: Will default (Yes/No)\n",
    "- Business needs: Binary decision + confidence\n",
    "- Use probability threshold to balance risk\n",
    "- Could frame as regression (\"probability of default\") but fundamentally classification\n",
    "</details>\n",
    "\n",
    "### Scenario 5: Solar Panel Energy Output\n",
    "**Context:** Weather data (sunlight hours, cloud cover, temperature, etc.)\n",
    "**Task:** Forecast energy generation for tomorrow\n",
    "\n",
    "<details>\n",
    "<summary>ü§î Think first, then click</summary>\n",
    "\n",
    "**Answer: REGRESSION**\n",
    "- Output: kWh generated (continuous)\n",
    "- Business needs: Precise energy quantity for grid planning\n",
    "- Definitely not classification - need exact amounts\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: Problem Formulation Practice\n",
    "\n",
    "**Objective:** Master the art of choosing between regression and classification\n",
    "\n",
    "**Task:**  \n",
    "For each business problem below:\n",
    "1. Decide: Regression or Classification?\n",
    "2. Justify your choice\n",
    "3. List 3-5 features you'd use\n",
    "4. Choose appropriate evaluation metric(s)\n",
    "\n",
    "**Problems:**\n",
    "\n",
    "**A) Netflix - Predict User Rating**\n",
    "- User watches a movie, will they rate it 1-5 stars?\n",
    "\n",
    "**B) Insurance - Claim Amount**\n",
    "- Predict dollar amount of insurance claim\n",
    "\n",
    "**C) Customer Support - Ticket Priority**\n",
    "- Classify support tickets as Low/Medium/High/Critical\n",
    "\n",
    "**D) Real Estate - Days on Market**\n",
    "- How many days until a house sells?\n",
    "\n",
    "**E) Healthcare - ICU Admission**\n",
    "- Will emergency patient need ICU? (Yes/No)\n",
    "\n",
    "Use the code cell below to document your answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document your answers here\n",
    "\n",
    "my_answers = {\n",
    "    'Problem A - Netflix Ratings': {\n",
    "        'Type': '',  # 'Regression' or 'Classification'\n",
    "        'Justification': '',\n",
    "        'Features': [],\n",
    "        'Metrics': []\n",
    "    },\n",
    "    'Problem B - Insurance Claims': {\n",
    "        'Type': '',\n",
    "        'Justification': '',\n",
    "        'Features': [],\n",
    "        'Metrics': []\n",
    "    },\n",
    "    'Problem C - Ticket Priority': {\n",
    "        'Type': '',\n",
    "        'Justification': '',\n",
    "        'Features': [],\n",
    "        'Metrics': []\n",
    "    },\n",
    "    'Problem D - Days on Market': {\n",
    "        'Type': '',\n",
    "        'Justification': '',\n",
    "        'Features': [],\n",
    "        'Metrics': []\n",
    "    },\n",
    "    'Problem E - ICU Admission': {\n",
    "        'Type': '',\n",
    "        'Justification': '',\n",
    "        'Features': [],\n",
    "        'Metrics': []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print your answers\n",
    "for problem, details in my_answers.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{problem}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Build Both Models\n",
    "\n",
    "**Objective:** Experience the difference between regression and classification on the same data\n",
    "\n",
    "**Task:**  \n",
    "Using the student test score dataset:\n",
    "\n",
    "1. **Build a regression model** to predict exact test scores\n",
    "   - Use LinearRegression or Ridge\n",
    "   - Evaluate with R¬≤ and RMSE\n",
    "   - Analyze feature importance (coefficients)\n",
    "\n",
    "2. **Build a classification model** to predict grade categories\n",
    "   - Use LogisticRegression or RandomForestClassifier\n",
    "   - Evaluate with accuracy, precision, recall\n",
    "   - Create confusion matrix\n",
    "\n",
    "3. **Compare:**\n",
    "   - Which is easier to interpret?\n",
    "   - Which gives more actionable insights?\n",
    "   - Which would you deploy in a real school system?\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint: Getting Started</summary>\n",
    "\n",
    "```python\n",
    "# Regression\n",
    "X = df[['study_hours', 'previous_gpa', 'attendance_rate']]\n",
    "y_reg = df['score_continuous']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, test_size=0.2)\n",
    "\n",
    "model_reg = LinearRegression()\n",
    "model_reg.fit(X_train, y_train)\n",
    "# ... evaluate\n",
    "\n",
    "# Classification\n",
    "y_clf = df['domain_based']\n",
    "# ... repeat process\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Build and compare both models\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "You've mastered the art of choosing between regression and classification!\n",
    "\n",
    "- ‚úÖ **The Decision Framework**: Target variable type determines everything\n",
    "  - Continuous number ‚Üí Regression\n",
    "  - Discrete category ‚Üí Classification\n",
    "  - When in doubt, ask: \"Can I measure with increasing precision?\"\n",
    "\n",
    "- ‚úÖ **Converting Between Types**:\n",
    "  - Regression ‚Üí Classification: Discretization (binning)\n",
    "  - Classification ‚Üí Regression: Ordinal encoding (only for ordered categories)\n",
    "\n",
    "- ‚úÖ **Algorithms That Do Both**:\n",
    "  - Decision Trees, Random Forests, Neural Networks, SVM\n",
    "  - Difference is mainly output layer and loss function\n",
    "\n",
    "- ‚úÖ **Multi-Class Classification**:\n",
    "  - One-vs-Rest or One-vs-One strategies\n",
    "  - Most modern algorithms handle this automatically\n",
    "\n",
    "- ‚úÖ **Imbalanced Data**:\n",
    "  - Use class weights, resampling, or anomaly detection\n",
    "  - Don't trust accuracy alone!\n",
    "\n",
    "### ü§î The Big Picture:\n",
    "\n",
    "**Problem formulation is 50% of the solution!**\n",
    "\n",
    "Get this right:\n",
    "1. ‚úÖ Choose appropriate algorithms\n",
    "2. ‚úÖ Use correct evaluation metrics\n",
    "3. ‚úÖ Communicate results effectively to business\n",
    "4. ‚úÖ Deploy models that actually solve the problem\n",
    "\n",
    "Get this wrong:\n",
    "1. ‚ùå Waste time on inappropriate models\n",
    "2. ‚ùå Misleading evaluation metrics\n",
    "3. ‚ùå Business stakeholders don't understand results\n",
    "4. ‚ùå Model doesn't provide value\n",
    "\n",
    "**Always start by asking: \"What exactly am I trying to predict?\"** üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Learning\n",
    "\n",
    "**Recommended Reading:**\n",
    "- [sklearn Model Selection Guide](https://scikit-learn.org/stable/tutorial/machine_learning_map/) - Visual flowchart\n",
    "- [Google's ML Crash Course: Classification](https://developers.google.com/machine-learning/crash-course/classification) - Detailed guide\n",
    "- [Imbalanced Learning](https://imbalanced-learn.org/stable/) - Library and techniques\n",
    "\n",
    "**Deep Dives:**\n",
    "- [Multi-Class Classification Strategies](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/) - Comprehensive comparison\n",
    "- [SMOTE for Imbalanced Data](https://www.youtube.com/watch?v=FheTDyCwRdE) - Synthetic sampling explained\n",
    "- [When to Use What Algorithm](https://www.youtube.com/watch?v=yN7ypxC7838) - Decision guide\n",
    "\n",
    "**Case Studies:**\n",
    "- [Kaggle: Titanic](https://www.kaggle.com/c/titanic) - Classic binary classification\n",
    "- [Kaggle: House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) - Regression challenge\n",
    "\n",
    "**Research Papers:**\n",
    "- [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599) - Understanding prediction confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚û°Ô∏è What's Next?\n",
    "\n",
    "You've completed the core of supervised learning! Next up:\n",
    "\n",
    "**In Chapter 2.3 - Unsupervised Learning & Clustering**, you'll explore:\n",
    "\n",
    "**Coming up:**\n",
    "- **Clustering algorithms**: K-Means, DBSCAN, Hierarchical Clustering\n",
    "- **Dimensionality reduction**: PCA, t-SNE, UMAP for visualization\n",
    "- **Anomaly detection**: Finding outliers without labels\n",
    "- **Real-world applications**: Customer segmentation, feature engineering\n",
    "- **When to use unsupervised learning**: Problems without labels\n",
    "\n",
    "From supervised to unsupervised‚Äîdiscovering patterns without guidance! üîç\n",
    "\n",
    "Ready to explore the unlabeled world? Open **[Chapter 2.3 - Unsupervised Learning](2.3-unsupervised-learning.ipynb)**!\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Feedback & Community\n",
    "\n",
    "**Questions?** Join our [Discord community](https://discord.gg/madeforai)\n",
    "\n",
    "**Found a bug?** [Open an issue on GitHub](https://github.com/madeforai/madeforai/issues)\n",
    "\n",
    "**Share your decision framework!** Tweet with #MadeForAI\n",
    "\n",
    "**Keep building!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
