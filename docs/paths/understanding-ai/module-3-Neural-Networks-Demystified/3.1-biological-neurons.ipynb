{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 - Biological Inspiration: How Neurons Work\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/understanding-ai/module-3/3.1-biological-neurons.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Discover how nature's most powerful computer‚Äîthe brain‚Äîinspired the AI revolution.**\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- **Biological neurons**: How the brain actually works\n",
    "- **The inspiration**: What AI borrowed from neuroscience\n",
    "- **Mathematical neurons**: Translating biology into code\n",
    "- **The perceptron**: First artificial neuron (1958!)\n",
    "- **Limitations**: What brains can do that early AI couldn't\n",
    "- **Modern connections**: How today's deep learning relates to biology\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time\n",
    "30-35 minutes\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Completed Module 2 (Machine Learning Fundamentals)\n",
    "- Basic understanding of linear algebra\n",
    "- Curiosity about how the brain works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† The Most Powerful Computer in the Universe\n",
    "\n",
    "Right now, as you read this, **86 billion neurons** in your brain are firing in complex patterns, allowing you to:\n",
    "- ‚úÖ Read and understand text\n",
    "- ‚úÖ Recognize patterns\n",
    "- ‚úÖ Make predictions\n",
    "- ‚úÖ Learn new concepts\n",
    "- ‚úÖ Remember information\n",
    "\n",
    "**Your brain**:\n",
    "- ~86 billion neurons\n",
    "- ~100 trillion connections (synapses)\n",
    "- Uses only ~20 watts of power\n",
    "- Parallel processing at massive scale\n",
    "- Can learn from just a few examples\n",
    "\n",
    "**Modern AI**:\n",
    "- GPT-4: ~1.76 trillion parameters\n",
    "- Requires megawatts of power for training\n",
    "- Needs millions of examples to learn\n",
    "- Still can't match human general intelligence\n",
    "\n",
    "**The Big Question**: Can we build machines that learn like brains?\n",
    "\n",
    "The answer: We're trying! And it started with understanding a single neuron.\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a split-screen comparison showing brain vs AI.\n",
    "Style: Modern scientific illustration with subtle futuristic elements.\n",
    "Left side: Cross-section of human brain showing neural network (organic, biological)\n",
    "- Labeled regions: cortex, neurons, synapses\n",
    "- Glowing neural pathways showing activity\n",
    "- Stats overlay: '86 billion neurons, 20 watts'\n",
    "Right side: Abstract representation of artificial neural network\n",
    "- Geometric nodes and connections (digital, mathematical)\n",
    "- Glowing data flowing through network\n",
    "- Stats overlay: 'Trillions of parameters, megawatts'\n",
    "Center: Question mark with text 'Can we recreate intelligence?'\n",
    "Color scheme: Purple/pink for brain (organic), blue/cyan for AI (digital)\n",
    "Include icons: brain, circuit, electricity symbols\n",
    "Format: Wide horizontal split, 16:9 ratio, professional scientific aesthetic.\" -->\n",
    "\n",
    "Let's start with the building block: **one neuron**. üî¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import libraries\n",
    "# Uncomment if running in Google Colab\n",
    "# !pip install numpy pandas matplotlib seaborn plotly -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, FancyArrowPatch\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Better defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(\"üìò Module 3.1: Biological Neurons\")\n",
    "print(\"üß† Ready to explore the brain-AI connection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 1: Anatomy of a Biological Neuron\n",
    "\n",
    "### The Basic Structure\n",
    "\n",
    "A biological neuron has three main parts:\n",
    "\n",
    "#### 1. **Dendrites** (Input)\n",
    "- Branch-like structures\n",
    "- Receive signals from other neurons\n",
    "- Thousands of input connections\n",
    "- Think: \"Antennas receiving radio signals\"\n",
    "\n",
    "#### 2. **Cell Body / Soma** (Processing)\n",
    "- Integrates all incoming signals\n",
    "- Decides whether to \"fire\"\n",
    "- Contains nucleus and cellular machinery\n",
    "- Think: \"Decision-making headquarters\"\n",
    "\n",
    "#### 3. **Axon** (Output)\n",
    "- Long fiber extending from cell body\n",
    "- Transmits signals to other neurons\n",
    "- Can be over 1 meter long!\n",
    "- Think: \"Telephone wire transmitting message\"\n",
    "\n",
    "#### 4. **Synapses** (Connections)\n",
    "- Junctions between neurons\n",
    "- Chemical messengers (neurotransmitters)\n",
    "- Can be strengthened or weakened\n",
    "- **This is where learning happens!**\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a detailed anatomical diagram of a biological neuron.\n",
    "Style: Educational medical illustration with clear labels.\n",
    "Main neuron (center): Large cell body with branching dendrites on left, long axon extending right.\n",
    "Labeled components:\n",
    "- Dendrites (left): Multiple branch-like structures in purple, with arrows showing 'Inputs from other neurons'\n",
    "- Cell Body/Soma (center): Circular structure in pink with visible nucleus\n",
    "- Axon (right): Long tube extending right in blue, covered with myelin sheath segments\n",
    "- Synapses: Zoomed inset showing synaptic gap, vesicles, neurotransmitters\n",
    "- Axon terminals: Branching ends connecting to next neurons\n",
    "Show signal direction with glowing arrows: Dendrites ‚Üí Soma ‚Üí Axon ‚Üí Synapse\n",
    "Include annotations: 'Electrical signal', 'Chemical signal at synapse'\n",
    "Background: Subtle neural network pattern\n",
    "Color scheme: Purple for dendrites, pink for soma, blue for axon, green for synapses\n",
    "Format: Horizontal layout optimized for understanding flow, professional medical textbook style.\" -->\n",
    "\n",
    "### How It Works: The Action Potential\n",
    "\n",
    "**The Process** (simplified):\n",
    "\n",
    "1. **Resting state**: Neuron is negatively charged inside (-70mV)\n",
    "\n",
    "2. **Input signals arrive**: \n",
    "   - Dendrites receive chemical signals\n",
    "   - Convert to electrical charges\n",
    "   - Signals can be **excitatory** (+) or **inhibitory** (-)\n",
    "\n",
    "3. **Integration**:\n",
    "   - Cell body sums up all inputs\n",
    "   - Weighted sum of signals\n",
    "   - Each synapse has different \"strength\"\n",
    "\n",
    "4. **Decision (All-or-Nothing)**:\n",
    "   - If sum exceeds threshold (~-55mV): **FIRE!** ‚ö°\n",
    "   - If below threshold: Stay silent üò¥\n",
    "   - No partial firing (binary decision)\n",
    "\n",
    "5. **Propagation**:\n",
    "   - Action potential travels down axon\n",
    "   - Speed: up to 120 m/s!\n",
    "   - Reaches synapses\n",
    "\n",
    "6. **Transmission**:\n",
    "   - Electrical signal triggers chemical release\n",
    "   - Neurotransmitters cross synaptic gap\n",
    "   - Next neuron receives the message\n",
    "\n",
    "**Key Insight**: This is a **weighted sum** followed by a **threshold function**!\n",
    "\n",
    "Sound familiar? That's the core idea behind artificial neurons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate biological neuron behavior\n",
    "\n",
    "def simulate_neuron_firing(inputs, weights, threshold=-55):\n",
    "    \"\"\"\n",
    "    Simulate a biological neuron's decision to fire.\n",
    "    \n",
    "    Parameters:\n",
    "    - inputs: Array of input signals (from dendrites)\n",
    "    - weights: Synaptic strengths (how strong each connection is)\n",
    "    - threshold: Voltage threshold for firing (default -55mV)\n",
    "    \n",
    "    Returns:\n",
    "    - membrane_potential: Sum of weighted inputs\n",
    "    - fires: Boolean indicating if neuron fires\n",
    "    \"\"\"\n",
    "    # Step 1: Weight the inputs (different synaptic strengths)\n",
    "    weighted_inputs = inputs * weights\n",
    "    \n",
    "    # Step 2: Sum all inputs (integration in cell body)\n",
    "    membrane_potential = np.sum(weighted_inputs)\n",
    "    \n",
    "    # Step 3: Compare to threshold (all-or-nothing decision)\n",
    "    fires = membrane_potential >= threshold\n",
    "    \n",
    "    return membrane_potential, fires, weighted_inputs\n",
    "\n",
    "# Example: Neuron with 5 input connections\n",
    "print(\"üß† Simulating Biological Neuron\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Input signals (in mV, relative to resting potential)\n",
    "# Positive = excitatory, Negative = inhibitory\n",
    "inputs = np.array([10, -5, 15, 8, -3])  # 5 input signals\n",
    "weights = np.array([0.5, 1.0, 0.8, 0.3, 1.2])  # Synaptic strengths\n",
    "threshold = 10  # Simplified threshold\n",
    "\n",
    "membrane_potential, fires, weighted = simulate_neuron_firing(inputs, weights, threshold)\n",
    "\n",
    "print(\"\\nüìä Input Signals (from different dendrites):\")\n",
    "for i, (inp, w, wtd) in enumerate(zip(inputs, weights, weighted), 1):\n",
    "    signal_type = \"Excitatory (+)\" if inp > 0 else \"Inhibitory (-)\"\n",
    "    print(f\"  Dendrite {i}: {inp:+6.1f} mV √ó {w:.2f} weight = {wtd:+6.2f} mV  [{signal_type}]\")\n",
    "\n",
    "print(f\"\\n‚ö° Integration (Cell Body):\")\n",
    "print(f\"  Sum of weighted inputs: {membrane_potential:.2f} mV\")\n",
    "print(f\"  Threshold: {threshold:.2f} mV\")\n",
    "\n",
    "print(f\"\\nüéØ Decision:\")\n",
    "if fires:\n",
    "    print(f\"  ‚úÖ FIRES! ({membrane_potential:.2f} >= {threshold:.2f})\")\n",
    "    print(f\"  ‚ö° Action potential sent down axon!\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Silent ({membrane_potential:.2f} < {threshold:.2f})\")\n",
    "    print(f\"  üò¥ Neuron remains at rest\")\n",
    "\n",
    "print(f\"\\nüí° This is the biological basis of neural computation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neuron integration process\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Input visualization\n",
    "ax1 = axes[0]\n",
    "colors = ['green' if x > 0 else 'red' for x in inputs]\n",
    "bars = ax1.bar(range(len(inputs)), inputs, color=colors, alpha=0.6, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax1.set_xlabel('Input Dendrite', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Signal Strength (mV)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Input Signals from Dendrites', fontsize=15, fontweight='bold', pad=15)\n",
    "ax1.set_xticks(range(len(inputs)))\n",
    "ax1.set_xticklabels([f'D{i+1}' for i in range(len(inputs))])\n",
    "ax1.grid(alpha=0.3, axis='y')\n",
    "ax1.legend(['Baseline', 'Excitatory (+)', 'Inhibitory (-)'], loc='upper right')\n",
    "\n",
    "# Right: Integration and threshold\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Show cumulative sum (integration process)\n",
    "cumsum = np.cumsum(weighted)\n",
    "ax2.plot(range(len(cumsum)), cumsum, 'bo-', linewidth=2.5, markersize=10, label='Integration Process')\n",
    "ax2.axhline(y=threshold, color='red', linestyle='--', linewidth=2.5, label=f'Threshold ({threshold} mV)')\n",
    "ax2.axhline(y=membrane_potential, color='green' if fires else 'orange', \n",
    "           linestyle=':', linewidth=2, label=f'Final Potential ({membrane_potential:.1f} mV)')\n",
    "\n",
    "# Fill area\n",
    "ax2.fill_between(range(len(cumsum)), 0, cumsum, alpha=0.3, color='blue')\n",
    "\n",
    "# Annotation\n",
    "if fires:\n",
    "    ax2.annotate('üî• FIRES!', xy=(len(cumsum)-1, membrane_potential),\n",
    "                xytext=(len(cumsum)-2, membrane_potential+3),\n",
    "                fontsize=14, fontweight='bold', color='green',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "else:\n",
    "    ax2.annotate('üò¥ Silent', xy=(len(cumsum)-1, membrane_potential),\n",
    "                xytext=(len(cumsum)-2, membrane_potential-3),\n",
    "                fontsize=14, fontweight='bold', color='orange',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
    "                arrowprops=dict(arrowstyle='->', color='orange', lw=2))\n",
    "\n",
    "ax2.set_xlabel('Integration Step', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Membrane Potential (mV)', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Neural Integration & Threshold Decision', fontsize=15, fontweight='bold', pad=15)\n",
    "ax2.set_xticks(range(len(cumsum)))\n",
    "ax2.set_xticklabels([f'After D{i+1}' for i in range(len(cumsum))])\n",
    "ax2.legend(loc='best', fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization shows:\")\n",
    "print(\"  ‚Ä¢ Left: Raw input signals (green = excitatory, red = inhibitory)\")\n",
    "print(\"  ‚Ä¢ Right: Cumulative integration process\")\n",
    "print(\"  ‚Ä¢ Neuron fires only if sum crosses threshold!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 2: From Biology to Mathematics\n",
    "\n",
    "### The Mathematical Translation\n",
    "\n",
    "Scientists in the 1940s-1950s looked at biological neurons and asked:\n",
    "> \"Can we capture this in a mathematical equation?\"\n",
    "\n",
    "**The answer**: YES! Here's the translation:\n",
    "\n",
    "| Biological Component | Mathematical Equivalent |\n",
    "|---------------------|------------------------|\n",
    "| **Dendrite inputs** | Input vector: $x = [x_1, x_2, ..., x_n]$ |\n",
    "| **Synaptic weights** | Weight vector: $w = [w_1, w_2, ..., w_n]$ |\n",
    "| **Cell body integration** | Weighted sum: $z = \\sum_{i=1}^{n} w_i x_i + b$ |\n",
    "| **Threshold decision** | Activation function: $a = f(z)$ |\n",
    "| **Action potential** | Binary output: 0 or 1 |\n",
    "\n",
    "### The Artificial Neuron Equation\n",
    "\n",
    "**Step 1: Weighted Sum (Integration)**\n",
    "$$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$\n",
    "\n",
    "Or in vector notation:\n",
    "$$z = \\mathbf{w}^T\\mathbf{x} + b$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ = input $i$ (dendrite signal)\n",
    "- $w_i$ = weight $i$ (synaptic strength)\n",
    "- $b$ = bias (neuron's baseline excitability)\n",
    "- $z$ = pre-activation (membrane potential)\n",
    "\n",
    "**Step 2: Threshold Function (Firing Decision)**\n",
    "$$a = f(z) = \\begin{cases} \n",
    "1 & \\text{if } z \\geq \\theta \\\\\n",
    "0 & \\text{if } z < \\theta\n",
    "\\end{cases}$$\n",
    "\n",
    "This is called the **step function** or **Heaviside function**.\n",
    "\n",
    "**The Beauty**: This simple equation captures the essence of neural computation!\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a side-by-side comparison showing biological neuron vs artificial neuron.\n",
    "Style: Educational diagram with mathematical notation.\n",
    "Left side (Biological):\n",
    "- Neuron diagram with dendrites, soma, axon\n",
    "- Label dendrites as 'x‚ÇÅ, x‚ÇÇ, x‚ÇÉ' (inputs)\n",
    "- Label synapses as 'w‚ÇÅ, w‚ÇÇ, w‚ÇÉ' (weights)\n",
    "- Label soma as 'Œ£ (summation)'\n",
    "- Label axon as 'Output'\n",
    "Right side (Artificial):\n",
    "- Mathematical diagram showing computation graph\n",
    "- Inputs x‚ÇÅ, x‚ÇÇ, x‚ÇÉ as circles on left\n",
    "- Arrows with weights w‚ÇÅ, w‚ÇÇ, w‚ÇÉ pointing to summation node\n",
    "- Summation node (Œ£) with equation: z = Œ£wixi + b\n",
    "- Activation function box: f(z)\n",
    "- Output node on right\n",
    "Center: Large equals sign showing equivalence\n",
    "Include equations and annotations showing the mapping\n",
    "Color scheme: Purple for biological, blue for mathematical\n",
    "Format: Side-by-side comparison, clear mathematical notation.\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement mathematical neuron\n",
    "\n",
    "class ArtificialNeuron:\n",
    "    \"\"\"\n",
    "    Mathematical model of a neuron.\n",
    "    Inspired by biological neurons (McCulloch-Pitts, 1943).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        \"\"\"Initialize with random weights and bias\"\"\"\n",
    "        # Synaptic weights (learnable)\n",
    "        self.weights = np.random.randn(num_inputs) * 0.5\n",
    "        \n",
    "        # Bias term (neuron's baseline excitability)\n",
    "        self.bias = np.random.randn() * 0.5\n",
    "    \n",
    "    def step_function(self, z, threshold=0.0):\n",
    "        \"\"\"Heaviside step function (threshold activation)\"\"\"\n",
    "        return 1 if z >= threshold else 0\n",
    "    \n",
    "    def forward(self, inputs, threshold=0.0):\n",
    "        \"\"\"\n",
    "        Compute neuron output (forward pass).\n",
    "        \n",
    "        Steps:\n",
    "        1. Weighted sum: z = w¬∑x + b\n",
    "        2. Activation: a = f(z)\n",
    "        \"\"\"\n",
    "        # Step 1: Weighted sum (integration)\n",
    "        z = np.dot(self.weights, inputs) + self.bias\n",
    "        \n",
    "        # Step 2: Threshold function (firing decision)\n",
    "        a = self.step_function(z, threshold)\n",
    "        \n",
    "        return a, z\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ArtificialNeuron(weights={self.weights}, bias={self.bias:.3f})\"\n",
    "\n",
    "# Create and test an artificial neuron\n",
    "print(\"ü§ñ Artificial Neuron Simulation\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "neuron = ArtificialNeuron(num_inputs=3)\n",
    "print(f\"\\nüìä Neuron Parameters:\")\n",
    "print(f\"  Weights: {neuron.weights}\")\n",
    "print(f\"  Bias: {neuron.bias:.3f}\")\n",
    "\n",
    "# Test with different inputs\n",
    "test_inputs = [\n",
    "    np.array([1.0, 0.5, -0.3]),\n",
    "    np.array([0.2, 0.1, 0.8]),\n",
    "    np.array([-1.0, -0.5, -0.2])\n",
    "]\n",
    "\n",
    "print(f\"\\nüî¨ Testing Different Input Patterns:\\n\")\n",
    "for i, inputs in enumerate(test_inputs, 1):\n",
    "    output, pre_activation = neuron.forward(inputs)\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"  Inputs: {inputs}\")\n",
    "    print(f\"  Weighted sum (z): {pre_activation:.3f}\")\n",
    "    print(f\"  Output (a): {output} {'‚ö° (FIRES!)' if output == 1 else 'üò¥ (silent)'}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° The artificial neuron mimics biological behavior!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 3: The Perceptron (1958) - First Learning Algorithm\n",
    "\n",
    "### Frank Rosenblatt's Breakthrough\n",
    "\n",
    "In 1958, Frank Rosenblatt invented the **Perceptron**‚Äîthe first artificial neuron that could **learn**!\n",
    "\n",
    "**Key Innovation**: The weights could be adjusted based on errors.\n",
    "\n",
    "### The Perceptron Learning Algorithm\n",
    "\n",
    "**Goal**: Learn weights that correctly classify inputs\n",
    "\n",
    "**Algorithm**:\n",
    "```\n",
    "1. Initialize weights randomly\n",
    "2. For each training example (x, y_true):\n",
    "   a. Compute prediction: y_pred = step(w¬∑x + b)\n",
    "   b. Calculate error: error = y_true - y_pred\n",
    "   c. Update weights: w = w + learning_rate √ó error √ó x\n",
    "   d. Update bias: b = b + learning_rate √ó error\n",
    "3. Repeat until convergence (or max iterations)\n",
    "```\n",
    "\n",
    "**Intuition**:\n",
    "- If prediction is correct: weights don't change\n",
    "- If prediction is wrong: adjust weights to reduce error\n",
    "- Learning rate controls size of adjustments\n",
    "\n",
    "**The Learning Rule** (delta rule):\n",
    "$$\\Delta w_i = \\eta \\cdot (y_{\\text{true}} - y_{\\text{pred}}) \\cdot x_i$$\n",
    "\n",
    "Where $\\eta$ is the learning rate.\n",
    "\n",
    "### What Can a Perceptron Learn?\n",
    "\n",
    "**Good news**: Perceptrons can learn any **linearly separable** function\n",
    "- AND gate ‚úÖ\n",
    "- OR gate ‚úÖ\n",
    "- NOT gate ‚úÖ\n",
    "\n",
    "**Bad news**: Can't learn **non-linearly separable** functions\n",
    "- XOR gate ‚ùå\n",
    "- This limitation caused the first \"AI Winter\" in the 1970s!\n",
    "\n",
    "<!-- [PLACEHOLDER IMAGE]\n",
    "Prompt for image generation:\n",
    "\"Create a diagram showing linearly separable vs non-linearly separable problems.\n",
    "Style: Educational visualization with geometric clarity.\n",
    "Top row: 'Perceptron CAN solve'\n",
    "- Left: AND gate - 2D plot with 4 points (0,0), (0,1), (1,0), (1,1)\n",
    "  Class 0 (red): (0,0), (0,1), (1,0)\n",
    "  Class 1 (blue): (1,1)\n",
    "  Linear boundary (green line) separating them\n",
    "- Right: OR gate - similar plot with linear separation possible\n",
    "Bottom row: 'Perceptron CANNOT solve'\n",
    "- XOR gate - same 4 points but arranged:\n",
    "  Class 0 (red): (0,0), (1,1)\n",
    "  Class 1 (blue): (0,1), (1,0)\n",
    "  Show failed linear boundaries (dotted lines) unable to separate\n",
    "  Big red X over the plot\n",
    "Include text: 'The XOR Problem - Why we needed deeper networks!'\n",
    "Color scheme: Red/blue for classes, green for successful boundary, red X for failure\n",
    "Format: 2x2 grid showing the limitation clearly.\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Perceptron learning algorithm\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Rosenblatt's Perceptron (1958)\n",
    "    First artificial neuron that could LEARN!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, learning_rate=0.1):\n",
    "        self.weights = np.zeros(num_inputs)\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.errors_history = []\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Make prediction (0 or 1)\"\"\"\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        return 1 if z >= 0 else 0\n",
    "    \n",
    "    def train(self, X, y, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Train perceptron using the delta rule.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input features (n_samples, n_features)\n",
    "        - y: True labels (n_samples,)\n",
    "        - epochs: Number of training iterations\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_error = 0\n",
    "            \n",
    "            for xi, yi_true in zip(X, y):\n",
    "                # Forward pass\n",
    "                yi_pred = self.predict(xi)\n",
    "                \n",
    "                # Calculate error\n",
    "                error = yi_true - yi_pred\n",
    "                \n",
    "                # Update weights using delta rule\n",
    "                if error != 0:  # Only update if wrong\n",
    "                    self.weights += self.learning_rate * error * xi\n",
    "                    self.bias += self.learning_rate * error\n",
    "                    total_error += abs(error)\n",
    "            \n",
    "            self.errors_history.append(total_error)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 2 == 0:\n",
    "                print(f\"Epoch {epoch+1:2d}: Errors = {total_error}\")\n",
    "            \n",
    "            # Early stopping if perfect\n",
    "            if total_error == 0:\n",
    "                if verbose:\n",
    "                    print(f\"‚úÖ Converged at epoch {epoch+1}!\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Example: Learn AND gate\n",
    "print(\"üéì Training Perceptron on AND Gate\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# AND gate truth table\n",
    "X_and = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y_and = np.array([0, 0, 0, 1])  # Only 1 if both inputs are 1\n",
    "\n",
    "print(\"\\nüìã AND Gate Truth Table:\")\n",
    "print(\"  x1  x2  | output\")\n",
    "print(\"  -------+-------\")\n",
    "for x, y in zip(X_and, y_and):\n",
    "    print(f\"  {x[0]}   {x[1]}  |   {y}\")\n",
    "\n",
    "# Train perceptron\n",
    "print(\"\\nüî¨ Training Process:\\n\")\n",
    "perceptron_and = Perceptron(num_inputs=2, learning_rate=0.1)\n",
    "perceptron_and.train(X_and, y_and, epochs=10)\n",
    "\n",
    "print(f\"\\nüéØ Final Parameters:\")\n",
    "print(f\"  Weights: {perceptron_and.weights}\")\n",
    "print(f\"  Bias: {perceptron_and.bias:.3f}\")\n",
    "\n",
    "# Test predictions\n",
    "print(f\"\\n‚úÖ Testing Learned Function:\\n\")\n",
    "print(\"  x1  x2  | True | Pred | Correct?\")\n",
    "print(\"  -------+------+------+---------\")\n",
    "for x, y_true in zip(X_and, y_and):\n",
    "    y_pred = perceptron_and.predict(x)\n",
    "    correct = \"‚úì\" if y_pred == y_true else \"‚úó\"\n",
    "    print(f\"  {x[0]}   {x[1]}  |  {y_true}   |  {y_pred}   |    {correct}\")\n",
    "\n",
    "print(\"\\nüéâ Perceptron successfully learned the AND function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary and learning process\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left: Decision boundary\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Plot data points\n",
    "for x, y in zip(X_and, y_and):\n",
    "    color = 'blue' if y == 1 else 'red'\n",
    "    marker = 'o' if y == 1 else 'x'\n",
    "    ax1.scatter(x[0], x[1], c=color, s=200, marker=marker, \n",
    "               edgecolors='black', linewidth=2, zorder=3)\n",
    "\n",
    "# Plot decision boundary\n",
    "# Boundary is where w¬∑x + b = 0\n",
    "# x2 = -(w1*x1 + b) / w2\n",
    "x1_boundary = np.linspace(-0.5, 1.5, 100)\n",
    "if perceptron_and.weights[1] != 0:\n",
    "    x2_boundary = -(perceptron_and.weights[0] * x1_boundary + perceptron_and.bias) / perceptron_and.weights[1]\n",
    "    ax1.plot(x1_boundary, x2_boundary, 'g-', linewidth=3, label='Decision Boundary', zorder=2)\n",
    "\n",
    "# Shade regions\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "Z = np.array([perceptron_and.predict(np.array([x, y])) for x, y in zip(xx.ravel(), yy.ravel())])\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax1.contourf(xx, yy, Z, alpha=0.2, levels=1, colors=['red', 'blue'], zorder=1)\n",
    "\n",
    "ax1.set_xlabel('Input x‚ÇÅ', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Input x‚ÇÇ', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Perceptron Decision Boundary (AND Gate)', fontsize=15, fontweight='bold', pad=15)\n",
    "ax1.legend(['Decision Boundary', 'Class 1 (True)', 'Class 0 (False)'], fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xlim(-0.5, 1.5)\n",
    "ax1.set_ylim(-0.5, 1.5)\n",
    "\n",
    "# Right: Learning curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(1, len(perceptron_and.errors_history) + 1), \n",
    "        perceptron_and.errors_history, 'bo-', linewidth=2.5, markersize=8)\n",
    "ax2.set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Total Errors', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Learning Curve', fontsize=15, fontweight='bold', pad=15)\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_ylim(bottom=0)\n",
    "\n",
    "# Add annotation\n",
    "if perceptron_and.errors_history[-1] == 0:\n",
    "    ax2.annotate('Perfect Classification!', \n",
    "                xy=(len(perceptron_and.errors_history), 0),\n",
    "                xytext=(len(perceptron_and.errors_history)//2, max(perceptron_and.errors_history)//2),\n",
    "                fontsize=12, fontweight='bold', color='green',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization shows:\")\n",
    "print(\"  ‚Ä¢ Left: Linear decision boundary separating the two classes\")\n",
    "print(\"  ‚Ä¢ Right: Error decreases to zero as perceptron learns\")\n",
    "print(\"  ‚Ä¢ The perceptron found the right weights to solve AND!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 4: The XOR Problem - The Perceptron's Fatal Flaw\n",
    "\n",
    "### The Problem That Broke AI\n",
    "\n",
    "In 1969, Marvin Minsky and Seymour Papert published \"Perceptrons\", proving that a single perceptron **cannot** learn XOR.\n",
    "\n",
    "**XOR (Exclusive OR)**:\n",
    "- Output is 1 if inputs are different\n",
    "- Output is 0 if inputs are same\n",
    "\n",
    "**Truth Table**:\n",
    "```\n",
    "x1  x2  | XOR\n",
    "--------+----\n",
    "0   0   |  0\n",
    "0   1   |  1\n",
    "1   0   |  1\n",
    "1   1   |  0\n",
    "```\n",
    "\n",
    "**Why perceptrons fail**:\n",
    "- XOR is **not linearly separable**\n",
    "- No single straight line can separate the classes\n",
    "- Need a curved boundary\n",
    "\n",
    "**The Solution** (came later):\n",
    "- Multiple layers of neurons (Multi-Layer Perceptron)\n",
    "- Non-linear activation functions\n",
    "- This is what we call **deep learning** today!\n",
    "\n",
    "Let's demonstrate the failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to learn XOR (it will fail!)\n",
    "print(\"‚ùå Attempting to Learn XOR (This Will Fail!)\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# XOR truth table\n",
    "X_xor = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR pattern\n",
    "\n",
    "print(\"\\nüìã XOR Truth Table:\")\n",
    "print(\"  x1  x2  | output\")\n",
    "print(\"  -------+-------\")\n",
    "for x, y in zip(X_xor, y_xor):\n",
    "    print(f\"  {x[0]}   {x[1]}  |   {y}\")\n",
    "\n",
    "# Try to train (it will oscillate)\n",
    "print(\"\\nüî¨ Training Process (Max 20 epochs):\\n\")\n",
    "perceptron_xor = Perceptron(num_inputs=2, learning_rate=0.1)\n",
    "perceptron_xor.train(X_xor, y_xor, epochs=20, verbose=True)\n",
    "\n",
    "# Test (will have errors)\n",
    "print(f\"\\n‚ùå Testing Results:\\n\")\n",
    "print(\"  x1  x2  | True | Pred | Correct?\")\n",
    "print(\"  -------+------+------+---------\")\n",
    "correct_count = 0\n",
    "for x, y_true in zip(X_xor, y_xor):\n",
    "    y_pred = perceptron_xor.predict(x)\n",
    "    correct = \"‚úì\" if y_pred == y_true else \"‚úó\"\n",
    "    if y_pred == y_true:\n",
    "        correct_count += 1\n",
    "    print(f\"  {x[0]}   {x[1]}  |  {y_true}   |  {y_pred}   |    {correct}\")\n",
    "\n",
    "accuracy = correct_count / len(y_xor)\n",
    "print(f\"\\nüìä Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "if accuracy < 1.0:\n",
    "    print(\"\\n‚ö†Ô∏è  Perceptron CANNOT learn XOR!\")\n",
    "    print(\"    The problem is not linearly separable.\")\n",
    "    print(\"    This limitation led to the first AI Winter (1970s).\")\n",
    "    print(\"\\nüí° Solution: Use multiple layers (deep learning)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize why XOR fails\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left: XOR data points\n",
    "ax1 = axes[0]\n",
    "for x, y in zip(X_xor, y_xor):\n",
    "    color = 'blue' if y == 1 else 'red'\n",
    "    marker = 'o' if y == 1 else 'x'\n",
    "    ax1.scatter(x[0], x[1], c=color, s=200, marker=marker,\n",
    "               edgecolors='black', linewidth=2, zorder=3,\n",
    "               label=f'Class {y}' if x[0] == 0 and x[1] == 0 or (x[0] == 0 and x[1] == 1 and y == 1) else '')\n",
    "\n",
    "# Try to show some failed boundaries\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "boundaries = [\n",
    "    ('Horizontal', 0.5 * np.ones_like(x_line)),\n",
    "    ('Vertical', x_line * 0 + 0.5),\n",
    "    ('Diagonal', x_line)\n",
    "]\n",
    "\n",
    "for label, y_line in boundaries:\n",
    "    if label == 'Vertical':\n",
    "        ax1.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    elif label == 'Horizontal':\n",
    "        ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    else:\n",
    "        ax1.plot(x_line, y_line, 'gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Input x‚ÇÅ', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Input x‚ÇÇ', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('XOR: No Linear Boundary Works!', fontsize=15, fontweight='bold', pad=15, color='red')\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xlim(-0.5, 1.5)\n",
    "ax1.set_ylim(-0.5, 1.5)\n",
    "ax1.legend(['Class 0 (red x)', 'Class 1 (blue o)', 'Failed boundaries'], fontsize=10)\n",
    "\n",
    "# Add text annotation\n",
    "ax1.text(0.75, 0.2, '‚ùå No single line\\ncan separate!', \n",
    "        fontsize=12, fontweight='bold', color='red',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "        ha='center')\n",
    "\n",
    "# Right: Learning curve showing oscillation\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(1, len(perceptron_xor.errors_history) + 1),\n",
    "        perceptron_xor.errors_history, 'ro-', linewidth=2.5, markersize=8)\n",
    "ax2.set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Total Errors', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('XOR Learning Curve (Never Converges)', fontsize=15, fontweight='bold', pad=15, color='red')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_ylim(bottom=0)\n",
    "\n",
    "# Annotation\n",
    "ax2.annotate('Errors oscillate,\\nnever reach zero!',\n",
    "            xy=(len(perceptron_xor.errors_history)//2, np.mean(perceptron_xor.errors_history)),\n",
    "            xytext=(len(perceptron_xor.errors_history)*0.7, max(perceptron_xor.errors_history)*0.8),\n",
    "            fontsize=11, fontweight='bold', color='red',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization reveals:\")\n",
    "print(\"  ‚Ä¢ Left: No straight line can separate the XOR pattern\")\n",
    "print(\"  ‚Ä¢ Right: Training errors never converge to zero\")\n",
    "print(\"  ‚Ä¢ This is a fundamental limitation of single-layer perceptrons!\")\n",
    "print(\"\\nüí° Next chapter: We'll see how multiple layers solve this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Part 5: Modern Neural Networks - How Far We've Come\n",
    "\n",
    "### From Single Neurons to Deep Networks\n",
    "\n",
    "**1943**: McCulloch-Pitts neuron (theoretical model)  \n",
    "**1958**: Perceptron (first learning algorithm)  \n",
    "**1969**: XOR problem discovered ‚Üí AI Winter  \n",
    "**1986**: Backpropagation rediscovered ‚Üí Multi-layer networks  \n",
    "**2012**: Deep learning revolution begins (ImageNet)  \n",
    "**2017+**: Transformers, GPT, modern AI era  \n",
    "\n",
    "### What Changed?\n",
    "\n",
    "**Then (1958)**:\n",
    "- Single layer\n",
    "- Step activation function\n",
    "- Simple learning rule\n",
    "- Only linear separation\n",
    "\n",
    "**Now (2026)**:\n",
    "- Hundreds of layers\n",
    "- Sophisticated activation functions (ReLU, GELU)\n",
    "- Advanced optimizers (Adam, AdamW)\n",
    "- Can learn any function (universal approximation)\n",
    "\n",
    "### What Stayed the Same?\n",
    "\n",
    "**Core principles**:\n",
    "1. ‚úÖ Weighted sums of inputs\n",
    "2. ‚úÖ Non-linear activation\n",
    "3. ‚úÖ Learning by adjusting weights\n",
    "4. ‚úÖ Inspired by biological neurons\n",
    "\n",
    "**The foundation hasn't changed‚Äîwe just stacked it deeper!**\n",
    "\n",
    "### Biological vs Artificial (Today)\n",
    "\n",
    "| Aspect | Biological Brain | Artificial Neural Networks |\n",
    "|--------|-----------------|---------------------------|\n",
    "| **Speed** | ~100 Hz firing rate | Billions of ops/second |\n",
    "| **Precision** | Noisy, analog | Perfect, digital |\n",
    "| **Energy** | ~20W | Megawatts (training) |\n",
    "| **Learning** | Few examples | Millions of examples |\n",
    "| **Parallel** | Massively parallel | Somewhat parallel (GPUs) |\n",
    "| **Flexibility** | Extreme generalization | Narrow specialization |\n",
    "| **Repair** | Self-healing | Brittle |\n",
    "\n",
    "**The Gap**: Despite amazing progress, we're still far from matching biological intelligence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise: Build Your Own Logic Gates\n",
    "\n",
    "**Objective**: Implement and train perceptrons for different logic gates\n",
    "\n",
    "**Tasks**:\n",
    "1. Train perceptrons for OR and NAND gates\n",
    "2. Verify they learn correctly\n",
    "3. Visualize the decision boundaries\n",
    "4. Compare learning curves\n",
    "\n",
    "**Truth Tables**:\n",
    "\n",
    "**OR Gate**:\n",
    "```\n",
    "(0,0) ‚Üí 0\n",
    "(0,1) ‚Üí 1\n",
    "(1,0) ‚Üí 1\n",
    "(1,1) ‚Üí 1\n",
    "```\n",
    "\n",
    "**NAND Gate**:\n",
    "```\n",
    "(0,0) ‚Üí 1\n",
    "(0,1) ‚Üí 1\n",
    "(1,0) ‚Üí 1\n",
    "(1,1) ‚Üí 0\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hint: Getting started</summary>\n",
    "\n",
    "```python\n",
    "# OR gate\n",
    "X_or = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "perceptron_or = Perceptron(num_inputs=2)\n",
    "perceptron_or.train(X_or, y_or, epochs=10)\n",
    "```\n",
    "</details>\n",
    "\n",
    "**Bonus Challenge**: \n",
    "- Can you build XOR using only OR, AND, and NAND gates?\n",
    "- This is how multi-layer networks solve XOR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Train perceptrons for OR and NAND gates\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "You've traced AI's journey from biology to mathematics!\n",
    "\n",
    "- ‚úÖ **Biological Neurons**:\n",
    "  - Dendrites receive signals (inputs)\n",
    "  - Soma integrates signals (weighted sum)\n",
    "  - Axon fires if threshold exceeded (activation)\n",
    "  - Synapses have variable strengths (weights)\n",
    "  - Learning happens by changing synaptic strengths\n",
    "\n",
    "- ‚úÖ **Mathematical Neurons**:\n",
    "  - Inputs: $\\mathbf{x} = [x_1, ..., x_n]$\n",
    "  - Weights: $\\mathbf{w} = [w_1, ..., w_n]$\n",
    "  - Integration: $z = \\mathbf{w}^T\\mathbf{x} + b$\n",
    "  - Activation: $a = f(z)$\n",
    "  - Direct translation from biology!\n",
    "\n",
    "- ‚úÖ **The Perceptron (1958)**:\n",
    "  - First artificial neuron that could learn\n",
    "  - Delta rule: $\\Delta w = \\eta(y_{true} - y_{pred})x$\n",
    "  - Can learn linearly separable functions\n",
    "  - Pioneered machine learning\n",
    "\n",
    "- ‚úÖ **The XOR Problem**:\n",
    "  - Single perceptron cannot learn XOR\n",
    "  - Problem is not linearly separable\n",
    "  - Led to first AI Winter (1970s)\n",
    "  - Solution: Multiple layers (deep learning)\n",
    "\n",
    "- ‚úÖ **Modern Connection**:\n",
    "  - Core principles unchanged since 1958\n",
    "  - Modern networks: same neurons, stacked deeper\n",
    "  - Better activation functions (ReLU vs step)\n",
    "  - More sophisticated learning (backprop)\n",
    "  - Still inspired by biology, but not limited by it\n",
    "\n",
    "### ü§î The Big Picture:\n",
    "\n",
    "**From Biology to AI**:\n",
    "1. ‚úÖ Observe biological neurons\n",
    "2. ‚úÖ Abstract the key principles\n",
    "3. ‚úÖ Translate to mathematics\n",
    "4. ‚úÖ Implement in code\n",
    "5. ‚úÖ Stack into networks\n",
    "6. ‚úÖ Train on data\n",
    "7. ‚úÖ Solve real problems!\n",
    "\n",
    "**Remember**:\n",
    "> \"AI started with a simple question: Can we make machines that learn like brains? We're not there yet, but every deep learning model today uses the same basic neuron inspired by biology in 1943!\"\n",
    "\n",
    "**Next**: Building your first neural network from scratch! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Further Learning\n",
    "\n",
    "**Recommended Reading**:\n",
    "- [Neurons and Synapses](https://www.khanacademy.org/science/biology/human-biology/neuron-nervous-system) - Khan Academy biology\n",
    "- [The Perceptron Paper (1958)](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) - Original paper by Rosenblatt\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - Michael Nielsen's free book\n",
    "\n",
    "**Video Tutorials**:\n",
    "- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) - Best visual explanations\n",
    "- [How Neurons Communicate](https://www.youtube.com/watch?v=6qS83wD29PY) - Crash Course biology\n",
    "- [Perceptron Explained](https://www.youtube.com/watch?v=4Gac5I64LM4) - StatQuest\n",
    "\n",
    "**Interactive Demos**:\n",
    "- [Perceptron Playground](https://www.cs.utexas.edu/~teammco/misc/perceptron/) - Interactive perceptron\n",
    "- [Neural Network Playground](https://playground.tensorflow.org/) - TensorFlow visualization\n",
    "\n",
    "**Historical Context**:\n",
    "- [AI Timeline](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) - Wait But Why article\n",
    "- [Perceptrons Book](https://mitpress.mit.edu/9780262631112/perceptrons/) - Minsky & Papert (1969)\n",
    "- [The First AI Winter](https://en.wikipedia.org/wiki/AI_winter) - Historical overview\n",
    "\n",
    "**Research Papers** (classic):\n",
    "- [McCulloch & Pitts (1943)](https://link.springer.com/article/10.1007/BF02478259) - First artificial neuron\n",
    "- [Rosenblatt (1958)](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) - The Perceptron\n",
    "- [Rumelhart et al. (1986)](https://www.nature.com/articles/323533a0) - Backpropagation\n",
    "\n",
    "**Modern Perspectives**:\n",
    "- [Deep Learning Book](https://www.deeplearningbook.org/) - Goodfellow, Bengio, Courville\n",
    "- [Biological Plausibility](https://www.frontiersin.org/articles/10.3389/fncom.2016.00094/full) - How close is AI to biology?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚û°Ô∏è What's Next?\n",
    "\n",
    "You've mastered the biological foundations!\n",
    "\n",
    "**In Chapter 3.2 - Building Your First Neural Network**, you'll:\n",
    "\n",
    "**Coming up**:\n",
    "- **Implementing from scratch**: Build a multi-layer network in pure NumPy\n",
    "- **Forward propagation**: How signals flow through networks\n",
    "- **Solving XOR**: Using multiple layers to solve the impossible\n",
    "- **PyTorch introduction**: Modern deep learning framework\n",
    "- **Tensor operations**: The math behind deep learning\n",
    "- **Your first real network**: Classify handwritten digits!\n",
    "\n",
    "From single neurons to powerful networks! üß†\n",
    "\n",
    "Ready to build? Open **[Chapter 3.2 - First Neural Network](3.2-first-neural-network.ipynb)**!\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Feedback & Community\n",
    "\n",
    "**Questions?** Join our [Discord community](https://discord.gg/madeforai)\n",
    "\n",
    "**Found a bug?** [Open an issue on GitHub](https://github.com/madeforai/madeforai/issues)\n",
    "\n",
    "**Share your perceptron experiments!** Tweet with #MadeForAI\n",
    "\n",
    "**Keep learning!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
