{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 - Deep Dive: The Transformer Architecture\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/madeforai/madeforai/blob/main/docs/paths/developer/01-transformer-architecture.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83d\udcbb The Engineer Path | Module 1 of 7**\n",
    "\n",
    "The Transformer architecture, introduced in the seminal paper \"Attention Is All You Need\", revolutionized natural language processing by dispensing with recurrence and convolutions entirely. Instead, it relies solely on attention mechanisms to draw global dependencies between input and output.\n",
    "\n",
    "## \ud83d\udcda What You'll Learn\n",
    "\n",
    "- Self-attention mechanism explained\n",
    "- Multi-head attention implementation\n",
    "- Positional encoding strategies\n",
    "- Complete Transformer block in PyTorch\n",
    "\n",
    "> **Prerequisites**: Basic understanding of vector embeddings and matrix multiplication. We'll use PyTorch for implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in Colab)\n",
    "# !pip install torch numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"\u2705 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Self-Attention Mechanism\n",
    "\n",
    "At the core of the Transformer is the **self-attention** mechanism. It allows the model to weigh the importance of different words in a sentence when encoding a specific word.\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): What we're looking for\n",
    "- **K** (Key): What we match against\n",
    "- **V** (Value): What we retrieve\n",
    "- **d_k**: Dimension of keys (for scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries of shape (batch, seq_len, d_k)\n",
    "        K: Keys of shape (batch, seq_len, d_k)\n",
    "        V: Values of shape (batch, seq_len, d_v)\n",
    "        mask: Optional mask for future tokens\n",
    "        \n",
    "    Returns:\n",
    "        Output and attention weights\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Compute attention scores: QK^T / sqrt(d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (for decoder)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Multiply by values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Self-attention on a simple sequence\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "# Create sample input (random vectors for 4 tokens)\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# For self-attention, Q = K = V = input (or linear projections)\n",
    "Q = K = V = x\n",
    "\n",
    "# Compute attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nAttention weights (how much each token attends to others):\")\n",
    "print(attn_weights.squeeze().numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attn_weights.squeeze().numpy(), cmap='Blues', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.xticks(range(seq_len), [f'Token {i}' for i in range(seq_len)])\n",
    "plt.yticks(range(seq_len), [f'Token {i}' for i in range(seq_len)])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Multi-Head Attention\n",
    "\n",
    "Multi-head attention runs the attention mechanism **multiple times in parallel** with different learned projections. This allows the model to attend to information from different representation subspaces.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # Reshape to (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi-Head Attention\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(x, x, x)  # Self-attention\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\n\u2705 Multi-Head Attention working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Positional Encoding\n",
    "\n",
    "Since transformers process all positions in parallel, we need to inject information about token positions. We use sinusoidal positional encoding:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Compute the div term\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encoding\n",
    "pe_module = PositionalEncoding(d_model=64, max_seq_len=100)\n",
    "pe_matrix = pe_module.pe.squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.imshow(pe_matrix[:50, :].T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding Visualization')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Complete Transformer Block\n",
    "\n",
    "A Transformer block combines:\n",
    "1. Multi-Head Self-Attention\n",
    "2. Layer Normalization\n",
    "3. Feed-Forward Network\n",
    "4. Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single Transformer block.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete Transformer block\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 20\n",
    "batch_size = 4\n",
    "\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = transformer_block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\n\u2705 Transformer Block working!\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in transformer_block.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfaf Key Takeaways\n",
    "\n",
    "1. **Self-Attention** lets each token attend to all other tokens in parallel\n",
    "2. **Multi-Head Attention** learns multiple attention patterns simultaneously\n",
    "3. **Positional Encoding** injects sequence order information\n",
    "4. **Transformer Blocks** combine attention, FFN, and residual connections\n",
    "\n",
    "> **\ud83d\udca1 Optimization Tip**: When running on production data, always ensure your input tensors are on the GPU by calling `.to('cuda')` on your model and data.\n",
    "\n",
    "## \ud83d\udcd6 Next Steps\n",
    "\n",
    "Continue to **Module 2: Tokenization & Embeddings** to learn how text is converted to the vectors that Transformers process.\n",
    "\n",
    "---\n",
    "\n",
    "*\u00a9 2026 MadeForAI. Learn more at [madeforai.github.io](https://madeforai.github.io/madeforai)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}